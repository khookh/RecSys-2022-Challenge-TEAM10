{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19273534-4498-471a-ad72-fe538dac3fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/26 15:42:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/26 15:42:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"feature_selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6877564-849b-4cd0-94f4-6170e9cf5674",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "As we have many features (especially after the one-hot encoding of many categorical features), feature selection should be performed in order to reduce the computational load in further steps.\n",
    "\n",
    "Two feature selection algorithms will be used\n",
    "\n",
    "1) A Feature Ranking algorithm using MRMR (Maximum relevance - Minimum redundency)\n",
    "2) A foward-selection algorithm using cross-validation. With a machine learning model, features will iteratively added as they increase the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb2b273a-3452-476f-b2aa-5704c9042535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_sessions_engineered = spark.read.csv('../Data/session_engineered_features.csv',header=True,\n",
    "                                          inferSchema=True)\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085e67ba-08ae-482c-9b3e-105b3b9d07e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: double (nullable = true)\n",
      " |-- session_time: double (nullable = true)\n",
      " |-- season: double (nullable = true)\n",
      " |-- day_period: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- item_most_time_spent: double (nullable = true)\n",
      " |-- most_time_spent_on_item: double (nullable = true)\n",
      " |-- most_frequently_bought_for_time_spent: double (nullable = true)\n",
      " |-- least_frequently_bought_for_time_spent: double (nullable = true)\n",
      " |-- mean_time: double (nullable = true)\n",
      " |-- std_time: double (nullable = true)\n",
      " |-- item_most_visited: double (nullable = true)\n",
      " |-- number_o_visit: double (nullable = true)\n",
      " |-- number_o_revisited_items: double (nullable = true)\n",
      " |-- most_frequently_bought_for_most_revisited: double (nullable = true)\n",
      " |-- first_item_visited: double (nullable = true)\n",
      " |-- last_item_visited: double (nullable = true)\n",
      " |-- normalized_features_vector: double (nullable = true)\n",
      " |-- 1: double (nullable = true)\n",
      " |-- 2: double (nullable = true)\n",
      " |-- 3: double (nullable = true)\n",
      " |-- 4: double (nullable = true)\n",
      " |-- 5: double (nullable = true)\n",
      " |-- 6: double (nullable = true)\n",
      " |-- 7: double (nullable = true)\n",
      " |-- 8: double (nullable = true)\n",
      " |-- 9: double (nullable = true)\n",
      " |-- 10: double (nullable = true)\n",
      " |-- 11: double (nullable = true)\n",
      " |-- 12: double (nullable = true)\n",
      " |-- 13: double (nullable = true)\n",
      " |-- 14: double (nullable = true)\n",
      " |-- 15: double (nullable = true)\n",
      " |-- 16: double (nullable = true)\n",
      " |-- 17: double (nullable = true)\n",
      " |-- 18: double (nullable = true)\n",
      " |-- 19: double (nullable = true)\n",
      " |-- 20: double (nullable = true)\n",
      " |-- 21: double (nullable = true)\n",
      " |-- 22: double (nullable = true)\n",
      " |-- 23: double (nullable = true)\n",
      " |-- 24: double (nullable = true)\n",
      " |-- 25: double (nullable = true)\n",
      " |-- _45: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sessions_engineered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea86292b-7421-4533-b764-7889c83c9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sets the values inside a numpy array\n",
    "\n",
    "X = train_sessions_engineered.orderBy('session_id') \n",
    "X = X.drop('session_id')\n",
    "\n",
    "X_np = np.array(X.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7945db19-e64d-452e-aa83-33be24d17c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 44)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51733313-a3ef-4160-83a0-6d9037a97d8f",
   "metadata": {},
   "source": [
    "## Value binning\n",
    "\n",
    "The following values must be binned in order to be compatible with MRMR algorithms.\n",
    "\n",
    "* Session duration\n",
    "* Most time spent on a single item (float)\n",
    "* Mean time\n",
    "* Standard deviation time\n",
    "* The 25 item value features cluster per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae55c90-3e86-46a4-92ee-5aaefb8a9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data will be binned using percentile values\n",
    "\n",
    "def assign_value_bins(x, n_bins=10):\n",
    "    bin_array = np.zeros(x.shape[0], dtype=np.int32)\n",
    "    prev_percentile = None\n",
    "    \n",
    "    for bin_idx in range(n_bins):\n",
    "        percentile = (100.0 / n_bins) * (bin_idx + 1)\n",
    "        \n",
    "        percentile_value = np.percentile(x, percentile)\n",
    "        \n",
    "        if prev_percentile is None:\n",
    "            mask = (x <= percentile_value)\n",
    "            bin_array[mask] = bin_idx\n",
    "        else:\n",
    "            mask = np.bitwise_and(x <= percentile_value, x > prev_percentile)\n",
    "            bin_array[mask] = bin_idx\n",
    "                    \n",
    "        prev_percentile = percentile_value\n",
    "    \n",
    "    return bin_array\n",
    "\n",
    "columns_to_bin = [0, 6, 9, 10, 12, 13, *range(19, 44)]\n",
    "\n",
    "for col in columns_to_bin:\n",
    "    array_to_bin = X_np[:, col].astype(np.float32)\n",
    "    binned_array = assign_value_bins(array_to_bin)\n",
    "    X_np[:, col] = binned_array\n",
    "\n",
    "X_np = X_np.astype(np.int32)  # Sets the type to int (should work well as everything is binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad83ef0-2600-42dc-97eb-f07347c691e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_X = np.transpose(X_np)\n",
    "n_total_features = t_X.shape[0]\n",
    "\n",
    "# we want the nb partition to be between 2 or 3 times more than the number of core in our computer\n",
    "Nb_partition = 10\n",
    "X_RDD = sc.parallelize(t_X, Nb_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cf323-e426-48ac-8bfb-b49b8812dea3",
   "metadata": {},
   "source": [
    "## Item label clustering\n",
    "\n",
    "As there is a high number of possible output items, the corellations migh be very low. We will use put them inside clusters in order to obtain better ranking values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f769642d-0317-4731-9861-bbed796698d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23691"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Loading the item to cluster dictionnary\n",
    "with open('../Data/item_dict.pd', 'rb') as file:\n",
    "    item_cluster_dict = pickle.load(file)\n",
    "    \n",
    "len(item_cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd94095-7f3e-4db6-8067-7ef266a35ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_purchases_clustered = (\n",
    "    train_purchases.rdd\n",
    "    .map(lambda x: (x.session_id, item_cluster_dict[x.item_id]))\n",
    ")\n",
    "\n",
    "Y_numpy = np.array(train_purchases_clustered.collect())[:, 1]\n",
    "Y_numpy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659b6a8-492a-4375-a6a2-5205af4ddec6",
   "metadata": {},
   "source": [
    "# mRMR algorithm\n",
    "\n",
    "The most relevant - minimum redundancy algorithm will select the features with the most corellation with the output values by computing the Pearson statistical test, but also with as less redundancy with previously selectionned features.\n",
    "\n",
    "In many cases, this algorithm showed to prune unecessary features while keeping good model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "287ae51c-9ced-4836-9319-7a3a7f5b0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_mrmr(x, y):\n",
    "    \"\"\"\n",
    "    return the correlation value between two variable in absolute value\n",
    "    \"\"\"\n",
    "    return np.abs(scipy.stats.pearsonr(x, y)[0])\n",
    "\n",
    "\n",
    "def get_mrmr_score_spark(x,selected_features):\n",
    "    \"\"\"\n",
    "    x : the feature to evaluate to add into the model\n",
    "    y : output value\n",
    "    selected_features : the features already selected\n",
    "    return the score for x with the rest of selected variables\n",
    "    \"\"\"\n",
    "    \n",
    "    y = broadcast_Y.value\n",
    "    # Get correlation score between feature x and output y (relevance)\n",
    "    score_x_y_s = get_score_mrmr(x, y)\n",
    "    \n",
    "    \n",
    "    nb_selected_features = selected_features.shape[0]\n",
    "    # If some features have already been selected\n",
    "    if nb_selected_features > 0:\n",
    "        \n",
    "        # Get corrrelation scores between x and each feature already selected (redundancy)\n",
    "        score_features_x_s = np.zeros(nb_selected_features, dtype=float)\n",
    "        \n",
    "        for j in range(nb_selected_features):\n",
    "            \n",
    "            score_x_s_j = get_score_mrmr(x, selected_features[j,:])\n",
    "            \n",
    "            # if score is nan considering that we want to calculate the mean\n",
    "            # we transform it in 0 ?\n",
    "            if np.isnan(score_x_s_j):\n",
    "                score_x_s_j=0\n",
    "            \n",
    "                \n",
    "            score_features_x_s[j] = score_x_s_j\n",
    "                        \n",
    "        # Final score is relevance to output feature - average redundancy with already selected features\n",
    "        score_x_y_s = score_x_y_s - np.mean(score_features_x_s)\n",
    "        \n",
    "    return score_x_y_s\n",
    "\n",
    "\n",
    "def mrmr_spark(n_total_features, K, sc, X_RDD):\n",
    "    \"\"\"\n",
    "    n_total_features : total number of features\n",
    "    K : number of feature to select\n",
    "    sc : spark context\n",
    "    X_RDD : RDD of the variable X\n",
    "    Y: Output data\n",
    "    \n",
    "    :return: the indice of selected features and time execution using mrmr\n",
    "    \"\"\"\n",
    "    time_execution = []\n",
    "    remaining_features_indices = list(range(n_total_features))\n",
    "    selected_features_indices = []\n",
    "    \n",
    "\n",
    "    for k in range(K):\n",
    "        print(\"Step: \"+str(k))\n",
    "    \n",
    "        start_time=time.time()\n",
    "    \n",
    "        # Get the subset of selected features values, and cast as an array\n",
    "        selected_features = X_RDD.zipWithIndex().filter(lambda x: x[1] in selected_features_indices).map(lambda x: x[0]).collect()\n",
    "        selected_features = np.array(selected_features)\n",
    "    \n",
    "        # mRMR scores are computed by first filtering `t_X` to remove already selected features, and then mapping \n",
    "        # each remaining feature using the `get_mrmr_score_spark` function\n",
    "        scores = X_RDD.zipWithIndex().filter(lambda x: x[1] in remaining_features_indices).map(lambda x:get_mrmr_score_spark(x[0],selected_features)).collect()\n",
    "    \n",
    "        # Once all mRMR scores are computed, the index of the feature with the highest score is selected\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "    \n",
    "        index_max_score_features = np.argmax(scores)\n",
    "    \n",
    "        selected_features_indices.append(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        del(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        print('Took time:', time.time()-start_time)\n",
    "        time_execution.append(time.time()-start_time)\n",
    "        \n",
    "    return selected_features_indices, time_execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3c172-42cb-453e-b14b-dfbdf8f73b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_Y = sc.broadcast(Y_numpy)\n",
    "selected_features_indices, execution_time = mrmr_spark(n_total_features, 20, sc, X_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d654e13-2771-4a4b-ba00-964865dfd33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 11, 15, 16, 17, 19, 23, 26, 27, 28, 29, 33, 34, 36, 37, 39, 40, 42]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_indices = sorted(selected_features_indices)\n",
    "selected_features_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35556d26-dee4-441d-82a8-ad5bc65c824c",
   "metadata": {},
   "source": [
    "# Forward feature selection algorithm\n",
    "\n",
    "Using a machine learning model, we will iteratively add new features into the training and compare at each iteration which feature provided the best results using cross-validation.\n",
    "\n",
    "As Naïve Bayes does not require training and can directly output prediction probabilities, it is considered the best for this problem as many different results are possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14bc308-7a5d-4e58-b652-9b5b619f22f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
