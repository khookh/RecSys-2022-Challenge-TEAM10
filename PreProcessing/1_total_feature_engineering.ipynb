{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8b7641-757d-48be-b113-d680a3a617ba",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "From the dataset explored in the 0_data_analysis notebook, we compute several features.\n",
    "\n",
    "In order to obtain a learnable dataset, features must be preprocessed and engineered in order to contain valuable learning data. This notebook will generate a PySpark RDD that contains the preprocessed dataset, ready for feature selection algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfefb50-d3ea-42d8-b0e4-fd5981520e8c",
   "metadata": {},
   "source": [
    "### Configuring and launching the pyspark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82df480c-3e79-468f-a378-e5477796c93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/27 09:07:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "import sys\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"load_explore\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752e400-58b3-4576-bd3a-a6c47dea0bb1",
   "metadata": {},
   "source": [
    "### Loading the datasets inside spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d668984b-81c2-4ff8-96d7-90b2a739aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loads relevant datas in DataFrames\n",
    "train_sessions = spark.read.load('../Data/train_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "candidate_items = spark.read.load('../Data/candidate_items.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "item_features = spark.read.load('../Data/item_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "test_sessions = spark.read.load('../Data/test_leaderboard_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed91f9d-7fc4-4e26-b549-4d6daebea0f7",
   "metadata": {},
   "source": [
    "## Time related features.\n",
    "\n",
    "As fashion purchases are highly dependant on seasonal trends, we will extract date features from the session item visit dates.\n",
    "\n",
    "For each session, the first item visit date will be taken and used as the reference session date. (The average session duration is an hour)\n",
    "\n",
    "Engineered features include:\n",
    "* Month of the session ✅\n",
    "* Season of the session ✅\n",
    "* Year of the session ✅\n",
    "* Duration of the session ✅\n",
    "* Day period of the session ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f1a988c-16d0-4101-86ec-be2db53a5236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, 3703), (28, 87), (36, 43), (44, 33), (48, 657)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def parse_datetime(timestamp):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')    \n",
    "\n",
    "date_parsed_sessions = (train_sessions.rdd\n",
    "                        .map(lambda x: (x.session_id, parse_datetime(x.date)))  # Maps rows to (key=session_id, values=(parsed_date)) tuples\n",
    "                        .cache())\n",
    "\n",
    "# Reduces by key from the MAX monoid\n",
    "max_date_sessions = (\n",
    "    date_parsed_sessions\n",
    "    .reduceByKey(max)\n",
    ")\n",
    "\n",
    "# Reduces by key from the MIN monoid\n",
    "min_date_sessions = (\n",
    "    date_parsed_sessions\n",
    "    .reduceByKey(min)\n",
    ")\n",
    "\n",
    "# Computes delta time in seconds\n",
    "time_sessions_in_seconds = (\n",
    "    max_date_sessions\n",
    "    .join(min_date_sessions)  # Joins the max dates with the min dates from the session_id (on pair per session)\n",
    "    .mapValues(lambda x: (x[0] - x[1]).seconds)  # Computes delta time for each session \n",
    ")\n",
    "\n",
    "time_sessions_in_seconds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "857ed548-6a58-43ad-b082-99d7b86cf976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, datetime.datetime(2020, 2, 26, 17, 22, 48, 903000)), (28, datetime.datetime(2020, 5, 18, 12, 50, 24, 248000)), (36, datetime.datetime(2020, 6, 21, 10, 29, 8, 263000)), (44, datetime.datetime(2020, 11, 27, 20, 45, 10, 302000)), (48, datetime.datetime(2020, 4, 15, 17, 17, 42, 594000))]\n",
      "[(24, 0), (28, 1), (36, 2), (44, 3), (48, 1)]\n",
      "[(24, 2), (28, 2), (36, 1), (44, 3), (48, 2)]\n",
      "[(24, 1), (28, 4), (36, 5), (44, 10), (48, 3)]\n",
      "[(24, 0), (28, 0), (36, 0), (44, 0), (48, 0)]\n"
     ]
    }
   ],
   "source": [
    "def get_season(date_time):\n",
    "    '''Converts the date_time into a season\n",
    "    \n",
    "    :returns: an integer\n",
    "        0 -> Winter\n",
    "        1 -> Spring\n",
    "        2 -> Summer\n",
    "        3 -> Autumn\n",
    "    '''    \n",
    "    season = (date_time.month - 1) // 3\n",
    "    season += (date_time.month == 3)&(date_time.day>=20)\n",
    "    season += (date_time.month == 6)&(date_time.day>=21)\n",
    "    season += (date_time.month == 9)&(date_time.day>=23)\n",
    "    season -= 3*int(((date_time.month == 12)&(date_time.day>=21)))\n",
    "    return season\n",
    "\n",
    "def get_day_period(date_time):\n",
    "    '''Converts the date_time into the day of the week.\n",
    "    \n",
    "    0 -> Morning (from 6am to 12am)\n",
    "    1 -> Afternoon (from 12am to 6pm)\n",
    "    2 -> Evening (from 6pm to 12pm)\n",
    "    3 -> Night (from 12pm to 6am)\n",
    "    '''\n",
    "    return date_time.hour // 6\n",
    "\n",
    "# Assigns a season for each session\n",
    "season_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(get_season)\n",
    ")\n",
    "\n",
    "# Assigns a day period (morning, afternoon, evening, night...) for each session\n",
    "day_period_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(get_day_period)\n",
    ")\n",
    "\n",
    "# Assigns a month for each session\n",
    "month_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(lambda x: x.month - 1)\n",
    ")\n",
    "\n",
    "# Assigns a year for each session\n",
    "year_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(lambda x: x.year - 2020)\n",
    ")\n",
    "\n",
    "print(min_date_sessions.take(5))\n",
    "print(season_per_session.take(5))\n",
    "print(day_period_per_session.take(5))\n",
    "print(month_per_session.take(5))\n",
    "print(year_per_session.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570a914-1596-41f5-bfdf-72eadae751e9",
   "metadata": {},
   "source": [
    "## Item time related features\n",
    "\n",
    "Computing the items where time was the most spent on. The time spent on an item is the difference between the visit of that item and the visit of the last item. For the last item, the time is computed from the session purchase date.\n",
    "\n",
    "We will extract two feautures:\n",
    "* The item on which the use has spent the most time on ✅\n",
    "* The time spent on that item ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e42080da-0cf0-4f15-94a0-ce54144b5e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24,\n",
       " [(2927, 15),\n",
       "  (2927, 180),\n",
       "  (16064, 23),\n",
       "  (11662, 42),\n",
       "  (434, 3096),\n",
       "  (18539, 183),\n",
       "  (10414, 42),\n",
       "  (28075, 118),\n",
       "  (18476, 191)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing session purchase_date\n",
    "session_purchase_date = (\n",
    "    train_purchases.rdd\n",
    "    .map(lambda x: (x.session_id, parse_datetime(x.date)))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_time_per_item(session_info):\n",
    "    '''Computes the time spent on each item (in seconds)\n",
    "    \n",
    "    :param session_info: The information of the session ([(item_id, date)...], purchase_date)\n",
    "    :returns: Time per item information [(item_id, visit_time)...]\n",
    "    '''\n",
    "    # Unpacks the info\n",
    "    visited_items, purchase_date = session_info\n",
    "    item_time_list = []\n",
    "    \n",
    "    for idx in range(len(visited_items)):\n",
    "        item_id, item_visit_date = visited_items[idx]\n",
    "        \n",
    "        # Last item, need to check the purchase date\n",
    "        if idx == len(visited_items) - 1:\n",
    "            item_time_list.append((item_id, (purchase_date - item_visit_date).seconds))\n",
    "        # Not the last item, checks the next item \n",
    "        else:\n",
    "            _, next_item_visit_date = visited_items[idx + 1]\n",
    "            item_time_list.append((item_id, (next_item_visit_date - item_visit_date).seconds))\n",
    "    \n",
    "    return item_time_list\n",
    "\n",
    "\n",
    "# Here, we group the tuples by their keys (session_id) using Map Reduce\n",
    "items_time_per_session = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, (x.item_id, parse_datetime(x.date))))\n",
    "    .mapValues(lambda x: [(x[0], x[1])])  # Sets all the values inside a list in order to easily reduce and join the sessions\n",
    "    .reduceByKey(lambda x, y: x + y)  # Joins all the sessions togheter by reducing and joining the lists\n",
    "    .join(session_purchase_date)  # Each tuple has now the shape (session_id, ([(item_id, date)...], purchase_date))\n",
    "    .mapValues(lambda x: (sorted(x[0], key=lambda t:t[1]), x[1]))  # For each session, sorts the (item_id, date) tuples by date \n",
    "    .mapValues(compute_time_per_item)\n",
    ")\n",
    "\n",
    "items_time_per_session.take(1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9fbd84c-3437-44ca-9b0e-e1b714e1fd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(session_id=26, item_id=19185, date='2021-06-16 09:53:54.158')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_items_time_per_session = (\n",
    "    test_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, (x.item_id, parse_datetime(x.date))))\n",
    "    .mapValues(lambda x: [(x[0], x[1])])  # Sets all the values inside a list in order to easily reduce and join the sessions\n",
    "    .reduceByKey(lambda x, y: x + y)  # Joins all the sessions togheter by reducing and joining the lists\n",
    "    .join(session_purchase_date)  # Each tuple has now the shape (session_id, ([(item_id, date)...], purchase_date))\n",
    "    .mapValues(lambda x: (sorted(x[0], key=lambda t:t[1]), x[1]))  # For each session, sorts the (item_id, date) tuples by date \n",
    "    .mapValues(compute_time_per_item)\n",
    ")\n",
    "\n",
    "test_sessions.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3171ed3a-1d7f-43e1-a5e3-2fb8a3c58a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24, (434, 3096))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_spent_time = (\n",
    "    items_time_per_session\n",
    "    .mapValues(lambda x: sorted(x, reverse=True, key=lambda t: t[1]))  # Sorts the item visits by time\n",
    "    .mapValues(lambda x: x[0])  # Gets the item with the most time spent on\n",
    ")\n",
    "\n",
    "most_spent_time.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd074afc-0354-403a-be6d-89429d0a7614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106349/2511250147.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mTEST_most_spent_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "TEST_most_spent_time = (\n",
    "    TEST_items_time_per_session\n",
    "    .mapValues(lambda x: sorted(x, reverse=True, key=lambda t: t[1]))  # Sorts the item visits by time\n",
    "    .mapValues(lambda x: x[0])  # Gets the item with the most time spent on\n",
    ")\n",
    "\n",
    "TEST_most_spent_time.take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655494d0-248a-4a63-83dd-7fe955d568bb",
   "metadata": {},
   "source": [
    "## Session mean and standard deviation time\n",
    "\n",
    "We will extract two features:\n",
    "\n",
    "* For each session, the mean visit time on each item ✅\n",
    "* For each session, the standard time on each time ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e933198-b6ab-4429-a71f-0a64fa651497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, (432.22223, 944.24854))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_std_time_session = (\n",
    "    items_time_per_session\n",
    "    .mapValues(lambda x: np.array(x, dtype=np.float32)[:, 1])  # Gets a numpy array for each time and takes only the visit time (drops the item id)\n",
    "    .mapValues(lambda x: (x.mean(), x.std()))\n",
    ")\n",
    "\n",
    "mean_std_time_session.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38139e-8af7-441e-8e72-895ed2fdd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_std_time_session = (\n",
    "    items_time_per_session\n",
    "    .mapValues(lambda x: np.array(x, dtype=np.float32)[:, 1])  # Gets a numpy array for each time and takes only the visit time (drops the item id)\n",
    "    .mapValues(lambda x: (x.mean(), x.std()))\n",
    ")\n",
    "\n",
    "mean_std_time_session.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35435804-2cfb-4c72-aebf-589ef1ad61f0",
   "metadata": {},
   "source": [
    "## Item revisit time\n",
    "\n",
    "Item that have been revisited are the most likely to catch the interest of the user, and therefore to be purchased.\n",
    "\n",
    "We will extract three features:\n",
    "* The item that has been revisited the most times ✅\n",
    "* The number of times this item has been revisited ✅\n",
    "* The number of items that have been revisited at least once ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40a7631-69a1-4dff-afa3-c0861cdf3f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, (2927, 2, 1)), (28, (11529, 2, 1)), (36, (-1, -1, 0)), (44, (-1, -1, 0)), (48, (-1, -1, 0)), (52, (-1, -1, 0)), (108, (12735, 3, 2)), (124, (-1, -1, 0)), (140, (-1, -1, 0)), (156, (-1, -1, 0))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "item_visit_counts = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, x.item_id))  # Maps the rows to a (key=session_id, values=item_id) tuple\n",
    "    .mapValues(lambda x: [x])  # Puts the items inside a list, so they can be reduced easily.\n",
    "    .reduceByKey(lambda x, y: x + y)  # Joins the lists togheter, grouping the tuples by keys\n",
    "    .mapValues(lambda x: np.unique(x, return_counts=True))  # Transforms values into two arrays: an item_id array and an occurence_array\n",
    "    .mapValues(lambda x: np.vstack((x[0], x[1])))\n",
    ")\n",
    "\n",
    "def get_revisited_items(count_array):\n",
    "    '''Gets the number of items revisited at least once'''\n",
    "    revisited_indices = count_array[1, :] > 1\n",
    "    return np.count_nonzero(revisited_indices)\n",
    "\n",
    "def get_item_revisits_info(count_array):\n",
    "    '''Returns the item_id of the item that\n",
    "    has been revisited the most as well as the \n",
    "    number of times it has been revisited\n",
    "    \n",
    "    If no item was revisited return -1, -1, 0\n",
    "    '''\n",
    "    number_of_revisits = get_revisited_items(count_array)\n",
    "    if number_of_revisits == 0:\n",
    "        return (-1, -1, number_of_revisits)\n",
    "    \n",
    "    most_revisited_item_idx = np.argmax(count_array[1, :])\n",
    "    most_revisits = count_array[1, :].max()\n",
    "    \n",
    "    return (count_array[0, most_revisited_item_idx], most_revisits, number_of_revisits)\n",
    "    \n",
    "\n",
    "item_revisit_info = (\n",
    "    item_visit_counts\n",
    "    .mapValues(get_item_revisits_info) \n",
    ")\n",
    "\n",
    "\n",
    "print(item_revisit_info.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b8fb4-fab5-4c91-96d5-e8a91a0b45c7",
   "metadata": {},
   "source": [
    "# Engineering dataset features from item features\n",
    "\n",
    "Each item in the dataset has a finite number of features. Presented in tuples of (feature_cateogy_id, feature_value_id), each item has multiple of those feauters.\n",
    "\n",
    "As there are 73 unique features categories and 904 different unique (feature_category, feature_value) pairs. As most of those categories are not documented, it is difficult to interepret their meaning.\n",
    "\n",
    "In this phase, we will engineer features by performing K-Means clustering using Map-Reduce.\n",
    "\n",
    "Multiple features will be engineered:\n",
    "* One feature telling in wich category cluster is the session belonging ✅\n",
    "* One feature for each cluster on the item feature value clustering (25 clusters) ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a673c8-70c3-4497-a60f-2dd694baf802",
   "metadata": {},
   "source": [
    "Counting the number of occurences of each item category in each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d8eb90-c2d4-4116-a50c-c6d513b69cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(474192,\n",
       "  array([0.        , 0.00381679, 0.01145038, 0.02290076, 0.01145038,\n",
       "         0.01908397, 0.04961832, 0.        , 0.        , 0.        ,\n",
       "         0.02290076, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.02290076, 0.02290076, 0.02290076, 0.        ,\n",
       "         0.        , 0.        , 0.00381679, 0.01145038, 0.        ,\n",
       "         0.02290076, 0.        , 0.00381679, 0.02671756, 0.04961832,\n",
       "         0.00381679, 0.02290076, 0.00763359, 0.01145038, 0.        ,\n",
       "         0.00381679, 0.00381679, 0.00381679, 0.00381679, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.02290076,\n",
       "         0.01145038, 0.05725191, 0.        , 0.00381679, 0.05725191,\n",
       "         0.        , 0.        , 0.01908397, 0.        , 0.02671756,\n",
       "         0.05725191, 0.        , 0.        , 0.02290076, 0.01908397,\n",
       "         0.05343511, 0.02671756, 0.02290076, 0.00381679, 0.02290076,\n",
       "         0.        , 0.00381679, 0.05343511, 0.04961832, 0.        ,\n",
       "         0.00381679, 0.05343511, 0.01908397], dtype=float32))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts the number of unique category IDs\n",
    "unique_categories_nb = len(\n",
    "    item_features.rdd\n",
    "    .map(lambda x: set([x.feature_category_id]))  # Maps the rows to a set containing the feature category\n",
    "    .reduce(lambda x, y: x.union(y))  # Reduces by joining the sets, keeping only the unique categories values\n",
    ")\n",
    "\n",
    "def initialize_vector(category_nb, max_categories=unique_categories_nb):\n",
    "    '''\n",
    "    Returns a vector of categories, where each element is 0 except the one at the category's index.\n",
    "    '''\n",
    "    vector = np.zeros(max_categories, dtype=np.float32)\n",
    "    vector[category_nb - 1] = 1.0\n",
    "    return vector\n",
    "\n",
    "def normalize_vector(category_vector):\n",
    "    '''\n",
    "    Noramlizes a category vector, dividing all the elements by the sum of of occurences\n",
    "    '''\n",
    "    total_occurences = np.sum(category_vector)\n",
    "    return category_vector / total_occurences\n",
    "\n",
    "\n",
    "normalized_categories_vector_per_session = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.item_id, x.session_id))  # Maps the rows to (key=item_id,values=sessions_id) tuples\n",
    "    .join(item_features.rdd.map(lambda x: (x.item_id, x.feature_category_id)))  # Joins the two datasets togheter by item_id\n",
    "    .map(lambda x: x[1])  # Only keeps some parts of the tuples, now we have (key=session_id,values=feature_category_id)\n",
    "    .mapValues(initialize_vector)  # Encodes the feature category inside a counter vector\n",
    "    .reduceByKey(lambda x, y: x + y)   # Reduces by session_id, and adds the counter vectors\n",
    "    .mapValues(normalize_vector)  # Noramlizes the vector, so that the sum of every element is equal to 1.0\n",
    ")\n",
    "\n",
    "normalized_categories_vector_per_session.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b2f89-408e-4041-8dcc-819e43c7c347",
   "metadata": {},
   "source": [
    "## Clustering on item feature categories\n",
    "\n",
    "Performing clustering from the Normalized Category Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8cdfca8-cacd-436b-956d-986624244b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 09:13:06 WARN BlockManager: Task 64 already completed, not releasing lock for rdd_105_0\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "##### CLUSTERING WITH MAP/REDUCE\n",
    "# inspired from https://uv.ulb.ac.be/pluginfile.php/3410436/mod_resource/content/1/Kmeans.html?fbclid=IwAR1h5xJKxQ1nrCHlgtQsofwjgc8B7oVl69GG6Mm8WxChTH3zBc4SKFo3Noo\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def mapstep(x,br_clusters): \n",
    "    \"\"\"\n",
    "    This function returns the closest cluster of the current row x\n",
    "    x : current row\n",
    "    broadcast_clusters : current value of the clusters\n",
    "    \"\"\"\n",
    "    M = br_clusters.value.shape[0]\n",
    "    d=np.zeros(M) # distance of the current row with each cluster\n",
    "    for m in range(M):\n",
    "        d[m]=sum(abs(np.subtract(x,br_clusters.value[m]))) # compute distance between cluster m and row x\n",
    "    return np.argmin(d)\n",
    "\n",
    "def add_values(x, y):\n",
    "    return x[0] + y[0], x[1] + y[1]\n",
    "\n",
    "def k_means_MR(dataset, M, steps):\n",
    "    \"\"\"\n",
    "    dataset : RDD data structure, all columns are used for distance computation (1xn dimension)\n",
    "    M : number of clusters\n",
    "    steps : number of steps\n",
    "    \"\"\"\n",
    "    n = len(dataset.take(1)[0]) # input dimension\n",
    "    clusters = np.array(dataset.takeSample(True,M)) # starts with random clusters\n",
    "    broadcast_clusters = sc.broadcast(clusters) ## broadcast cluster position\n",
    "    for i in range(steps):\n",
    "        distance_set = dataset.map(lambda x : (mapstep(x, broadcast_clusters), (x, 1))) # set containing (closest_cluster,(row value),counter=1)\n",
    "        new_clusters_set = distance_set.reduceByKey(add_values) # adds all cluster related entry (+ increments counter)\n",
    "        new_clusters_set = new_clusters_set.map(lambda x : x[1][0] / x[1][1]) # apply mean operation\n",
    "        new_clusters = np.array(new_clusters_set.take(M))\n",
    "        broadcast_clusters = sc.broadcast(new_clusters)\n",
    "    return broadcast_clusters.value\n",
    "\n",
    "\n",
    "mapped_vectors = normalized_categories_vector_per_session.map(lambda x: x[1]).cache()  # Drops the session id\n",
    "\n",
    "feature_category_clusters = k_means_MR(\n",
    "    mapped_vectors,\n",
    "    10,  # 10 Clusters are taken\n",
    "    5  # Iterates over 5 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a06d274-6c4f-4245-9186-9b2993b776a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(474192, 8),\n",
       " (1119330, 8),\n",
       " (1309362, 0),\n",
       " (1468758, 8),\n",
       " (1910616, 0),\n",
       " (2691762, 1),\n",
       " (2933592, 8),\n",
       " (4141098, 8),\n",
       " (18, 0),\n",
       " (93486, 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_clusters_142 = sc.broadcast(feature_category_clusters) # broadcasting is a good practice for parallel computing\n",
    "clustered_categories = normalized_categories_vector_per_session.map(lambda x : (x[0], mapstep(x[1],broadcast_clusters_142))) # compute associated clusters for each item\n",
    "\n",
    "clustered_categories.take(10) # (example) OUTPUT [SESSION_ID | CLUSTER_ID]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14cf37-2008-4bf5-930e-d52d37c8822c",
   "metadata": {},
   "source": [
    "## Clustering on item feature values\n",
    "\n",
    "We will perform clustering on the item feature values only. In total, 25 different clusters will be computed and each item will be classified in one of those clusters.\n",
    "\n",
    "Then, for each session we will look at the visited items. For each of those items we will increment the session's counter for the class of that item. In total, 25 new features will be added to our sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860eaa1d-4ae5-487a-8d56-61b3e415a621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_feature_values = (\n",
    "    item_features.rdd\n",
    "    .map(lambda x: (set([x.feature_value_id])))  # Maps each row to a set containing the feature value\n",
    "    .reduce(lambda x, y: x.union(y))  # Joins the sets, discarding duplicate values (only keeping unique values)\n",
    ")\n",
    "\n",
    "highest_feature_value = max(unique_feature_values)  # Takes the highest value in the set, used as vector sie\n",
    "\n",
    "\n",
    "def initialize_value_vector(value_nb, max_values=highest_feature_value):\n",
    "    '''\n",
    "    Returns a vector of item feature values, where each element is 0 except the one at the values's index.\n",
    "    '''\n",
    "    vector = np.zeros(max_values, dtype=np.float32)\n",
    "    vector[value_nb - 1] = 1.0\n",
    "    return vector\n",
    "\n",
    "encoded_item_values = (\n",
    "    item_features.rdd\n",
    "    .map(lambda x: (x.item_id, x.feature_value_id))  # Maps each row to a (key=item_id, values=feature_value_id) tuples\n",
    "    .mapValues(initialize_value_vector)  # Initializes each value vector (one hot encoding type)\n",
    "    .reduceByKey(lambda x, y: x + y)  # Reduces on each item by summing the feature value vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f83b868-b5d9-4683-b9fe-84ac411d510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Performs clustering\n",
    "mapped_item_values = encoded_item_values.map(lambda x: x[1])\n",
    "\n",
    "feature_values_clusters = k_means_MR(\n",
    "    mapped_item_values,\n",
    "    25,  # As analysed in notebook 2b_feature_clustering, 25 is a good cluster compromise\n",
    "    5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f4b2600-d027-42f1-af4d-b8d8bb3faa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 13),\n",
       " (4, 0),\n",
       " (8, 14),\n",
       " (10, 1),\n",
       " (14, 15),\n",
       " (16, 0),\n",
       " (18, 2),\n",
       " (20, 16),\n",
       " (24, 1),\n",
       " (26, 16)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_clusters_47 = sc.broadcast(feature_values_clusters) # broadcasting is a good practice for parallel computing\n",
    "clustered_item_values = encoded_item_values.map(lambda x : (x[0], mapstep(x[1],broadcast_clusters_47))) # compute associated clusters for each item\n",
    "\n",
    "clustered_item_values.take(10) # (example) OUTPUT [ITEM_ID | CLUSTER_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dbccdf0-de8d-42de-b81f-c8718d32200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(474192,\n",
       "  (0.06666667,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.06666667,\n",
       "   0.0,\n",
       "   0.13333334,\n",
       "   0.0,\n",
       "   0.2,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.2,\n",
       "   0.06666667,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.2,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.06666667,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encodes the clustered item values in one-hot encoding\n",
    "encoded_item_values = (\n",
    "    clustered_item_values\n",
    "    .mapValues(lambda x: initialize_value_vector(x, max_values=25))\n",
    ")\n",
    "\n",
    "# Computes the vector for each session\n",
    "item_features_session = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.item_id, x.session_id))\n",
    "    .join(encoded_item_values)\n",
    "    .map(lambda x: (x[1][0], x[1][1]))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .mapValues(normalize_vector)\n",
    "    .mapValues(tuple)\n",
    ")\n",
    "\n",
    "item_features_session.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7ac21-27cd-4ab2-8b37-99e4b3879c7b",
   "metadata": {},
   "source": [
    "# Cluster items depending on their categories\n",
    "\n",
    "As there is a too high amount of different items, one-hot encoding them would result in an undesirable explosion of features.\n",
    "\n",
    "Items are therefore assigned to one of 50 clusters, based on their categories. Two similar items have a higher chance to have common categories, and item values are too unique to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e387209-0cf3-44d2-83eb-e14392bbd0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0),\n",
       " (4, 1),\n",
       " (8, 22),\n",
       " (10, 23),\n",
       " (14, 2),\n",
       " (16, 1),\n",
       " (18, 3),\n",
       " (20, 24),\n",
       " (24, 25),\n",
       " (26, 4),\n",
       " (28, 5),\n",
       " (30, 4),\n",
       " (32, 26),\n",
       " (36, 4),\n",
       " (38, 27),\n",
       " (40, 6),\n",
       " (42, 25),\n",
       " (44, 7),\n",
       " (46, 28),\n",
       " (50, 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def mapstep(x,br_clusters): \n",
    "    \"\"\"\n",
    "    This function returns the closest cluster of the current row x\n",
    "    x : current row\n",
    "    broadcast_clusters_k : current value of the clusters\n",
    "    \"\"\"\n",
    "    M = br_clusters.value.shape[0]\n",
    "    d=np.zeros(M) # distance of the current row with each cluster\n",
    "    for m in range(M):\n",
    "        d[m]=sum(abs(np.subtract(x,br_clusters.value[m]))) # compute distance between cluster m and row x\n",
    "    return(np.argmin(d),)    \n",
    "\n",
    "def k_means_MR(dataset,M,steps):\n",
    "    \"\"\"\n",
    "    dataset : RDD data structure, all columns are used for distance computation (1xn dimension)\n",
    "    M : number of clusters\n",
    "    steps : number of steps\n",
    "    \"\"\"\n",
    "    n = len(dataset.take(1)[0]) # input dimension\n",
    "    clusters = np.array(dataset.takeSample(True,M)) # starts with random clusters\n",
    "    broadcast_clusters_k = sc.broadcast(clusters) ## broadcast cluster position\n",
    "    for i in range(steps):\n",
    "        distance_set = dataset.map(lambda x : mapstep(x,broadcast_clusters_k)+(x+(1,),)) # set containing (closest_cluster,(row value),counter=1)\n",
    "        new_clusters_set = distance_set.reduceByKey(lambda x,y : np.add(x,y)) # adds all cluster related entry (+ increments counter)\n",
    "        new_clusters_set = new_clusters_set.map(lambda x : x[1][:-1]/x[1][-1]) # apply mean operation\n",
    "        new_clusters = np.array(new_clusters_set.take(M))\n",
    "        broadcast_clusters_k = sc.broadcast(new_clusters)\n",
    "    return broadcast_clusters_k.value\n",
    "\n",
    "category_count = item_features.rdd.map(lambda x: x[1]).distinct().collect() # 904 different features value\n",
    "renamed_set = item_features.rdd.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "dic_values = {elem:i for i, elem in enumerate(category_count)} # maps the features values in a dictionnary\n",
    "\n",
    "def fill_column(x):\n",
    "    value = dic_values.get(x[1])\n",
    "    return (x[0],((0,)*(value)+(1,)+(0,)*(len(category_count)-value-1)))\n",
    "\n",
    "columns_features = renamed_set.map(fill_column) # creates all empty columns with features values and fills it\n",
    "reduced_map = columns_features.reduceByKey(lambda x,y : tuple(sum(x) for x in zip(x,y))) # reduce by key (item)\n",
    "\n",
    "dataset = reduced_map.map(lambda x : x[1]) # remap everything in one single line of 904 elements (easier for clustering)\n",
    "\n",
    "item_cat_clusters = k_means_MR(dataset,50,5) # compute cluster values\n",
    "\n",
    "broadcast_clusters_pp = sc.broadcast(item_cat_clusters) # broadcasting is a good practice for parallel computing\n",
    "item_clustered_by_cat = reduced_map.map(lambda x : (x[0],)+mapstep(x[1],broadcast_clusters_pp)) # compute associated clusters for each item\n",
    "\n",
    "item_clustered_by_cat.take(20) # (example) OUTPUT [ITEM_ID | CLUSTER_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27098b77-f31f-4117-a93c-576fad67ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saves a dictionnary that assigns each item to its cluster (so no clustering operation must be done again)\n",
    "item_cluster_dict = { item_id: cluster for item_id, cluster in item_clustered_by_cat.collect() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca16becb-1e7d-4cdd-9af7-640fcb81ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../Data/item_dict.pd', 'wb') as handle:\n",
    "    pickle.dump(item_cluster_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70c4e116-291d-4698-9ffd-29bd8ed924f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cluster_revisited(x):\n",
    "    if x[0] != -1:\n",
    "        return item_cluster_dict[x[0]], x[1], x[2]\n",
    "    return x\n",
    "    \n",
    "most_spent_time_clustered = most_spent_time.mapValues(lambda x: (item_cluster_dict[x[0]], x[1]))  # Encodes the item id in its cluster\n",
    "revisited_clustered = item_revisit_info.mapValues(map_cluster_revisited)  # Encodes the item id in its cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76955202-e5b6-4dd7-b2e2-424e30f4a3e9",
   "metadata": {},
   "source": [
    "## First and last visited items\n",
    "\n",
    "In each session, take the item that has been visited as first and revisited as last\n",
    "\n",
    "Two features will be engineered:\n",
    "\n",
    "* The item that has been visited as first (clustered) ✅\n",
    "* The item that has been visites as last (clustered) ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b39eff4-88cf-4f23-822e-3931612e8483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 09:24:20 WARN BlockManager: Task 219 already completed, not releasing lock for rdd_244_0\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, (2927, 18476)),\n",
       " (28, (16895, 21902)),\n",
       " (36, (25417, 26536)),\n",
       " (44, (22747, 17089)),\n",
       " (48, (8398, 26404)),\n",
       " (52, (24636, 12047)),\n",
       " (108, (4816, 15421)),\n",
       " (124, (26092, 26092)),\n",
       " (140, (18723, 13914)),\n",
       " (156, (3462, 3462))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_last_item = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, (x.item_id, parse_datetime(x.date))))  # Maps the rows into a (key=session_id, value=(item_id, date)) tuple\n",
    "    .mapValues(lambda x: ([(x)]))  # Puts the (item_id, date) tuple into a list to easily merge the values\n",
    "    .reduceByKey(lambda x, y: x + y)  # Groups the items per session, merging the session's items and their date\n",
    "    .mapValues(lambda x: sorted(x, key=lambda x_: x_[1]))  # In each session, sorts the items by date (earliest to latest)\n",
    "    .mapValues(lambda x: (x[0][0], x[-1][0]))  # Takes the first and last visited items\n",
    "    .cache()\n",
    ")\n",
    "first_last_item.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "114660c3-a7d9-4952-8ebe-bd56b74cda91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, (17, 33)),\n",
       " (28, (7, 7)),\n",
       " (36, (32, 31)),\n",
       " (44, (5, 5)),\n",
       " (48, (30, 8)),\n",
       " (52, (9, 1)),\n",
       " (108, (36, 14)),\n",
       " (124, (32, 32)),\n",
       " (140, (5, 5)),\n",
       " (156, (38, 38))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encodes the first and last visited item from their clusters\n",
    "first_last_item_clustered = first_last_item.mapValues(lambda x: (item_cluster_dict[x[0]], item_cluster_dict[x[1]]))\n",
    "first_last_item_clustered.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7753f77-48c5-4557-868d-1b0711781895",
   "metadata": {},
   "source": [
    "## Most and Least frequent bought item (clustered), based on the most visited item (and most time spend on item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90290350-ae00-459c-9423-628be6190068",
   "metadata": {},
   "source": [
    "### FOR MOST VISITED ITEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bf5ec8a-71e2-4111-80ee-2e97056b8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the most revisited item (clustered) with the bought items (clustered)\n",
    "temp_clusters = (train_purchases\n",
    "                 .rdd.map(lambda x: (x[0],(item_cluster_dict.get(x[1]))))\n",
    "                 .join(revisited_clustered.map(lambda x: (x[0],(x[1][0]))))\n",
    "                ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "435ee932-4507-4116-88f7-49df3d603c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 09:24:20 WARN DAGScheduler: Broadcasting large task binary with size 1045.3 KiB\n",
      "22/05/27 09:24:44 WARN BlockManager: Task 229 already completed, not releasing lock for rdd_254_0\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, (5, 17))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_clusters.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90f46e78-1fc5-4f50-9011-6a52fbbc7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clusters_m = (\n",
    "    temp_clusters.map(lambda x : (x[1][1],(x[1][0],)))\n",
    "    .reduceByKey(lambda x,y : x+y)\n",
    ")# keep only most_visited cluster and bought cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55da05c0-2191-4cce-8e9a-da0d1853c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_clusters_m.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2168736b-6077-41fa-9e93-19f5b75afdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_and_least(row):\n",
    "    vals, counts = np.unique(np.array(row), return_counts=True)\n",
    "    mode_value = np.argwhere(counts == np.max(counts))[0][0]\n",
    "    least_value = np.argwhere(counts == np.min(counts))[0][0]\n",
    "    return (row[mode_value],row[least_value])\n",
    "\n",
    "temp_most_visited = temp_clusters_m.map(lambda x : (x[0], mode_and_least(x[1]))) # mapping : most visited clusted id -->TO--> most and least bought (cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd65d579-58d3-4cfe-b0d4-7d48c36bf05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_visited_dict = {k: v for k, v in temp_most_visited.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "466a248a-6f4e-493f-8a8a-06542dc4cd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_visited_dict.get(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abc5079e-f225-4eb0-8222-c1f034f1077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "revisited_clustered = revisited_clustered.map(lambda x: (x[0],x[1]+most_visited_dict.get(x[1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "922908f3-18a2-4b57-82b8-49e07852fe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, (17, 2, 1, 12, 28))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revisited_clustered.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d07101-2c77-4fac-95f1-870a438fea41",
   "metadata": {},
   "source": [
    "### FOR MOST TIME SPENT ON ITEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdfe234b-eb49-4bee-97f6-a043d41c0183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 09:26:07 WARN DAGScheduler: Broadcasting large task binary with size 1047.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, (5, 5))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the most revisited item (clustered) with the bought items (clustered)\n",
    "temp_clusters = (train_purchases\n",
    "                 .rdd.map(lambda x: (x[0],(item_cluster_dict.get(x[1]))))\n",
    "                 .join(most_spent_time_clustered.map(lambda x: (x[0],(x[1][0]))))\n",
    "                ).cache()\n",
    "temp_clusters.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09ac0459-13b1-4612-a5ff-f2e792537607",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clusters_m = (\n",
    "    temp_clusters.map(lambda x : (x[1][1],(x[1][0],)))\n",
    "    .reduceByKey(lambda x,y : x+y)\n",
    ").cache()# keep only most_visited cluster and bought cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a396751-8a0e-4579-a482-366615414693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_clusters_m.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f147304b-1dc1-450b-add0-0f10e36b50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_most_time = temp_clusters_m.map(lambda x : (x[0], mode_and_least(x[1]))) # mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b15284d-22d7-4be6-aec5-e30af689ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_most_time.take(10)\n",
    "\n",
    "most_time_dict = {k: v for k, v in temp_most_time.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "916406f9-8646-4b34-9845-6e1ec08df7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_spent_time_clustered = most_spent_time_clustered.map(lambda x: (x[0],x[1]+most_time_dict.get(x[1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e606a889-5a84-415d-b301-aa5f51486f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, (5, 3096, 22, 2))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_spent_time_clustered.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af26d9-bbe1-4159-aa51-d6e832195065",
   "metadata": {},
   "source": [
    "# Save the final dataset\n",
    "\n",
    "The final engineered dataset, containing all the values, is created by joining the other engineered value datasets with the session_id as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ace972bd-5610-4581-bc1b-355b1a3874d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_features_session.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3af999e-cd49-4d07-8e16-29ed439edffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(28560,\n",
       "  [272,\n",
       "   3,\n",
       "   2,\n",
       "   8,\n",
       "   0,\n",
       "   26,\n",
       "   73,\n",
       "   26,\n",
       "   7,\n",
       "   47.5,\n",
       "   19.94785,\n",
       "   -1,\n",
       "   -1,\n",
       "   0,\n",
       "   39,\n",
       "   22,\n",
       "   31,\n",
       "   26,\n",
       "   1,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.16666667,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.16666667,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.6666667,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unpack_tuples(row):\n",
    "    out = []\n",
    "    for elem in row:\n",
    "        if type(elem) == tuple:\n",
    "            out.extend(unpack_tuples(elem))\n",
    "        else:\n",
    "            out.append(elem)\n",
    "    return out\n",
    "\n",
    "final_engineered_dataset = (\n",
    "    time_sessions_in_seconds\n",
    "    .join(season_per_session)\n",
    "    .join(day_period_per_session)\n",
    "    .join(month_per_session)\n",
    "    .join(year_per_session)\n",
    "    .join(most_spent_time_clustered)\n",
    "    .join(mean_std_time_session)\n",
    "    .join(revisited_clustered)\n",
    "    .join(first_last_item_clustered)\n",
    "    .join(clustered_categories)\n",
    "    .join(item_features_session)\n",
    "    .mapValues(unpack_tuples)\n",
    ")\n",
    "\n",
    "\n",
    "final_engineered_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d1e55-bc14-4064-ac00-f7b58c5c8619",
   "metadata": {},
   "source": [
    "0 - session id (int)<br>\n",
    "<br>\n",
    "1 - session total time (float)<br>\n",
    "<br>\n",
    "2 - day period (int), 4 categories 0,..,3<br>\n",
    "3 - season (int), 4 categories 0,..,3<br>\n",
    "4 - month (int), 12 categories 0,..,11<br>\n",
    "5 - year (int), 2 categories 0,1<br>\n",
    "<br>\n",
    "6 - item with most time spent on (int), 50 categories<br>\n",
    "7 - most time spent on an item (float) <br>\n",
    "8 - most frequently bought item for given most time spent on item (int), 50 categories<br>\n",
    "9 - least \"\"<br>\n",
    "<br>\n",
    "10 - mean time per item (float)<br>\n",
    "11 - std time per item (float)<br>\n",
    "<br>\n",
    "12 - item revisited the most (int), 50 categories<br>\n",
    "13 - number of time it has been revisited (int)<br>\n",
    "14 - number of revisited items (int)<br>\n",
    "15 - most frequently bought item when item, when X is the most revisited item (int), 50 categories<br>\n",
    "16 - least \"\"<br>\n",
    "<br>\n",
    "17 - first visited item (int), 50 categories<br>\n",
    "18 - last \"\"<br>\n",
    "<br>\n",
    "19 - normalized feature category vector (int), 10 categories<br>\n",
    "<br>\n",
    "20 -> 44 - normalized feature vector (float)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85390fc1-c1c9-4a4f-8fa8-3977341e576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_final_engineered_dataset = (\n",
    "    final_engineered_dataset.map(lambda x: ((x[0],) + tuple(x[1])))\n",
    ")\n",
    "\n",
    "#tuple_final_engineered_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f80bd58d-deda-475a-8b26-189c2b407a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['session_id', 'session_time', 'season', 'day_period', 'month', 'year', 'item_most_time_spent', 'most_time_spent_on_item', 'most_frequently_bought_for_time_spent', 'least_frequently_bought_for_time_spent', 'mean_time', 'std_time', 'item_most_visited', 'number_o_visit', 'number_o_revisited_items', 'most_frequently_bought_for_most_revisited', 'first_item_visited', 'last_item_visited', 'normalized_features_vector', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25']\n"
     ]
    }
   ],
   "source": [
    "columns = ['session_id', 'session_time', 'season', 'day_period', 'month', 'year', 'item_most_time_spent', 'most_time_spent_on_item', 'most_frequently_bought_for_time_spent', \n",
    "          'least_frequently_bought_for_time_spent', 'mean_time', 'std_time', 'item_most_visited', 'number_o_visit', 'number_o_revisited_items', 'most_frequently_bought_for_most_revisited',\n",
    "          'first_item_visited', 'last_item_visited', 'normalized_features_vector']\n",
    "\n",
    "columns += [str(i+1) for i in range(25)]\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "033f888b-f6fb-4267-81e1-57feab081ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 09:32:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path file:/PROJ/Data/session_engineered_features.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106349/1636014255.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuple_final_engineered_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/PROJ/Data/session_engineered_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    953\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/PROJ/Data/session_engineered_features.csv already exists."
     ]
    }
   ],
   "source": [
    "tuple_final_engineered_dataset.map(lambda x : tuple([float(xi) for xi in x])).coalesce(1).toDF(columns).write.option(\"header\",True).csv('/PROJ/Data/session_engineered_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e8088-d5f1-4c73-a4de-915d1f234eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.csv('../Data/session_engineered_features.csv',header=False,\n",
    "                                          inferSchema=True)\n",
    "summary = test.describe().toPandas().set_index('summary').transpose()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0540df83-0516-4e72-817d-7efcd1ae820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves all the clusters array inside the data folder\n",
    "import pickle\n",
    "\n",
    "with open('../Data/clusters/feature_category_clusters.np', 'wb') as file:\n",
    "    pickle.dump(feature_category_clusters, file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../Data/clusters/feature_values_clusters.np', 'wb') as file:\n",
    "    pickle.dump(feature_values_clusters, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../Data/clusters/item_cat_clusters.np', 'wb') as file:\n",
    "    pickle.dump(item_cat_clusters, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70de1fcf-320a-43f1-bd79-f6465ad1afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/most_visited_dict.pd', 'wb') as file:\n",
    "    pickle.dump(most_visited_dict, file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../Data/most_time_dict.pd', 'wb') as file:\n",
    "    pickle.dump(most_time_dict, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51951a-c396-4890-a359-71add710c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
