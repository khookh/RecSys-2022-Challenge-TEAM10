{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ec86f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:34:28 WARN Utils: Your hostname, pierre-hp resolves to a loopback address: 127.0.1.1; using 192.168.0.194 instead (on interface eno1)\n",
      "22/05/22 17:34:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/22 17:34:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"feature_selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f14d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_sessions_engineered = spark.read.csv('../dressipi_recsys2022/session_engineered_features.txt',header=False,\n",
    "                                          inferSchema=True)\n",
    "\n",
    "train_purchases = spark.read.load('../dressipi_recsys2022/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abbb085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:26:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.1200000e+02 0.0000000e+00 1.6300000e+02 ... 0.0000000e+00\n",
      "  4.3600000e+02 2.5091000e+04]\n",
      " [3.0000000e+00 0.0000000e+00 2.0000000e+00 ... 3.0000000e+00\n",
      "  3.0000000e+00 3.0000000e+00]\n",
      " [5.0000000e+00 4.0000000e+00 4.0000000e+00 ... 5.0000000e+00\n",
      "  2.0000000e+00 4.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  1.4285715e-01 0.0000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:26:38 WARN TaskSetManager: Stage 9 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:26:39 WARN TaskSetManager: Stage 10 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:26:40 WARN TaskSetManager: Stage 11 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([  312.,     0.,   163., ...,     0.,   436., 25091.]),\n",
       " array([3., 0., 2., ..., 3., 3., 3.]),\n",
       " array([5., 4., 4., ..., 5., 2., 4.]),\n",
       " array([12.,  3.,  8., ..., 11., 11., 10.]),\n",
       " array([2020., 2020., 2020., ..., 2020., 2020., 2020.]),\n",
       " array([ 9655., 15654., 18316., ..., 25357., 15853., 27400.]),\n",
       " array([  312.,    48.,   121., ...,   111.,   132., 24039.]),\n",
       " array([ 209.5     ,   48.      ,   94.333336, ...,  111.      ,\n",
       "          80.71429 , 1337.7368  ]),\n",
       " array([ 102.5    ,    0.     ,   36.30733, ...,    0.     ,   38.9788 ,\n",
       "        5351.505  ]),\n",
       " array([ 9.6550e+03, -1.0000e+00, -1.0000e+00, ..., -1.0000e+00,\n",
       "        -1.0000e+00,  2.5273e+04])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by session\n",
    "X = train_sessions_engineered.orderBy('_c0') \n",
    "X = X.drop('_c0')\n",
    "X.take(10)\n",
    "n_total_features = len(X.columns)\n",
    "\n",
    "X_np = np.array(X.collect())\n",
    "t_X = X_np.transpose()\n",
    "\n",
    "print(t_X)\n",
    "# we want the nb partition to be between 2 or 3 times more than the number of core in our computer\n",
    "Nb_partition=10\n",
    "X_RDD = sc.parallelize(t_X,Nb_partition)\n",
    "\n",
    "\n",
    "# Counting the number of rows will allow to implicitly cache the data\n",
    "print(X_RDD.count())\n",
    "X_RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8462e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15085 18626 24911 ... 21630 16962 16631]\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # get Y output from the training set sorted by session id\n",
    "train_purchases_without_date = train_purchases.drop('date')\n",
    "\n",
    "train_purchases_reformated = train_purchases_without_date.withColumnRenamed(\"item_id\", \"item_id_purchased\")\n",
    "\n",
    "Y_order_by_session_id = train_purchases_reformated.drop('session_id')\n",
    "\n",
    "\n",
    "\n",
    "Y_order_by_session_id.take(10)\n",
    "Y_numpy = np.array(Y_order_by_session_id.collect())\n",
    "Y_numpy = Y_numpy.flatten()\n",
    "print(Y_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aadab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.count() == len(Y_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c77a9d3",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "After the feature engineering, we could have a lot of features which are not really worth to give to the predicting model. Considering this problem, we want to use some feature selection algorithms to take the feature which are the most interesting to the model.\n",
    "\n",
    "We were asked to implement two scalable feature selection algorithms, a ranking algorithm and a forward feature selection.\n",
    "\n",
    "We could be interested in Minimum-redundancy-maximum-relevance (mRMR) feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30368934",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def generate_dataset(n_samples=100, n_informative=1, n_noisy=2, n_redundant=1, random_seed=0):\n",
    "    \"\"\"\n",
    "    generate a dataset to test\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Use sklearn.datasets.make_regression to generate an artificial dataset where the output y \n",
    "    # is correlated with a subset of of the input features\n",
    "    X, Y = sklearn.datasets.make_classification(n_samples=n_samples, \n",
    "                                            n_features=n_informative+n_noisy, \n",
    "                                            n_informative=n_informative)\n",
    "    \n",
    "    # Create a random mixing matrix for generating redundant features from informative ones\n",
    "    mixing_matrix = np.random.random((n_informative, n_redundant))\n",
    "    \n",
    "    # Create redundant features by taking random linear combinations of informative features\n",
    "    redundant_features = np.dot(X[:,0:n_informative], mixing_matrix)\n",
    "    \n",
    "    # Add redundant features to the input data\n",
    "    X = np.concatenate((X, redundant_features), axis=1)\n",
    "    \n",
    "    # Return input data X, output data Y\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Let us generate the dataset\n",
    "N = 1000\n",
    "n_informative = 100\n",
    "n_noisy = 100\n",
    "n_redundant = 100\n",
    "\n",
    "X, Y = generate_dataset(n_samples=N, \n",
    "                        n_informative=n_informative, \n",
    "                        n_noisy=n_noisy, \n",
    "                        n_redundant=n_redundant)\n",
    "\n",
    "t_X = np.transpose(X)\n",
    "\n",
    "B=3\n",
    "X_RDD=sc.parallelize(t_X,B).cache()\n",
    "\n",
    "# Counting the number of rows will allow to implicitly cache the data\n",
    "X_RDD.count()\n",
    "X_RDD.take(10)\n",
    "\n",
    "print(type(Y))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905cec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_mrmr(x, y):\n",
    "    \"\"\"\n",
    "    return the correlation value between two variable in absolute value\n",
    "    \"\"\"\n",
    "\n",
    "    return np.abs(scipy.stats.pearsonr(x, y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf9570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrmr_score_spark(x,selected_features):\n",
    "    \"\"\"\n",
    "    x : the feature to evaluate to add into the model\n",
    "    y : output value\n",
    "    selected_features : the features already selected\n",
    "    return the score for x with the rest of selected variables\n",
    "    \"\"\"\n",
    "    \n",
    "    y = broadcast_Y.value\n",
    "    # Get correlation score between feature x and output y (relevance)\n",
    "    score_x_y_s = get_score_mrmr(x, y)\n",
    "    \n",
    "    \n",
    "    nb_selected_features = selected_features.shape[0]\n",
    "    # If some features have already been selected\n",
    "    if nb_selected_features > 0:\n",
    "        \n",
    "        # Get corrrelation scores between x and each feature already selected (redundancy)\n",
    "        score_features_x_s = np.zeros(nb_selected_features, dtype=float)\n",
    "        \n",
    "        for j in range(nb_selected_features):\n",
    "                \n",
    "            score_x_s_j = get_score_mrmr(x, selected_features[j,:])\n",
    "                \n",
    "            score_features_x_s[j] = score_x_s_j\n",
    "                \n",
    "        # Final score is relevance to output feature - average redundancy with already selected features\n",
    "        score_x_y_s = score_x_y_s - np.mean(score_features_x_s)\n",
    "        \n",
    "    return score_x_y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded7f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrmr_spark(n_total_features, K, sc, X_RDD):\n",
    "    \"\"\"\n",
    "    n_total_features : number of total features\n",
    "    K : number of feature to select\n",
    "    sc : spark context\n",
    "    X_RDD : RDD of the variable X\n",
    "    Y: Output data\n",
    "    \n",
    "    return the indice of selected features and time execution using mrmr\n",
    "    \"\"\"\n",
    "    time_execution = []\n",
    "    remaining_features_indices = list(range(n_total_features))\n",
    "    selected_features_indices = []\n",
    "    \n",
    "\n",
    "    for k in range(K):\n",
    "        print(\"Step: \"+str(k))\n",
    "    \n",
    "        start_time=time.time()\n",
    "        # Get the subset of selected features values, and cast as an array\n",
    "        selected_features = X_RDD.zipWithIndex().filter(lambda x: x[1] in selected_features_indices).map(lambda x: x[0]).collect()\n",
    "        selected_features = np.array(selected_features)\n",
    "    \n",
    "        # mRMR scores are computed by first filtering `t_X` to remove already selected features, and then mapping \n",
    "        # each remaining feature using the `get_mrmr_score_spark` function\n",
    "        scores = X_RDD.zipWithIndex().filter(lambda x: x[1] in remaining_features_indices).map(lambda x:get_mrmr_score_spark(x[0],selected_features)).collect()\n",
    "    \n",
    "        # Once all mRMR scores are computed, the index of the feature with the highest score is selected\n",
    "        scores = np.array(scores)\n",
    "    \n",
    "        index_max_score_features = np.argmax(scores)\n",
    "    \n",
    "        selected_features_indices.append(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        del(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        print(time.time()-start_time)\n",
    "        time_execution.append(time.time()-start_time)\n",
    "        \n",
    "    return selected_features_indices, time_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d803b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:26:59 WARN TaskSetManager: Stage 17 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:01 WARN TaskSetManager: Stage 18 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:01 WARN TaskSetManager: Stage 19 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:02 WARN TaskSetManager: Stage 20 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "22/05/22 17:27:04 WARN TaskSetManager: Stage 21 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.481203556060791\n",
      "Step: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:27:05 WARN TaskSetManager: Stage 22 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:06 WARN TaskSetManager: Stage 23 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:07 WARN TaskSetManager: Stage 24 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "22/05/22 17:27:08 WARN TaskSetManager: Stage 25 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9187705516815186\n",
      "Step: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:27:09 WARN TaskSetManager: Stage 26 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:10 WARN TaskSetManager: Stage 27 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:11 WARN TaskSetManager: Stage 28 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:12 WARN TaskSetManager: Stage 29 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.583209037780762\n",
      "Step: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:27:13 WARN TaskSetManager: Stage 30 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:14 WARN TaskSetManager: Stage 31 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:15 WARN TaskSetManager: Stage 32 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:17 WARN TaskSetManager: Stage 33 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.946712017059326\n",
      "Step: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:27:18 WARN TaskSetManager: Stage 34 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:19 WARN TaskSetManager: Stage 35 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:20 WARN TaskSetManager: Stage 36 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 36:=======================================>                 (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.216177225112915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:===================================================>     (9 + 1) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "broadcast_Y = sc.broadcast(Y_numpy)\n",
    "selected_features_indices, execution_time = mrmr_spark(n_total_features, 5, sc,X_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc4fd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 0, 1, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2f0e8",
   "metadata": {},
   "source": [
    "Then, we will look a the forward feature Selection by training on a decision tree with every feature and add the features which get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f884d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fit_get_score(x,selected_features):\n",
    "    \"\"\"\n",
    "    train an model to get the score with the addition of a specified x feature\n",
    "    \"\"\"    \n",
    "    \n",
    "    y = broadcast_Y.value\n",
    "    n_selected_features = selected_features.shape[0]\n",
    "    \n",
    "    if n_selected_features>0:\n",
    "        # need to merge x and the already selected features\n",
    "        x = np.vstack((selected_features,x))\n",
    "        x = x.transpose()\n",
    "    else:\n",
    "        x = x.reshape(-1, 1)\n",
    "        \n",
    "    \n",
    "    #split data\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y, test_size= 0.01)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train on training data\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model = model.fit(x_train,y_train)\n",
    "    \n",
    "    print(x_test.shape)\n",
    "    \n",
    "    # get score on testing set\n",
    "    return model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035c9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_feature_selection(n_total_features, K, sc, X_RDD):\n",
    "    \"\"\"\n",
    "    n_total_features : number of total features\n",
    "    K : number of feature to select\n",
    "    sc : spark context\n",
    "    X_RDD : RDD of the variable X\n",
    "    Y: Output data\n",
    "    \n",
    "    return the indice of selected features and time execution\n",
    "    by using a decision tree as model to calculate the score\n",
    "    \"\"\"\n",
    "    time_execution = []\n",
    "    \n",
    "    remaining_features_indices = list(range(n_total_features))\n",
    "    selected_features_indices = []\n",
    "    \n",
    "    for k in range(K):\n",
    "        print(\"Step: \"+str(k))\n",
    "    \n",
    "        start_time=time.time()\n",
    "\n",
    "        # Get the subset of selected features values, and cast as an array\n",
    "        selected_features = X_RDD.zipWithIndex().filter(lambda x: x[1] in selected_features_indices).map(lambda x: x[0]).collect()\n",
    "        selected_features = np.array(selected_features)\n",
    "        \n",
    "    \n",
    "        #  scores for a certain model are computed by first filtering `t_X` to remove already selected features, and then mapping \n",
    "        # each remaining feature using the `fit_get_score` function\n",
    "        scores = X_RDD.zipWithIndex().filter(lambda x: x[1] in remaining_features_indices).map(lambda x:fit_get_score(x[0],selected_features)).collect()\n",
    "    \n",
    "        # Once all scores are computed, the index of the feature with the highest value is chosen\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        print(\"best_score :\", np.max(scores))\n",
    "    \n",
    "        index_max_score_features = np.argmax(scores)\n",
    "    \n",
    "        selected_features_indices.append(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        del(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        print(time.time()-start_time)\n",
    "        time_execution.append(time.time()-start_time)\n",
    "        \n",
    "    return selected_features_indices, time_execution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f153a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 17:27:37 WARN TaskSetManager: Stage 37 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:38 WARN TaskSetManager: Stage 38 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:39 WARN TaskSetManager: Stage 39 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/22 17:27:40 WARN TaskSetManager: Stage 40 contains a task of very large size (23442 KiB). The maximum recommended task size is 1000 KiB.\n",
      "(10000, 1)>                                                        (0 + 8) / 10]\n",
      "(10000, 1)\n",
      "(10000, 1)\n",
      "(10000, 1)\n",
      "(10000, 1)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47190)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selected_features_indices_forward, execution_time_forward \u001b[38;5;241m=\u001b[39m \u001b[43mforward_feature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_total_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_RDD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m selected_features_indices_forward\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mforward_feature_selection\u001b[0;34m(n_total_features, K, sc, X_RDD)\u001b[0m\n\u001b[1;32m     24\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(selected_features)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#  scores for a certain model are computed by first filtering `t_X` to remove already selected features, and then mapping \u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# each remaining feature using the `fit_get_score` function\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mX_RDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzipWithIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremaining_features_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mfit_get_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Once all scores are computed, the index of the feature with the highest value is chosen\u001b[39;00m\n\u001b[1;32m     32\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py:949\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03m    Return a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03m    to be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 949\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[1;32m    950\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/traceback_utils.py:78\u001b[0m, in \u001b[0;36mSCCallSiteSync.__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     76\u001b[0m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:281\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:288\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:402\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "selected_features_indices_forward, execution_time_forward = forward_feature_selection(n_total_features, 5, sc,X_RDD)\n",
    "selected_features_indices_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e2f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_feature_selection_diff(n_total_features, inc_accuracy, sc, X_RDD):\n",
    "    \"\"\"\n",
    "    n_total_features : number of total features\n",
    "    inc_accuracy : increase of accuracy needed to continue the algorithm\n",
    "    sc : spark context\n",
    "    X_RDD : RDD of the variable X\n",
    "    Y: Output data\n",
    "    \n",
    "    return the indice of selected features and time execution\n",
    "    by using a decision tree as model to calculate the score\n",
    "    \"\"\"\n",
    "    time_execution = []\n",
    "    \n",
    "    remaining_features_indices = list(range(n_total_features))\n",
    "    selected_features_indices = []\n",
    "    \n",
    "    last_best_score = 0\n",
    "    diff_accuracy = 1\n",
    "    k = 0\n",
    "    while diff_accuracy > inc_accuracy:\n",
    "        print(\"Step: \"+str(k))\n",
    "    \n",
    "        start_time=time.time()\n",
    "\n",
    "        # Get the subset of selected features values, and cast as an array\n",
    "        selected_features = X_RDD.zipWithIndex().filter(lambda x: x[1] in selected_features_indices).map(lambda x: x[0]).collect()\n",
    "        selected_features = np.array(selected_features)\n",
    "        \n",
    "    \n",
    "        #  scores for a certain model are computed by first filtering `t_X` to remove already selected features, and then mapping \n",
    "        # each remaining feature using the `fit_get_score` function\n",
    "        scores = X_RDD.zipWithIndex().filter(lambda x: x[1] in remaining_features_indices).map(lambda x:fit_get_score(x[0],selected_features)).collect()\n",
    "    \n",
    "        # Once all scores are computed, the index of the feature with the highest value is chosen\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        # compute the difference between last result and new result\n",
    "        best_score = np.max(scores)\n",
    "        diff_accuracy = best_score - last_best_score\n",
    "        \n",
    "        \n",
    "        print(\"best_accuracy:\", best_score)\n",
    "        \n",
    "        if best_score > last_best_score:\n",
    "        \n",
    "            index_max_score_features = np.argmax(scores)\n",
    "    \n",
    "            selected_features_indices.append(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "            del(remaining_features_indices[index_max_score_features])\n",
    "        \n",
    "        last_best_score = best_score\n",
    "        print(time.time()-start_time)\n",
    "        time_execution.append(time.time()-start_time)\n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "    return selected_features_indices, time_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_indices_forward_diff, execution_time_forward_diff = forward_feature_selection_diff(n_total_features, 0.001, sc,X_RDD)\n",
    "selected_features_indices_forward_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1567b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1d6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
