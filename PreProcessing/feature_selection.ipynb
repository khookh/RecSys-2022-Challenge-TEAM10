{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ec86f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:12:27 WARN Utils: Your hostname, pierre-hp resolves to a loopback address: 127.0.1.1; using 192.168.0.194 instead (on interface eno1)\n",
      "22/05/21 17:12:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/21 17:12:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"8G\") \\\n",
    "    .appName(\"feature_selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f14d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_sessions_engineered = spark.read.csv('../dressipi_recsys2022/session_engineered_features.txt',header=False,\n",
    "                                          inferSchema=True)\n",
    "\n",
    "train_purchases = spark.read.load('../dressipi_recsys2022/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2abbb085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.1200000e+02 3.0000000e+00 5.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 4.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [1.6300000e+02 2.0000000e+00 4.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 3.0000000e+00 5.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [4.3600000e+02 3.0000000e+00 2.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 1.4285715e-01]\n",
      " [2.5091000e+04 3.0000000e+00 4.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "[[3.1200000e+02 0.0000000e+00 1.6300000e+02 ... 0.0000000e+00\n",
      "  4.3600000e+02 2.5091000e+04]\n",
      " [3.0000000e+00 0.0000000e+00 2.0000000e+00 ... 3.0000000e+00\n",
      "  3.0000000e+00 3.0000000e+00]\n",
      " [5.0000000e+00 4.0000000e+00 4.0000000e+00 ... 5.0000000e+00\n",
      "  2.0000000e+00 4.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  1.4285715e-01 0.0000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:20:17 WARN TaskSetManager: Stage 48 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:20:17 WARN TaskSetManager: Stage 49 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:18 WARN TaskSetManager: Stage 50 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([  312.,     0.,   163., ...,     0.,   436., 25091.]),\n",
       " array([3., 0., 2., ..., 3., 3., 3.]),\n",
       " array([5., 4., 4., ..., 5., 2., 4.]),\n",
       " array([12.,  3.,  8., ..., 11., 11., 10.]),\n",
       " array([2020., 2020., 2020., ..., 2020., 2020., 2020.]),\n",
       " array([ 9655., 15654., 18316., ..., 25357., 15853., 27400.]),\n",
       " array([  312.,    48.,   121., ...,   111.,   132., 24039.]),\n",
       " array([ 209.5     ,   48.      ,   94.333336, ...,  111.      ,\n",
       "          80.71429 , 1337.7368  ]),\n",
       " array([ 102.5    ,    0.     ,   36.30733, ...,    0.     ,   38.9788 ,\n",
       "        5351.505  ]),\n",
       " array([ 9.6550e+03, -1.0000e+00, -1.0000e+00, ..., -1.0000e+00,\n",
       "        -1.0000e+00,  2.5273e+04])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by session\n",
    "X = train_sessions_engineered.orderBy('_c0') \n",
    "X = X.drop('_c0')\n",
    "X.take(10)\n",
    "n_total_features = len(X.columns)\n",
    "\n",
    "X_np = np.array(X.collect())\n",
    "print(X_np)\n",
    "t_X = X_np.transpose()\n",
    "\n",
    "print(t_X)\n",
    "B=5\n",
    "X_RDD = sc.parallelize(t_X,B)\n",
    "\n",
    "#X_RDD = X.rdd\n",
    "\n",
    "\n",
    "# Counting the number of rows will allow to implicitly cache the data\n",
    "print(X_RDD.count())\n",
    "X_RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8462e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:====================================>                     (5 + 3) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15085 18626 24911 ... 21630 16962 16631]\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # get Y output from the training set sorted by session id\n",
    "train_purchases_without_date = train_purchases.drop('date')\n",
    "\n",
    "train_purchases_reformated = train_purchases_without_date.withColumnRenamed(\"item_id\", \"item_id_purchased\")\n",
    "\n",
    "Y_order_by_session_id = train_purchases_reformated.drop('session_id')\n",
    "\n",
    "\n",
    "\n",
    "Y_order_by_session_id.take(10)\n",
    "Y_numpy = np.array(Y_order_by_session_id.collect())\n",
    "Y_numpy = Y_numpy.flatten()\n",
    "print(Y_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aadab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.count() == len(Y_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c77a9d3",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "After the feature engineering, we could have a lot of features which are not really worth to give to the predicting model. Considering this problem, we want to use some feature selection algorithms to take the feature which are the most interesting to the model.\n",
    "\n",
    "We were asked to implement two scalable feature selection algorithms, a ranking algorithm and a forward feature selection.\n",
    "\n",
    "We could be interested in Minimum-redundancy-maximum-relevance (mRMR) feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30368934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generate_dataset(n_samples=100, n_informative=1, n_noisy=2, n_redundant=1, random_seed=0):\\n    \"\"\"\\n    generate a dataset to test\\n    \"\"\"\\n    # Set random seed\\n    np.random.seed(random_seed)\\n    \\n    # Use sklearn.datasets.make_regression to generate an artificial dataset where the output y \\n    # is correlated with a subset of of the input features\\n    X, Y = sklearn.datasets.make_classification(n_samples=n_samples, \\n                                            n_features=n_informative+n_noisy, \\n                                            n_informative=n_informative)\\n    \\n    # Create a random mixing matrix for generating redundant features from informative ones\\n    mixing_matrix = np.random.random((n_informative, n_redundant))\\n    \\n    # Create redundant features by taking random linear combinations of informative features\\n    redundant_features = np.dot(X[:,0:n_informative], mixing_matrix)\\n    \\n    # Add redundant features to the input data\\n    X = np.concatenate((X, redundant_features), axis=1)\\n    \\n    # Return input data X, output data Y\\n    return X, Y\\n\\n\\n# Let us generate the dataset\\nN = 1000\\nn_informative = 100\\nn_noisy = 100\\nn_redundant = 100\\n\\nX, Y = generate_dataset(n_samples=N, \\n                        n_informative=n_informative, \\n                        n_noisy=n_noisy, \\n                        n_redundant=n_redundant)\\n\\nt_X = np.transpose(X)\\n\\nB=3\\nX_RDD=sc.parallelize(t_X,B).cache()\\n\\n# Counting the number of rows will allow to implicitly cache the data\\nX_RDD.count()\\nX_RDD.take(10)\\n\\nprint(type(Y))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def generate_dataset(n_samples=100, n_informative=1, n_noisy=2, n_redundant=1, random_seed=0):\n",
    "    \"\"\"\n",
    "    generate a dataset to test\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Use sklearn.datasets.make_regression to generate an artificial dataset where the output y \n",
    "    # is correlated with a subset of of the input features\n",
    "    X, Y = sklearn.datasets.make_classification(n_samples=n_samples, \n",
    "                                            n_features=n_informative+n_noisy, \n",
    "                                            n_informative=n_informative)\n",
    "    \n",
    "    # Create a random mixing matrix for generating redundant features from informative ones\n",
    "    mixing_matrix = np.random.random((n_informative, n_redundant))\n",
    "    \n",
    "    # Create redundant features by taking random linear combinations of informative features\n",
    "    redundant_features = np.dot(X[:,0:n_informative], mixing_matrix)\n",
    "    \n",
    "    # Add redundant features to the input data\n",
    "    X = np.concatenate((X, redundant_features), axis=1)\n",
    "    \n",
    "    # Return input data X, output data Y\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Let us generate the dataset\n",
    "N = 1000\n",
    "n_informative = 100\n",
    "n_noisy = 100\n",
    "n_redundant = 100\n",
    "\n",
    "X, Y = generate_dataset(n_samples=N, \n",
    "                        n_informative=n_informative, \n",
    "                        n_noisy=n_noisy, \n",
    "                        n_redundant=n_redundant)\n",
    "\n",
    "t_X = np.transpose(X)\n",
    "\n",
    "B=3\n",
    "X_RDD=sc.parallelize(t_X,B).cache()\n",
    "\n",
    "# Counting the number of rows will allow to implicitly cache the data\n",
    "X_RDD.count()\n",
    "X_RDD.take(10)\n",
    "\n",
    "print(type(Y))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905cec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_mrmr(x, y):\n",
    "    \"\"\"\n",
    "    return the correlation value between two variable in absolute value\n",
    "    \"\"\"\n",
    "\n",
    "    return np.abs(scipy.stats.pearsonr(x, y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf9570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrmr_score_spark(x,y,selected_features):\n",
    "    \"\"\"\n",
    "    x : the feature to evaluate to add into the model\n",
    "    y : output value\n",
    "    selected_features : the features already selected\n",
    "    return the score for x with the rest of selected variables\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get correlation score between feature x and output y (relevance)\n",
    "    score_x_y_s = get_score_mrmr(x, y)\n",
    "    \n",
    "    nb_selected_features = selected_features.shape[0]\n",
    "    # If some features have already been selected\n",
    "    if nb_selected_features > 0:\n",
    "        \n",
    "        # Get corrrelation scores between x and each feature already selected (redundancy)\n",
    "        score_features_x_s = np.zeros(nb_selected_features, dtype=float)\n",
    "        \n",
    "        for j in range(nb_selected_features):\n",
    "                \n",
    "            score_x_s_j = get_score_mrmr(x, selected_features[j,:])\n",
    "                \n",
    "            score_features_x_s[j] = score_x_s_j\n",
    "                \n",
    "        # Final score is relevance to output feature - average redundancy with already selected features\n",
    "        score_x_y_s = score_x_y_s - np.mean(score_features_x_s)\n",
    "        \n",
    "    return score_x_y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded7f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrmr_spark(n_total_features, K, sc, X_RDD, Y):\n",
    "    \"\"\"\n",
    "    n_total_features : number of total features\n",
    "    K : number of feature to select\n",
    "    sc : spark context\n",
    "    X_RDD : RDD of the variable X\n",
    "    Y: Output data\n",
    "    \n",
    "    return the indice of selected features and time execution using mrmr\n",
    "    \"\"\"\n",
    "    time_execution = []\n",
    "    remaining_features_indices = list(range(n_total_features))\n",
    "    selected_features_indices = []\n",
    "\n",
    "    for k in range(K):\n",
    "        print(\"Step: \"+str(k))\n",
    "    \n",
    "        start_time=time.time()\n",
    "        # Get the subset of selected features values, and cast as an array\n",
    "        selected_features = X_RDD.zipWithIndex().filter(lambda x: x[1] in selected_features_indices).map(lambda x: x[0]).collect()\n",
    "        selected_features = np.array(selected_features)\n",
    "    \n",
    "        # mRMR scores are computed by first filtering `t_X` to remove already selected features, and then mapping \n",
    "        # each remaining feature using the `get_mrmr_score_spark` function\n",
    "        scores = X_RDD.zipWithIndex().filter(lambda x: x[1] in remaining_features_indices).map(lambda x:get_mrmr_score_spark(x[0],Y,selected_features)).collect()\n",
    "    \n",
    "        # Once all mRMR scores are computed, the index of the feature with the highest score is selected\n",
    "        scores = np.array(scores)\n",
    "    \n",
    "        index_max_score_features = np.argmax(scores)\n",
    "    \n",
    "        selected_features_indices.append(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        del(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        print(time.time()-start_time)\n",
    "        time_execution.append(time.time()-start_time)\n",
    "        \n",
    "    return selected_features_indices, time_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d803b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:20:34 WARN TaskSetManager: Stage 51 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:35 WARN TaskSetManager: Stage 52 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:35 WARN TaskSetManager: Stage 53 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:36 WARN TaskSetManager: Stage 54 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "22/05/21 17:20:37 WARN TaskSetManager: Stage 55 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7156221866607666\n",
      "Step: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:20:38 WARN TaskSetManager: Stage 56 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:39 WARN TaskSetManager: Stage 57 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:39 WARN TaskSetManager: Stage 58 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/pdefraene/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "22/05/21 17:20:41 WARN TaskSetManager: Stage 59 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.279282569885254\n",
      "Step: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:20:41 WARN TaskSetManager: Stage 60 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:42 WARN TaskSetManager: Stage 61 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:43 WARN TaskSetManager: Stage 62 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:45 WARN TaskSetManager: Stage 63 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9825942516326904\n",
      "Step: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:20:45 WARN TaskSetManager: Stage 64 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:46 WARN TaskSetManager: Stage 65 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:47 WARN TaskSetManager: Stage 66 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.119117259979248\n",
      "Step: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:20:49 WARN TaskSetManager: Stage 67 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:49 WARN TaskSetManager: Stage 68 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:50 WARN TaskSetManager: Stage 69 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:20:51 WARN TaskSetManager: Stage 70 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 70:=======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.917078733444214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 70:==============================================>           (4 + 1) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selected_features_indices, execution_time = mrmr_spark(n_total_features, 5, sc,X_RDD, Y_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc4fd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 0, 1, 2, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2f0e8",
   "metadata": {},
   "source": [
    "Then, we will look a the forward feature Selection by training on a decision tree with every feature and add the features which get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f884d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fit_get_score(x,y,selected_features):\n",
    "    \"\"\"\n",
    "    train an model to get the score with the addition of a specified x feature\n",
    "    \"\"\"    \n",
    "    \n",
    "    n_selected_features = selected_features.shape[0]\n",
    "    \n",
    "    if n_selected_features>0:\n",
    "        # need to merge x and the already selected features\n",
    "        x = np.vstack((selected_features,x))\n",
    "        x = x.transpose()\n",
    "    else:\n",
    "        x = x.reshape(-1, 1)\n",
    "    \n",
    "    #split data\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y, test_size= 0.3)\n",
    "    \n",
    "    # train on training data\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt = dt.fit(x_train,y_train)\n",
    "    \n",
    "    # get score on testing set\n",
    "    return dt.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035c9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_feature_selection(n_total_features, K, sc, X_RDD, Y):\n",
    "    \"\"\"\n",
    "    n_total_features : number of total features\n",
    "    K : number of feature to select\n",
    "    sc : spark context\n",
    "    X_RDD : RDD of the variable X\n",
    "    Y: Output data\n",
    "    \n",
    "    return the indice of selected features and time execution\n",
    "    by using a decision tree as model to calculate the score\n",
    "    \"\"\"\n",
    "    time_execution = []\n",
    "    \n",
    "    remaining_features_indices = list(range(n_total_features))\n",
    "    selected_features_indices = []\n",
    "    \n",
    "    for k in range(K):\n",
    "        print(\"Step: \"+str(k))\n",
    "    \n",
    "        start_time=time.time()\n",
    "\n",
    "        # Get the subset of selected features values, and cast as an array\n",
    "        selected_features = X_RDD.zipWithIndex().filter(lambda x: x[1] in selected_features_indices).map(lambda x: x[0]).collect()\n",
    "        selected_features = np.array(selected_features)\n",
    "        \n",
    "    \n",
    "        #  scores for a certain model are computed by first filtering `t_X` to remove already selected features, and then mapping \n",
    "        # each remaining feature using the `fit_get_score` function\n",
    "        scores = X_RDD.zipWithIndex().filter(lambda x: x[1] in remaining_features_indices).map(lambda x:fit_get_score(x[0],Y,selected_features)).collect()\n",
    "    \n",
    "        # Once all scores are computed, the index of the feature with the highest value is chosen\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        print(\"best_score :\", np.max(scores))\n",
    "    \n",
    "        index_max_score_features = np.argmax(scores)\n",
    "    \n",
    "        selected_features_indices.append(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        del(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "        print(time.time()-start_time)\n",
    "        time_execution.append(time.time()-start_time)\n",
    "        \n",
    "    return selected_features_indices, time_execution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f153a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:13:36 WARN TaskSetManager: Stage 39 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:13:36 WARN TaskSetManager: Stage 40 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:13:37 WARN TaskSetManager: Stage 41 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:13:38 WARN TaskSetManager: Stage 42 contains a task of very large size (54692 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/21 17:13:39 ERROR Executor: Exception in task 3.0 in stage 42.0 (TID 203)]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_4024/512940920.py\", line -1, in <lambda>\n",
      "  File \"/tmp/ipykernel_4024/1441107754.py\", line -1, in fit_get_score\n",
      "  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/base.py\", line 651, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 468, in predict\n",
      "    proba = self.tree_.predict(X)\n",
      "  File \"sklearn/tree/_tree.pyx\", line 753, in sklearn.tree._tree.Tree.predict\n",
      "  File \"sklearn/tree/_tree.pyx\", line 755, in sklearn.tree._tree.Tree.predict\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 41.1 GiB for an array with shape (300000, 1, 18406) and data type float64\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/21 17:13:39 WARN TaskSetManager: Lost task 3.0 in stage 42.0 (TID 203) (192.168.0.194 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_4024/512940920.py\", line -1, in <lambda>\n",
      "  File \"/tmp/ipykernel_4024/1441107754.py\", line -1, in fit_get_score\n",
      "  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/base.py\", line 651, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 468, in predict\n",
      "    proba = self.tree_.predict(X)\n",
      "  File \"sklearn/tree/_tree.pyx\", line 753, in sklearn.tree._tree.Tree.predict\n",
      "  File \"sklearn/tree/_tree.pyx\", line 755, in sklearn.tree._tree.Tree.predict\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 41.1 GiB for an array with shape (300000, 1, 18406) and data type float64\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/05/21 17:13:39 ERROR TaskSetManager: Task 3 in stage 42.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:13:40 WARN TaskSetManager: Lost task 2.0 in stage 42.0 (TID 202) (192.168.0.194 executor driver): TaskKilled (Stage cancelled)\n",
      "22/05/21 17:13:40 WARN TaskSetManager: Lost task 4.0 in stage 42.0 (TID 204) (192.168.0.194 executor driver): TaskKilled (Stage cancelled)\n",
      "[Stage 42:>                                                         (0 + 2) / 5]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 42.0 failed 1 times, most recent failure: Lost task 3.0 in stage 42.0 (TID 203) (192.168.0.194 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_4024/512940920.py\", line -1, in <lambda>\n  File \"/tmp/ipykernel_4024/1441107754.py\", line -1, in fit_get_score\n  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/base.py\", line 651, in score\n    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 468, in predict\n    proba = self.tree_.predict(X)\n  File \"sklearn/tree/_tree.pyx\", line 753, in sklearn.tree._tree.Tree.predict\n  File \"sklearn/tree/_tree.pyx\", line 755, in sklearn.tree._tree.Tree.predict\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 41.1 GiB for an array with shape (300000, 1, 18406) and data type float64\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor94.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_4024/512940920.py\", line -1, in <lambda>\n  File \"/tmp/ipykernel_4024/1441107754.py\", line -1, in fit_get_score\n  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/base.py\", line 651, in score\n    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 468, in predict\n    proba = self.tree_.predict(X)\n  File \"sklearn/tree/_tree.pyx\", line 753, in sklearn.tree._tree.Tree.predict\n  File \"sklearn/tree/_tree.pyx\", line 755, in sklearn.tree._tree.Tree.predict\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 41.1 GiB for an array with shape (300000, 1, 18406) and data type float64\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selected_features_indices_forward, execution_time_forward \u001b[38;5;241m=\u001b[39m \u001b[43mforward_feature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_total_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_RDD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m selected_features_indices_forward\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mforward_feature_selection\u001b[0;34m(n_total_features, K, sc, X_RDD, Y)\u001b[0m\n\u001b[1;32m     24\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(selected_features)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#  scores for a certain model are computed by first filtering `t_X` to remove already selected features, and then mapping \u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# each remaining feature using the `fit_get_score` function\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mX_RDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzipWithIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremaining_features_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mfit_get_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Once all scores are computed, the index of the feature with the highest value is chosen\u001b[39;00m\n\u001b[1;32m     32\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 42.0 failed 1 times, most recent failure: Lost task 3.0 in stage 42.0 (TID 203) (192.168.0.194 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_4024/512940920.py\", line -1, in <lambda>\n  File \"/tmp/ipykernel_4024/1441107754.py\", line -1, in fit_get_score\n  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/base.py\", line 651, in score\n    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 468, in predict\n    proba = self.tree_.predict(X)\n  File \"sklearn/tree/_tree.pyx\", line 753, in sklearn.tree._tree.Tree.predict\n  File \"sklearn/tree/_tree.pyx\", line 755, in sklearn.tree._tree.Tree.predict\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 41.1 GiB for an array with shape (300000, 1, 18406) and data type float64\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor94.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/pdefraene/Downloads/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_4024/512940920.py\", line -1, in <lambda>\n  File \"/tmp/ipykernel_4024/1441107754.py\", line -1, in fit_get_score\n  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/base.py\", line 651, in score\n    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n  File \"/home/pdefraene/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 468, in predict\n    proba = self.tree_.predict(X)\n  File \"sklearn/tree/_tree.pyx\", line 753, in sklearn.tree._tree.Tree.predict\n  File \"sklearn/tree/_tree.pyx\", line 755, in sklearn.tree._tree.Tree.predict\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 41.1 GiB for an array with shape (300000, 1, 18406) and data type float64\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "selected_features_indices_forward, execution_time_forward = forward_feature_selection(n_total_features, 10, sc,X_RDD, Y_numpy)\n",
    "selected_features_indices_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e2f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_feature_selection_diff(n_total_features, inc_accuracy, sc, X_RDD, Y):\n",
    "    \"\"\"\n",
    "    n_total_features : number of total features\n",
    "    inc_accuracy : increase of accuracy needed to continue the algorithm\n",
    "    sc : spark context\n",
    "    X_RDD : RDD of the variable X\n",
    "    Y: Output data\n",
    "    \n",
    "    return the indice of selected features and time execution\n",
    "    by using a decision tree as model to calculate the score\n",
    "    \"\"\"\n",
    "    time_execution = []\n",
    "    \n",
    "    remaining_features_indices = list(range(n_total_features))\n",
    "    selected_features_indices = []\n",
    "    \n",
    "    last_best_score = 0\n",
    "    diff_accuracy = 1\n",
    "    k = 0\n",
    "    while diff_accuracy > inc_accuracy:\n",
    "        print(\"Step: \"+str(k))\n",
    "    \n",
    "        start_time=time.time()\n",
    "\n",
    "        # Get the subset of selected features values, and cast as an array\n",
    "        selected_features = X_RDD.zipWithIndex().filter(lambda x: x[1] in selected_features_indices).map(lambda x: x[0]).collect()\n",
    "        selected_features = np.array(selected_features)\n",
    "        \n",
    "    \n",
    "        #  scores for a certain model are computed by first filtering `t_X` to remove already selected features, and then mapping \n",
    "        # each remaining feature using the `fit_get_score` function\n",
    "        scores = X_RDD.zipWithIndex().filter(lambda x: x[1] in remaining_features_indices).map(lambda x:fit_get_score(x[0],Y,selected_features)).collect()\n",
    "    \n",
    "        # Once all scores are computed, the index of the feature with the highest value is chosen\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        # compute the difference between last result and new result\n",
    "        best_score = np.max(scores)\n",
    "        diff_accuracy = best_score - last_best_score\n",
    "        \n",
    "        \n",
    "        print(\"best_accuracy:\", best_score)\n",
    "        \n",
    "        if best_score > last_best_score:\n",
    "        \n",
    "            index_max_score_features = np.argmax(scores)\n",
    "    \n",
    "            selected_features_indices.append(remaining_features_indices[index_max_score_features])\n",
    "    \n",
    "            del(remaining_features_indices[index_max_score_features])\n",
    "        \n",
    "        last_best_score = best_score\n",
    "        print(time.time()-start_time)\n",
    "        time_execution.append(time.time()-start_time)\n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "    return selected_features_indices, time_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_indices_forward_diff, execution_time_forward_diff = forward_feature_selection_diff(n_total_features, 0.001, sc,X_RDD, Y_numpy)\n",
    "selected_features_indices_forward_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1567b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 17:13:42 WARN PythonRunner: Incomplete task 0.0 in stage 42 (TID 200) interrupted: Attempting to kill Python Worker\n",
      "22/05/21 17:13:42 WARN PythonRunner: Incomplete task 1.0 in stage 42 (TID 201) interrupted: Attempting to kill Python Worker\n",
      "22/05/21 17:13:42 WARN TaskSetManager: Lost task 0.0 in stage 42.0 (TID 200) (192.168.0.194 executor driver): TaskKilled (Stage cancelled)\n",
      "22/05/21 17:13:42 WARN TaskSetManager: Lost task 1.0 in stage 42.0 (TID 201) (192.168.0.194 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604f400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
