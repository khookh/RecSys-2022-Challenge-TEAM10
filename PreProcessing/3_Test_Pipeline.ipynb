{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcdab385-6c50-48a6-b413-953d4508042f",
   "metadata": {},
   "source": [
    "# Test dataset pipeline\n",
    "\n",
    "In this notebook, a pipeline similar to Step 1 will be coded for the test dataset.\n",
    "\n",
    "Each operation are the same, except operations that require the purchase date, for which the last item visit date will be considered.\n",
    "\n",
    "Clusters used for item feature dimensionality reduction are imported from the one's computed in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1e1e3c-9fac-4df9-bb54-4c13bb548ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/28 08:22:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/28 08:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/28 08:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/05/28 08:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/05/28 08:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "import sys\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"load_explore\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c275ba-5edc-44eb-a130-28d566b488b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loads relevant datas in DataFrames\n",
    "test_sessions = spark.read.load('../Data/test_leaderboard_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "item_features = spark.read.load('../Data/item_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6919b71a-b8c3-4d8e-940e-117a91c75692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def parse_datetime(timestamp):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')    \n",
    "\n",
    "date_parsed_sessions = (test_sessions.rdd\n",
    "                        .map(lambda x: (x.session_id, parse_datetime(x.date)))  # Maps rows to (key=session_id, values=(parsed_date)) tuples\n",
    "                        .cache())\n",
    "\n",
    "# Reduces by key from the MAX monoid\n",
    "max_date_sessions = (\n",
    "    date_parsed_sessions\n",
    "    .reduceByKey(max)\n",
    ")\n",
    "\n",
    "# Reduces by key from the MIN monoid\n",
    "min_date_sessions = (\n",
    "    date_parsed_sessions\n",
    "    .reduceByKey(min)\n",
    ")\n",
    "\n",
    "# Computes delta time in seconds\n",
    "time_sessions_in_seconds = (\n",
    "    max_date_sessions\n",
    "    .join(min_date_sessions)  # Joins the max dates with the min dates from the session_id (on pair per session)\n",
    "    .mapValues(lambda x: (x[0] - x[1]).seconds)  # Computes delta time for each session \n",
    ")\n",
    "\n",
    "def get_season(date_time):\n",
    "    '''Converts the date_time into a season\n",
    "    \n",
    "    :returns: an integer\n",
    "        0 -> Winter\n",
    "        1 -> Spring\n",
    "        2 -> Summer\n",
    "        3 -> Autumn\n",
    "    '''    \n",
    "    season = (date_time.month - 1) // 3\n",
    "    season += (date_time.month == 3)&(date_time.day>=20)\n",
    "    season += (date_time.month == 6)&(date_time.day>=21)\n",
    "    season += (date_time.month == 9)&(date_time.day>=23)\n",
    "    season -= 3*int(((date_time.month == 12)&(date_time.day>=21)))\n",
    "    return season\n",
    "\n",
    "def get_day_period(date_time):\n",
    "    '''Converts the date_time into the day of the week.\n",
    "    \n",
    "    0 -> Morning (from 6am to 12am)\n",
    "    1 -> Afternoon (from 12am to 6pm)\n",
    "    2 -> Evening (from 6pm to 12pm)\n",
    "    3 -> Night (from 12pm to 6am)\n",
    "    '''\n",
    "    return date_time.hour // 6\n",
    "\n",
    "# Assigns a season for each session\n",
    "season_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(get_season)\n",
    ")\n",
    "\n",
    "# Assigns a day period (morning, afternoon, evening, night...) for each session\n",
    "day_period_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(get_day_period)\n",
    ")\n",
    "\n",
    "# Assigns a month for each session\n",
    "month_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(lambda x: x.month - 1)\n",
    ")\n",
    "\n",
    "# Assigns a year for each session\n",
    "year_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(lambda x: x.year - 2020)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ec8cb6-28df-425d-a91f-932a85c61e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we don't have access to the purchase date on the test dataset\n",
    "# we will instead take the last item visit date of each session...\n",
    "session_purchase_date = max_date_sessions\n",
    "\n",
    "def compute_time_per_item(session_info):\n",
    "    '''Computes the time spent on each item (in seconds)\n",
    "    \n",
    "    :param session_info: The information of the session ([(item_id, date)...], purchase_date)\n",
    "    :returns: Time per item information [(item_id, visit_time)...]\n",
    "    '''\n",
    "    # Unpacks the info\n",
    "    visited_items, purchase_date = session_info\n",
    "    item_time_list = []\n",
    "    \n",
    "    for idx in range(len(visited_items)):\n",
    "        item_id, item_visit_date = visited_items[idx]\n",
    "        \n",
    "        # Last item, need to check the purchase date\n",
    "        if idx == len(visited_items) - 1:\n",
    "            item_time_list.append((item_id, (purchase_date - item_visit_date).seconds))\n",
    "        # Not the last item, checks the next item \n",
    "        else:\n",
    "            _, next_item_visit_date = visited_items[idx + 1]\n",
    "            item_time_list.append((item_id, (next_item_visit_date - item_visit_date).seconds))\n",
    "    \n",
    "    return item_time_list\n",
    "\n",
    "\n",
    "# Here, we group the tuples by their keys (session_id) using Map Reduce\n",
    "items_time_per_session = (\n",
    "    test_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, (x.item_id, parse_datetime(x.date))))\n",
    "    .mapValues(lambda x: [(x[0], x[1])])  # Sets all the values inside a list in order to easily reduce and join the sessions\n",
    "    .reduceByKey(lambda x, y: x + y)  # Joins all the sessions togheter by reducing and joining the lists\n",
    "    .join(session_purchase_date)  # Each tuple has now the shape (session_id, ([(item_id, date)...], purchase_date))\n",
    "    .mapValues(lambda x: (sorted(x[0], key=lambda t:t[1]), x[1]))  # For each session, sorts the (item_id, date) tuples by date \n",
    "    .mapValues(compute_time_per_item)\n",
    ")\n",
    "\n",
    "# The item on which the most time was spent\n",
    "most_spent_time = (\n",
    "    items_time_per_session\n",
    "    .mapValues(lambda x: sorted(x, reverse=True, key=lambda t: t[1]))  # Sorts the item visits by time\n",
    "    .mapValues(lambda x: x[0])  # Gets the item with the most time spent on\n",
    ")\n",
    "\n",
    "# Session mean and std time per item\n",
    "mean_std_time_session = (\n",
    "    items_time_per_session\n",
    "    .mapValues(lambda x: np.array(x, dtype=np.float32)[:, 1])  # Gets a numpy array for each time and takes only the visit time (drops the item id)\n",
    "    .mapValues(lambda x: (x.mean(), x.std()))\n",
    ")\n",
    "\n",
    "# Takes the number of visits per item in each session\n",
    "item_visit_counts = (\n",
    "    test_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, x.item_id))  # Maps the rows to a (key=session_id, values=item_id) tuple\n",
    "    .mapValues(lambda x: [x])  # Puts the items inside a list, so they can be reduced easily.\n",
    "    .reduceByKey(lambda x, y: x + y)  # Joins the lists togheter, grouping the tuples by keys\n",
    "    .mapValues(lambda x: np.unique(x, return_counts=True))  # Transforms values into two arrays: an item_id array and an occurence_array\n",
    "    .mapValues(lambda x: np.vstack((x[0], x[1])))\n",
    ")\n",
    "\n",
    "def get_revisited_items(count_array):\n",
    "    '''Gets the number of items revisited at least once'''\n",
    "    revisited_indices = count_array[1, :] > 1\n",
    "    return np.count_nonzero(revisited_indices)\n",
    "\n",
    "def get_item_revisits_info(count_array):\n",
    "    '''Returns the item_id of the item that\n",
    "    has been revisited the most as well as the \n",
    "    number of times it has been revisited\n",
    "    \n",
    "    If no item was revisited return -1, -1, 0\n",
    "    '''\n",
    "    number_of_revisits = get_revisited_items(count_array)\n",
    "    if number_of_revisits == 0:\n",
    "        return (-1, -1, number_of_revisits)\n",
    "    \n",
    "    most_revisited_item_idx = np.argmax(count_array[1, :])\n",
    "    most_revisits = count_array[1, :].max()\n",
    "    \n",
    "    return (count_array[0, most_revisited_item_idx], most_revisits, number_of_revisits)\n",
    "    \n",
    "\n",
    "# Returns the most visited item ID, the number of time it has been revisited and the number of items visited at least once\n",
    "item_revisit_info = (\n",
    "    item_visit_counts\n",
    "    .mapValues(get_item_revisits_info) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b916c6ff-9da7-41e0-8af4-7498c5514fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the computed clusters\n",
    "import pickle\n",
    "\n",
    "with open('../Data/clusters/feature_category_clusters.np', 'rb') as file:\n",
    "    feature_category_clusters = pickle.load(file)\n",
    "    \n",
    "with open('../Data/clusters/feature_values_clusters.np', 'rb') as file:\n",
    "    feature_values_clusters = pickle.load(file)\n",
    "        \n",
    "with open('../Data/item_dict.pd', 'rb') as file:\n",
    "    item_cluster_dict = pickle.load(file)\n",
    "    \n",
    "with open('../Data/most_visited_dict.pd', 'rb') as file:\n",
    "    most_visited_dict = pickle.load(file)\n",
    "    \n",
    "with open('../Data/most_time_dict.pd', 'rb') as file:\n",
    "    most_time_dict = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa5b848-8047-4408-b52b-2e9f768de093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature that contain item ID are clustered to reduce dimensionality\n",
    "\n",
    "def map_cluster_revisited(x):\n",
    "    if x[0] != -1:\n",
    "        return item_cluster_dict[x[0]], x[1], x[2]\n",
    "    return x\n",
    "\n",
    "most_spent_time_clustered = most_spent_time.mapValues(lambda x: (item_cluster_dict[x[0]], x[1]))  # Encodes the item id in its cluster\n",
    "revisited_clustered = item_revisit_info.mapValues(map_cluster_revisited)  # Encodes the item id in its cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123e06c1-13f4-40d8-a190-f0d8c9198a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Mapstep, to assign clusters\n",
    "def mapstep(x,br_clusters): \n",
    "    \"\"\"\n",
    "    This function returns the closest cluster of the current row x\n",
    "    x : current row\n",
    "    broadcast_clusters : current value of the clusters\n",
    "    \"\"\"\n",
    "    M = br_clusters.value.shape[0]\n",
    "    d=np.zeros(M) # distance of the current row with each cluster\n",
    "    for m in range(M):\n",
    "        d[m]=sum(abs(np.subtract(x,br_clusters.value[m]))) # compute distance between cluster m and row x\n",
    "    return np.argmin(d)\n",
    "\n",
    "# Computing the clustering of session by item category occurence\n",
    "\n",
    "# Counts the number of unique category IDs\n",
    "unique_categories_nb = len(\n",
    "    item_features.rdd\n",
    "    .map(lambda x: set([x.feature_category_id]))  # Maps the rows to a set containing the feature category\n",
    "    .reduce(lambda x, y: x.union(y))  # Reduces by joining the sets, keeping only the unique categories values\n",
    ")\n",
    "\n",
    "def initialize_vector(category_nb, max_categories=unique_categories_nb):\n",
    "    '''\n",
    "    Returns a vector of categories, where each element is 0 except the one at the category's index.\n",
    "    '''\n",
    "    vector = np.zeros(max_categories, dtype=np.float32)\n",
    "    vector[category_nb - 1] = 1.0\n",
    "    return vector\n",
    "\n",
    "def normalize_vector(category_vector):\n",
    "    '''\n",
    "    Noramlizes a category vector, dividing all the elements by the sum of of occurences\n",
    "    '''\n",
    "    total_occurences = np.sum(category_vector)\n",
    "    return category_vector / total_occurences\n",
    "\n",
    "\n",
    "normalized_categories_vector_per_session = (\n",
    "    test_sessions.rdd\n",
    "    .map(lambda x: (x.item_id, x.session_id))  # Maps the rows to (key=item_id,values=sessions_id) tuples\n",
    "    .join(item_features.rdd.map(lambda x: (x.item_id, x.feature_category_id)))  # Joins the two datasets togheter by item_id\n",
    "    .map(lambda x: x[1])  # Only keeps some parts of the tuples, now we have (key=session_id,values=feature_category_id)\n",
    "    .mapValues(initialize_vector)  # Encodes the feature category inside a counter vector\n",
    "    .reduceByKey(lambda x, y: x + y)   # Reduces by session_id, and adds the counter vectors\n",
    "    .mapValues(normalize_vector)  # Noramlizes the vector, so that the sum of every element is equal to 1.0\n",
    ")\n",
    "\n",
    "bc_feature_category_clusters = sc.broadcast(feature_category_clusters) # broadcasting is a good practice for parallel computing\n",
    "clustered_categories = normalized_categories_vector_per_session.map(lambda x : (x[0], mapstep(x[1] ,bc_feature_category_clusters))) # compute associated clusters for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf41d76-b922-4594-b1e7-b3fbfc3cae97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute clustered item value counter in sessions (25 features)\n",
    "\n",
    "unique_feature_values = (\n",
    "    item_features.rdd\n",
    "    .map(lambda x: (set([x.feature_value_id])))  # Maps each row to a set containing the feature value\n",
    "    .reduce(lambda x, y: x.union(y))  # Joins the sets, discarding duplicate values (only keeping unique values)\n",
    ")\n",
    "\n",
    "highest_feature_value = max(unique_feature_values)  # Takes the highest value in the set, used as vector sie\n",
    "\n",
    "\n",
    "def initialize_value_vector(value_nb, max_values=highest_feature_value):\n",
    "    '''\n",
    "    Returns a vector of item feature values, where each element is 0 except the one at the values's index.\n",
    "    '''\n",
    "    vector = np.zeros(max_values, dtype=np.float32)\n",
    "    vector[value_nb - 1] = 1.0\n",
    "    return vector\n",
    "\n",
    "encoded_item_values = (\n",
    "    item_features.rdd\n",
    "    .map(lambda x: (x.item_id, x.feature_value_id))  # Maps each row to a (key=item_id, values=feature_value_id) tuples\n",
    "    .mapValues(initialize_value_vector)  # Initializes each value vector (one hot encoding type)\n",
    "    .reduceByKey(lambda x, y: x + y)  # Reduces on each item by summing the feature value vector\n",
    ")\n",
    "\n",
    "bc_feature_values_clusters = sc.broadcast(feature_values_clusters) # broadcasting is a good practice for parallel computing\n",
    "clustered_item_values = encoded_item_values.map(lambda x : (x[0], mapstep(x[1],bc_feature_values_clusters))) # compute associated clusters for each item\n",
    "\n",
    "# Encodes the clustered item values in one-hot encoding\n",
    "encoded_item_values = (\n",
    "    clustered_item_values\n",
    "    .mapValues(lambda x: initialize_value_vector(x, max_values=25))\n",
    ")\n",
    "\n",
    "# Computes the vector for each session\n",
    "item_features_session = (\n",
    "    test_sessions.rdd\n",
    "    .map(lambda x: (x.item_id, x.session_id))\n",
    "    .join(encoded_item_values)\n",
    "    .map(lambda x: (x[1][0], x[1][1]))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .mapValues(normalize_vector)\n",
    "    .mapValues(tuple)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ab197e-8a00-42c5-989a-4e940f07101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First and last visited items\n",
    "\n",
    "first_last_item = (\n",
    "    test_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, (x.item_id, parse_datetime(x.date))))  # Maps the rows into a (key=session_id, value=(item_id, date)) tuple\n",
    "    .mapValues(lambda x: ([(x)]))  # Puts the (item_id, date) tuple into a list to easily merge the values\n",
    "    .reduceByKey(lambda x, y: x + y)  # Groups the items per session, merging the session's items and their date\n",
    "    .mapValues(lambda x: sorted(x, key=lambda x_: x_[1]))  # In each session, sorts the items by date (earliest to latest)\n",
    "    .mapValues(lambda x: (x[0][0], x[-1][0]))  # Takes the first and last visited items\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "first_last_item_clustered = first_last_item.mapValues(lambda x: (item_cluster_dict[x[0]], item_cluster_dict[x[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837af5e2-3688-468a-92d4-cf96741bd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "revisited_clustered = revisited_clustered.map(lambda x: (x[0],x[1]+most_visited_dict.get(x[1][0])))\n",
    "most_spent_time_clustered = most_spent_time_clustered.map(lambda x: (x[0],x[1]+most_time_dict.get(x[1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bc3d7-0f45-4943-906d-b88d6145685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/28 08:33:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def unpack_tuples(row):\n",
    "    out = []\n",
    "    for elem in row:\n",
    "        if type(elem) == tuple:\n",
    "            out.extend(unpack_tuples(elem))\n",
    "        else:\n",
    "            out.append(elem)\n",
    "    return out\n",
    "\n",
    "final_engineered_dataset = (\n",
    "    time_sessions_in_seconds\n",
    "    .join(season_per_session)\n",
    "    .join(day_period_per_session)\n",
    "    .join(month_per_session)\n",
    "    .join(year_per_session)\n",
    "    .join(most_spent_time_clustered)\n",
    "    .join(mean_std_time_session)\n",
    "    .join(revisited_clustered)\n",
    "    .join(first_last_item_clustered)\n",
    "    .join(clustered_categories)\n",
    "    .join(item_features_session)\n",
    "    .mapValues(unpack_tuples)\n",
    ")\n",
    "\n",
    "tuple_final_engineered_dataset = (\n",
    "    final_engineered_dataset.map(lambda x: ((x[0],) + tuple(x[1])))\n",
    ")\n",
    "\n",
    "columns = ['session_id', 'session_time', 'season', 'day_period', 'month', 'year', 'item_most_time_spent', 'most_time_spent_on_item', 'most_frequently_bought_for_time_spent', \n",
    "          'least_frequently_bought_for_time_spent', 'mean_time', 'std_time', 'item_most_visited', 'number_o_visit', 'number_o_revisited_items', 'most_frequently_bought_for_most_revisited',\n",
    "          'first_item_visited', 'last_item_visited', 'normalized_features_vector']\n",
    "\n",
    "columns += [str(i+1) for i in range(25)]\n",
    "\n",
    "(\n",
    "tuple_final_engineered_dataset\n",
    "    .map(lambda x : tuple([float(xi) for xi in x]))\n",
    "    .coalesce(1)\n",
    "    .toDF(columns)\n",
    "    .write.option(\"header\",True)\n",
    "    .csv('../Data/TEMPtrain_sessions.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09ce72-9e10-40ef-bc27-a47e576a57b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
