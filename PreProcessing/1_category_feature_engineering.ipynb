{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2a143c-ba2a-4ad6-b63a-4a91839d345b",
   "metadata": {},
   "source": [
    "# Feature engineering on Item feature categories.\n",
    "\n",
    "\n",
    "Item features can hold many interesting information, but there is a very high amount of them.\n",
    "\n",
    "Some item share some feature categories (such as color, size) but some don't.\n",
    "\n",
    "Different features can be engineered from item featues, such as:\n",
    "\n",
    "* The most common visited (feature_category, feature_value) pairs. ✅\n",
    "* The number of visits on the most common pair. ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75fc79c-cb34-40ca-9071-c4fdcedc6acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/16 14:40:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "import sys\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"load_explore\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0d5031b-011c-490a-bef5-cd68aa442d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loads relevant datas in DataFrames\n",
    "train_sessions = spark.read.load('../Data/train_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "candidate_items = spark.read.load('../Data/candidate_items.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "item_features = spark.read.load('../Data/item_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9872f04b-12f3-46f3-aa9b-42e0b4f6bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_item_features = item_features.rdd.map(lambda x: (x.item_id, (x.feature_category_id, x.feature_value_id)))\n",
    "\n",
    "item_session_and_features = (train_sessions.rdd\n",
    "                             .map(lambda x: (x.item_id, (x.session_id)))  # Maps sessions to a (key, value pair)\n",
    "                             .join(mapped_item_features)  # Huge operation, might be too heavy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce747bac-63b7-4c1d-b4b7-5354c91c4efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(15654, (13, (56, 365))),\n",
       " (15654, (13, (47, 516))),\n",
       " (15654, (13, (69, 780))),\n",
       " (15654, (13, (68, 351))),\n",
       " (15654, (13, (62, 801)))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_session_and_features.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90410fe5-0711-4b93-bf17-4511f5d86d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Groupping by session too heavy, let's try to perform a reduce operation\n",
    "\n",
    "# At this point, item_features_per_session contains tuples of (item_id, (sessions_id, (feature_cat_id, feature_val_id))\n",
    "# Let's reorder the tuple to (session_id, (item_id, feature_cat_id, feature_val_id))\n",
    "\n",
    "def reduce_count_item_pairs(x, y):\n",
    "    '''Merges the y dictionnary into the x dictionnary.\n",
    "    Expects to have both x and y as dictionnary having the keys (feature_cat_id, feature_val_id)\n",
    "    and a counter of occurences as a value.\n",
    "    '''\n",
    "    for feature_pair in y:\n",
    "        if x.get(feature_pair) is None:\n",
    "            x[feature_pair] = y[feature_pair]\n",
    "        else:\n",
    "            x[feature_pair] += y[feature_pair]\n",
    "    return x\n",
    "        \n",
    "item_features_per_session = (item_session_and_features\n",
    "                             .map(lambda x: (x[1][0], (x[0], x[1][1][0], x[1][1][1])))  # Reorder to (session_id, (item_id, feature_cat_id, feature_val_id))\n",
    "                             .mapValues(lambda x: {(x[1], x[2]): 1})  # Transforms to (session_id, counter_of(feature_cat_id, feature_val_id))\n",
    "                             .reduceByKey(reduce_count_item_pairs)  # Reduce by merging the dictionaries and their values\n",
    "                             .mapValues(lambda x: Counter(x).most_common()[0])  # Takes the most common key tuple\n",
    "                             .mapValues(lambda x: (x[0][0], x[0][1], x[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2973efc6-3021-44c5-8fb1-3c9090446426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(290712, (56, 365, 8)),\n",
       " (1264194, (56, 365, 5)),\n",
       " (3219234, (56, 365, 6)),\n",
       " (282840, (56, 365, 10)),\n",
       " (4209018, (56, 365, 7))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features_per_session.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce52f9-b37a-464f-a0ab-502f54fae4f2",
   "metadata": {},
   "source": [
    "## Counting number of categories.\n",
    "\n",
    "We want to provide features engineered from the item feature categories. There are many feature categories. Let's count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c185360-97a3-484d-ba9c-bacf41107f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_unique(x, y):\n",
    "    ''' Merges all the sets togheter '''\n",
    "    return (x[0], x[1] | y[1])\n",
    "\n",
    "unique_categories = (item_features.rdd\n",
    "                    .map(lambda x: (x.item_id, (x.feature_category_id)))  # Maps every item feature entry to (item_id, (feature_category_id))\n",
    "                    .mapValues(lambda x: set([x]))  # Maps the values to a set, so they will be counted as unique during the reduce opetations.\n",
    "                    .reduce(reduce_unique)  # Reduces the values by merging the sets, eliminating duplicate feature_category_ids\n",
    ")\n",
    "\n",
    "len(unique_categories[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399e0e2-a601-4a67-bec5-4faee62f5115",
   "metadata": {},
   "source": [
    "There are 73 different feature categories. \n",
    "\n",
    "## Engineering our first 73 features\n",
    "\n",
    "Some features could be engineered such as counting the number of occurences for each category met per session.\n",
    "We will have 73 different features only for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ac2171-ba44-4d50-afca-185c3720fdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2066478,\n",
       "  (0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   5,\n",
       "   1,\n",
       "   4,\n",
       "   5,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   5,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   4,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   4,\n",
       "   4,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   5,\n",
       "   0,\n",
       "   0,\n",
       "   5,\n",
       "   0,\n",
       "   0,\n",
       "   4,\n",
       "   0,\n",
       "   5,\n",
       "   5,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   4,\n",
       "   5,\n",
       "   4,\n",
       "   5,\n",
       "   0,\n",
       "   5,\n",
       "   0,\n",
       "   0,\n",
       "   5,\n",
       "   5,\n",
       "   0,\n",
       "   0,\n",
       "   5,\n",
       "   5))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_categories(x, y):\n",
    "    ''' Merges all the dictionnary values togheter, adding the categories values alltogheter '''\n",
    "    for key in y:\n",
    "        x[key] = x.get(key, 0) + y[key]\n",
    "    return x\n",
    "\n",
    "def fill_empties(x, nb_categories=len(unique_categories[1])):\n",
    "    ''' Fill the dictionnary with categories that are not present\n",
    "    setting their occurence count to 0 '''\n",
    "    for idx in range(0, nb_categories+1):\n",
    "        x[idx] = x.get(idx, 0)\n",
    "    return x\n",
    "\n",
    "def get_occurences(x):\n",
    "    ''' Remaps the values so we only have the occurences. \n",
    "    The feature category id by itself is defined by the order (from 1 to 73).\n",
    "    '''\n",
    "    listed_x = list(x)\n",
    "    new_list = []\n",
    "    for elem in listed_x:\n",
    "        new_list.append(elem[1])\n",
    "    return tuple(new_list)\n",
    "\n",
    "feature_categories_per_session = (item_session_and_features\n",
    "                             .map(lambda x: (x[1][0], (x[1][1][0])))  # Project to (session_id, (feature_cat_id))\n",
    "                             .mapValues(lambda x: {x: 1})  # Maps value before being reduced, putting them in a dictionnary\n",
    "                             .reduceByKey(count_categories)  # Counts feature categories occurence in a dictionnary\n",
    "                             .mapValues(fill_empties)  # Fills unmet categories and sets their occurence to 0\n",
    "                             .mapValues(lambda x: tuple(sorted(x.items(), key=lambda x: x[0])))  # Sorts by key value\n",
    "                             .mapValues(get_occurences)\n",
    ")\n",
    "\n",
    "feature_categories_per_session.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4110aa-eb57-4c80-babb-9d162d767637",
   "metadata": {},
   "source": [
    "## Feature groupping using frequent itemset\n",
    "\n",
    "The dimensionality can be reduced. For example, we merge the feature categories inside groups, determined from frequent itemset analysis.\n",
    "\n",
    "Spark ML package has Frequent Pattern Mining functionalities, implementing FP-Growth for example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc0f913-eb51-4c72-9fda-003e2c79df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurences_to_present_category(x):\n",
    "    ''' Maps a list of occurences (where the feature category id is defined by the order)\n",
    "    to a list containing the feautre categories with a non null occurency.\n",
    "    '''\n",
    "    present_categories = set()\n",
    "    assert(len(x) == 74) # Checks that the data has 73 features\n",
    "    for idx in range(74):\n",
    "        if x[idx] != 0:\n",
    "            present_categories.add(idx)\n",
    "    return tuple(present_categories)\n",
    "            \n",
    "\n",
    "present_categories_per_session = feature_categories_per_session.mapValues(occurences_to_present_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c400d66a-bb6d-4249-9f8c-827f9fa4da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2066478,\n",
       "  (3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   11,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   24,\n",
       "   26,\n",
       "   29,\n",
       "   30,\n",
       "   32,\n",
       "   34,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   50,\n",
       "   53,\n",
       "   55,\n",
       "   56,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   65,\n",
       "   68,\n",
       "   69,\n",
       "   72,\n",
       "   73))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_categories_per_session.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03296cb9-85ac-457a-b32f-1f2e99803130",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_categories_itemsets = present_categories_per_session.map(lambda x: list(x[1])).cache()  # Dropping the sessions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8c73644-b98c-4793-9c83-57df3b293504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 14:44:40 WARN FPGrowth: Input data is not cached.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "# Starts training the FPGrowth tree on the pipelined RDD\n",
    "model = FPGrowth.train(present_categories_per_session.map(lambda x: list(x[1])).cache(), minSupport=0.4, numPartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e4adbd0-a52e-4949-aabe-e3e097318675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.freqItemsets().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff886a1-1313-41b5-adfe-788e1a46b559",
   "metadata": {},
   "source": [
    "# Selecting the biggest itemsets\n",
    "\n",
    "Now that we have itemsets with a support of at least 20%, let's select a number of large groups and use them as features.\n",
    "\n",
    "If in a session, the categories visited at least once are equal or a superset of one of those groups, sets the value to 1; else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e1ee8dc-f705-40b4-9f1c-6e0b40903408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the largest itemsets and sorts them by the length of the groups (largest groups first)\n",
    "frequent_itemsets = model.freqItemsets().sortBy(lambda x: len(x.items), ascending=False, numPartitions=4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8179022a-4965-4fdb-bd87-5dd075d6b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_20_frequent = frequent_itemsets.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c12da51-9acd-4068-b04a-bc57be69d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_20_frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429bdbd-b777-472a-8016-dc159cafcf4c",
   "metadata": {},
   "source": [
    "We have a problem: the most frequent itemsets found here have a big part in common. The ideal would be to find frequent itemsets but with fairly different items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6907b7-cda5-445c-b412-9805b205cf69",
   "metadata": {},
   "source": [
    "# Item feature category clustering\n",
    "\n",
    "One technique would be to perform clustering on item category features. For each session, we take the feature occurences per session, and perform normalization.\n",
    "\n",
    "An arbitrary number of clusters X could be created. From that, we could one-hot encode the classified cluster of each session and create from that X new features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1a66f4e-91fc-4ff8-84be-34b74f3d5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_vector(x):\n",
    "    values = np.array(x, dtype=np.float64)\n",
    "    norm = np.linalg.norm(values)\n",
    "    values[:] = values[:] / norm\n",
    "    return values\n",
    "\n",
    "normalized_categories = feature_categories_per_session.mapValues(normalize_vector).map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "651953da-60db-45af-935a-74d6c0041f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 532:======================================>                  (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 300793.6195249505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "number_of_clusters = 10\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(normalized_categories, number_of_clusters, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return np.sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = normalized_categories.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5f15b6b-be8d-49ad-ab0c-30ad42faa44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.48357077,  9.30867849, 13.23844269, 17.61514928,  8.82923313,\n",
       "        8.82931522, 17.89606408, 13.83717365,  7.65262807, 12.71280022,\n",
       "        7.68291154,  7.67135123, 11.20981136,  0.43359878,  1.37541084,\n",
       "        7.18856235,  4.9358444 , 11.57123666,  5.45987962,  2.93848149,\n",
       "       17.32824384,  8.8711185 , 10.33764102,  2.87625907,  7.27808638,\n",
       "       10.55461295,  4.24503211, 11.87849009,  6.99680655,  8.54153125,\n",
       "        6.99146694, 19.26139092,  9.93251388,  4.71144536, 14.11272456,\n",
       "        3.89578175, 11.04431798,  0.20164938,  3.35906976, 10.98430618,\n",
       "       13.6923329 , 10.85684141,  9.42175859,  8.49448152,  2.60739005,\n",
       "        6.40077896,  7.69680615, 15.28561113, 11.71809145,  1.18479922,\n",
       "       11.62041985,  8.0745886 ,  6.61539   , 13.05838144, 15.15499761,\n",
       "       14.6564006 ,  5.80391238,  8.45393812, 11.65631716, 14.87772564,\n",
       "        7.60412881,  9.07170512,  4.46832513,  4.01896688, 14.06262911,\n",
       "       16.78120014,  9.63994939, 15.01766449, 11.80818013,  6.77440123,\n",
       "       11.80697803, 17.69018283,  9.8208698 , 17.82321828])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed to draw always the same vector\n",
    "np.random.seed(42)\n",
    "\n",
    "# We try our clusters by creating a fake vector and predicting its cluster.\n",
    "category_vector = np.random.normal(loc=10.0, scale=5.0, size=len(unique_categories[1])+1)\n",
    "\n",
    "# We clip the negative values to 0 \n",
    "negative_values = category_vector < 0.0\n",
    "category_vector[negative_values] = 0.0\n",
    "\n",
    "category_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc303319-a9bb-401a-a72b-52632f3ae492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We predict our vector values\n",
    "clusters.predict(category_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7fb5c-1ef7-4a25-879f-d141975245ea",
   "metadata": {},
   "source": [
    "# Feature engineering on item feature cateogries\n",
    "\n",
    "Now that we have our clusters, we perform a simple mapValues operation to cluster each item session and to one hot encode the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d287f18d-cd43-4941-b49e-187cd1b69031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_integer(x, max_range=number_of_clusters):\n",
    "    values = np.zeros(max_range, dtype=np.uint8)\n",
    "    values[x-1] = 1\n",
    "    return values\n",
    "    \n",
    "\n",
    "clustered_sessions = feature_categories_per_session.mapValues(normalize_vector).mapValues(lambda x: clusters.predict(x)).mapValues(one_hot_encode_integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f5788aac-a2ad-43a0-9d46-aaa39d5b0d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2066478, array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=uint8)),\n",
       " (3582234, array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=uint8)),\n",
       " (1985388, array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8)),\n",
       " (2130960, array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8)),\n",
       " (2269938, array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8)),\n",
       " (2700660, array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=uint8)),\n",
       " (2990472, array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8)),\n",
       " (3730554, array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8)),\n",
       " (4161570, array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=uint8)),\n",
       " (2393550, array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_sessions.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
