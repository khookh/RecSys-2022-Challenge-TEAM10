{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2a143c-ba2a-4ad6-b63a-4a91839d345b",
   "metadata": {},
   "source": [
    "# Feature engineering on Item feature categories.\n",
    "\n",
    "\n",
    "Item features can hold many interesting information, but there is a very high amount of them.\n",
    "\n",
    "Some item share some feature categories (such as color, size) but some don't.\n",
    "\n",
    "Different features can be engineered from item featues, such as:\n",
    "\n",
    "* The most common visited (feature_category, feature_value) pairs. ✅\n",
    "* The number of visits on the most common pair. ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75fc79c-cb34-40ca-9071-c4fdcedc6acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/14 13:55:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "import sys\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"load_explore\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0d5031b-011c-490a-bef5-cd68aa442d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loads relevant datas in DataFrames\n",
    "train_sessions = spark.read.load('../Data/train_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "candidate_items = spark.read.load('../Data/candidate_items.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "item_features = spark.read.load('../Data/item_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9872f04b-12f3-46f3-aa9b-42e0b4f6bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_item_features = item_features.rdd.map(lambda x: (x.item_id, (x.feature_category_id, x.feature_value_id)))\n",
    "\n",
    "item_session_and_features = (train_sessions.rdd\n",
    "                             .map(lambda x: (x.item_id, (x.session_id)))  # Maps sessions to a (key, value pair)\n",
    "                             .join(mapped_item_features)  # Huge operation, might be too heavy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce747bac-63b7-4c1d-b4b7-5354c91c4efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4230, (31, (45, 559))),\n",
       " (4230, (31, (56, 153))),\n",
       " (4230, (31, (34, 275))),\n",
       " (4230, (31, (19, 769))),\n",
       " (4230, (31, (68, 864)))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_session_and_features.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90410fe5-0711-4b93-bf17-4511f5d86d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Groupping by session too heavy, let's try to perform a reduce operation\n",
    "\n",
    "# At this point, item_features_per_session contains tuples of (item_id, (sessions_id, (feature_cat_id, feature_val_id))\n",
    "# Let's reorder the tuple to (session_id, (item_id, feature_cat_id, feature_val_id))\n",
    "\n",
    "def reduce_count_item_pairs(x, y):\n",
    "    '''Merges the y dictionnary into the x dictionnary.\n",
    "    Expects to have both x and y as dictionnary having the keys (feature_cat_id, feature_val_id)\n",
    "    and a counter of occurences as a value.\n",
    "    '''\n",
    "    for feature_pair in y:\n",
    "        if x.get(feature_pair) is None:\n",
    "            x[feature_pair] = y[feature_pair]\n",
    "        else:\n",
    "            x[feature_pair] += y[feature_pair]\n",
    "    return x\n",
    "        \n",
    "item_features_per_session = (item_session_and_features\n",
    "                             .map(lambda x: (x[1][0], (x[0], x[1][1][0], x[1][1][1])))  # Reorder to (session_id, (item_id, feature_cat_id, feature_val_id))\n",
    "                             .mapValues(lambda x: {(x[1], x[2]): 1})  # Transforms to (session_id, counter_of(feature_cat_id, feature_val_id))\n",
    "                             .reduceByKey(reduce_count_item_pairs)  # Reduce by merging the dictionaries and their values\n",
    "                             .mapValues(lambda x: Counter(x).most_common()[0])  # Takes the most common key tuple\n",
    "                             .mapValues(lambda x: (x[0][0], x[0][1], x[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2973efc6-3021-44c5-8fb1-3c9090446426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2019690, (55, 267, 3)),\n",
       " (1633440, (32, 286, 3)),\n",
       " (1501040, (72, 75, 7)),\n",
       " (490710, (56, 365, 10)),\n",
       " (2357160, (46, 825, 11))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features_per_session.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ac26b-fde0-41f8-8c52-b82e3dd1ef54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
