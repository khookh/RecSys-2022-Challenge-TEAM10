{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a049691e-9322-422f-98e4-e1b82b33e195",
   "metadata": {},
   "source": [
    "# Data preprocessing and Feature Engineering\n",
    "\n",
    "In order to obtain a learnable dataset, features must be preprocessed and engineered in order to contain valuable learning data. This notebook will generate a PySpark RDD that contains the preprocessed dataset, ready for feature selection algorithms.\n",
    "\n",
    "More than 20 features must be engineered, some ideas are:\n",
    "\n",
    "Session-time related features:\n",
    "* Weekday of the session ✅\n",
    "* Month of the session ✅\n",
    "* Season of the session ✅\n",
    "* Duration of the session ✅\n",
    "* Day period of the session ✅\n",
    "* Is it during the weekend ✅\n",
    "\n",
    "Session-item related features:\n",
    "* Number of viewed items per session ✅\n",
    "* Number of unique viewed items per session ✅\n",
    "* Item with the most time spent on. ✅\n",
    "* Longest time spend on item in the session. ✅\n",
    "* The last item of the session. ✅\n",
    "* Mean time per item. ✅\n",
    "* Variance time per item. ✅\n",
    "* Item revisit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36eea6d-07a5-4b56-b4d3-b153c90982f4",
   "metadata": {},
   "source": [
    "### Starting the Spark engine and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2721be9d-0016-4ce9-804f-acf0bd760e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/11 10:56:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "import sys\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"load_explore\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7745e6-e461-493c-99d6-e7607bcb4766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loads relevant datas in DataFrames\n",
    "train_sessions = spark.read.load('../Data/train_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "candidate_items = spark.read.load('../Data/candidate_items.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "item_features = spark.read.load('../Data/item_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "datas = [train_sessions, train_purchases, candidate_items, item_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd4315-5708-4a38-a8de-43c26e158c86",
   "metadata": {},
   "source": [
    "## Group sessions and aggregate their duration.\n",
    "\n",
    "Using a map reduce operation, we will start by computing the session duration in seconds. For that, we use a function that will convert the timestamp into seconds since the 1st January 1970 (UNIX time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f44f8d-6e1a-4fb4-b42c-74275791eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def timestamp_to_unix(timestamp: str) -> int:\n",
    "    '''Converts the timestamp, on a string format to the UNIX time\n",
    "    '''\n",
    "    try:\n",
    "        date = datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        date = datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    return int(date.timestamp())\n",
    "\n",
    "posix_date_mapped_sessions = train_sessions.rdd.map(lambda x: (x.session_id, timestamp_to_unix(x.date))).cache()\n",
    "sampled_sessions = posix_date_mapped_sessions.sample(False, 0.0001, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd56975-6178-4d9f-9435-0e5a98acb937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/11 10:56:54 WARN BlockManager: Task 23 already completed, not releasing lock for rdd_45_0\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4163, 1606579200)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_sessions.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d683158d-65b2-48a6-b0c9-73b92f0cb651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, (1582741472, 1582737768))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_min_max(x, y):\n",
    "    if type(x) == tuple:\n",
    "        return(max(x[0], y), min(x[1], y))\n",
    "    return (max(x, y), min(x, y))\n",
    "\n",
    "reduced_sessions = posix_date_mapped_sessions.reduceByKey(lambda x, y: reduce_min_max(x, y))\n",
    "reduced_sessions.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a2288e-033e-40f4-9eca-892a4483b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_seconds(tupl):\n",
    "    if type(tupl[1]) is int:\n",
    "        return (tupl[0], 0)\n",
    "    else:\n",
    "        return (tupl[0], tupl[1][0] - tupl[1][1])\n",
    "\n",
    "time_sessions = reduced_sessions.map(lambda x: get_duration_seconds(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def67b3e-2982-4bab-a2ac-f45c903af30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 3704),\n",
       " (48, 657),\n",
       " (184, 0),\n",
       " (208, 0),\n",
       " (232, 0),\n",
       " (248, 1716),\n",
       " (352, 190),\n",
       " (376, 171),\n",
       " (384, 409),\n",
       " (464, 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_sessions.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fe22f-e748-43d8-8621-6f3043cc4a12",
   "metadata": {},
   "source": [
    "We cannot compute the sessions that only show one item, so we set the session duration to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9f97c-7b41-4f90-84f4-cf187e3bc978",
   "metadata": {},
   "source": [
    "## Group sessions and aggregate the number of unique AND total viewed items\n",
    "\n",
    "We compute how much unique and total items have been viewed during a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544e51f8-31da-46c4-ae6d-f2052c56d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, 8),\n",
       " (48, 2),\n",
       " (184, 1),\n",
       " (208, 1),\n",
       " (232, 1),\n",
       " (248, 10),\n",
       " (352, 2),\n",
       " (376, 4),\n",
       " (384, 10),\n",
       " (464, 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_mapped_sessions = train_sessions.rdd.map(lambda x: (x.session_id, x.item_id)).cache()\n",
    "uniquely_viewed_items = item_mapped_sessions.groupByKey().mapValues(lambda vals: len(set(vals)))\n",
    "\n",
    "uniquely_viewed_items.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a28b6d-5fb3-45e4-9061-5bed3c0915f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, 9),\n",
       " (48, 2),\n",
       " (184, 1),\n",
       " (208, 1),\n",
       " (232, 1),\n",
       " (248, 15),\n",
       " (352, 2),\n",
       " (376, 5),\n",
       " (384, 10),\n",
       " (464, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_viewed_items = item_mapped_sessions.groupByKey().mapValues(lambda vals: len(list(vals)))\n",
    "total_viewed_items.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37070e-1e86-4f0d-884d-b1b8a50a3ea7",
   "metadata": {},
   "source": [
    "# Time period sessions evaluation\n",
    "\n",
    "We will map for each session its day in the week, if it is on weekend, the month, the season and the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5ab17c-62a8-48de-a310-e49f810f3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the starting date for each session\n",
    "starting_date_sessions = train_sessions.rdd.map(lambda x: (x.session_id, x.date)).reduceByKey(lambda x, y: min(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "985275e6-7cf8-466a-8b61-319f7b4a5ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(starting_date_sessions.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2390092-d0ee-4c68-87e6-91743011f72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, (4, 2, 0, 1, 0, 2020)),\n",
       " (48, (4, 2, 0, 3, 1, 2020)),\n",
       " (184, (0, 6, 1, 3, 1, 2021)),\n",
       " (208, (2, 6, 1, 11, 3, 2020)),\n",
       " (232, (3, 4, 0, 8, 2, 2020)),\n",
       " (248, (3, 0, 0, 11, 3, 2020)),\n",
       " (352, (4, 0, 0, 5, 1, 2020)),\n",
       " (376, (2, 5, 1, 0, 0, 2020)),\n",
       " (384, (2, 4, 0, 11, 3, 2020)),\n",
       " (464, (2, 3, 0, 5, 1, 2020))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_datetime(timestamp):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "\n",
    "def get_season(date_time):\n",
    "    season = (date_time.month - 1) // 3\n",
    "    season += (date_time.month == 3)&(date_time.day>=20)\n",
    "    season += (date_time.month == 6)&(date_time.day>=21)\n",
    "    season += (date_time.month == 9)&(date_time.day>=23)\n",
    "    season -= 3*int(((date_time.month == 12)&(date_time.day>=21)))\n",
    "    return season\n",
    "\n",
    "def get_day_period(date_time):\n",
    "    '''Converts the date_time into the day of the week.\n",
    "    \n",
    "    0 -> Morning (from 6am to 12am)\n",
    "    1 -> Afternoon (from 12am to 6pm)\n",
    "    2 -> Evening (from 6pm to 12pm)\n",
    "    3 -> Night (from 12pm to 6am)\n",
    "    '''\n",
    "    return date_time.hour // 4\n",
    "    \n",
    "def map_day_period(timestamp):\n",
    "    # Converts into datetime\n",
    "    date_time = parse_datetime(timestamp)\n",
    "    # Computes the season\n",
    "    season = get_season(date_time)\n",
    "    # Computes the \n",
    "    return (get_day_period(date_time), date_time.weekday(), int(date_time.weekday() > 4), date_time.month - 1, season, date_time.year)\n",
    "\n",
    "date_period_sessions = starting_date_sessions.map(lambda x: (x[0], map_day_period(x[1])))\n",
    "date_period_sessions.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327f68f-590f-4b82-b5b8-d043f5e77920",
   "metadata": {},
   "source": [
    "# Time per item evaluation\n",
    "\n",
    "Computing the items where time was the most spent on. The time is considered to be the one between two clicks.\n",
    "\n",
    "As it is impossible to determine how much time was spent on the last item of the session, the information will be computed from the train_purchase dataset, where the date of the purchase is also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2078d0a6-377f-4fa3-9cd3-3c30fd9ee27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_item_duration(item_list):\n",
    "    '''Return a new list of tuples, where the first element is the item_id and the second element\n",
    "    is its related duration. The duration is computed from the time difference with the next visited item.\n",
    "    \n",
    "    :param item_list: A list containing tuples of (item_id, datetime)\n",
    "    '''\n",
    "    new_list = []\n",
    "    for idx in range(len(item_list)):\n",
    "        if idx != len(item_list) - 1:\n",
    "            time_begin = parse_datetime(item_list[idx][1])\n",
    "            time_end = parse_datetime(item_list[idx+1][1])\n",
    "            delta_seconds = (time_end - time_begin).seconds\n",
    "            new_list.append((item_list[idx][0], delta_seconds))\n",
    "        else:\n",
    "            new_list.append((item_list[idx][0], item_list[idx][1]))\n",
    "    return new_list\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "time_per_item = (train_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, (x.item_id, x.date)))  # For every entry maps a tuple with key=session_id and values=(item_id, date)\n",
    "    .groupByKey()  # Groups by session_id, result (key: session_id, values: Iterable[item_id, date])\n",
    "    .mapValues(lambda x: sorted(list(x), key=lambda _x: _x[1]))  # For each key, maps the values to a sorted version\n",
    "    .mapValues(lambda x: compute_item_duration(x)))  # For each value: Iterable[item_id, date] maps to a new Iterable[item_id, duration]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22ef443d-eaff-4a89-9c73-1a02426704a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2927, 15)\n",
      "(2927, 180)\n",
      "(16064, 23)\n",
      "(11662, 42)\n",
      "(434, 3096)\n",
      "(18539, 183)\n",
      "(10414, 42)\n",
      "(28075, 118)\n",
      "(18476, '2020-02-26 18:24:32.77')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for it in time_per_item.take(1)[0][1]:\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82100a6c-1708-47a0-b0bc-113137ef2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_last_item_time(item_list, purchase_time):\n",
    "    '''Computes the time spent on the last item.\n",
    "    :param item_list: A list of visited items in a session, already processed by the compute_item_duration function\n",
    "    :param purchase_time: The timestamp of the last purchased item\n",
    "    '''\n",
    "    new_list = []\n",
    "    for idx in range(len(item_list)):\n",
    "        if idx != len(item_list) - 1:\n",
    "            new_list.append(item_list[idx])\n",
    "        else:\n",
    "            begin_time = parse_datetime(item_list[idx][1])\n",
    "            end_time = parse_datetime(purchase_time)\n",
    "            delta_seconds = (end_time - begin_time).seconds\n",
    "            new_list.append((item_list[idx][0], delta_seconds))\n",
    "    return new_list\n",
    "\n",
    "\n",
    "time_per_item = (train_purchases.rdd\n",
    "    .map(lambda x: (x.session_id, x.date))  # For every entry maps a tuple with key=session_id and value=x.date\n",
    "    .join(time_per_item)  #  Joins the purchase dataset with the time_per_item dataset from the key (session_id)\n",
    "    .mapValues(lambda x: compute_last_item_time(x[1], x[0]))  # Sets the duration of the last viewed item.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e0f116a-83e1-42d6-be84-0b60d8d753b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(48, [(8398, 657), (26404, 35)]),\n",
       " (208, [(26257, 20)]),\n",
       " (352, [(8530, 189), (13653, 359)]),\n",
       " (384,\n",
       "  [(9582, 14),\n",
       "   (1271, 35),\n",
       "   (13412, 49),\n",
       "   (7474, 25),\n",
       "   (12140, 31),\n",
       "   (5692, 44),\n",
       "   (14953, 106),\n",
       "   (784, 29),\n",
       "   (2991, 71),\n",
       "   (27183, 27)]),\n",
       " (464, [(7607, 90)]),\n",
       " (480, [(2915, 240), (7548, 152), (16377, 59)]),\n",
       " (496,\n",
       "  [(23462, 26),\n",
       "   (5880, 52),\n",
       "   (10892, 67),\n",
       "   (6636, 66),\n",
       "   (21197, 51),\n",
       "   (17239, 129),\n",
       "   (26180, 158)]),\n",
       " (512,\n",
       "  [(26283, 26),\n",
       "   (3382, 41),\n",
       "   (14360, 71),\n",
       "   (18765, 25),\n",
       "   (2273, 36),\n",
       "   (4061, 48),\n",
       "   (19866, 367),\n",
       "   (10017, 100),\n",
       "   (9699, 28),\n",
       "   (2366, 68),\n",
       "   (9106, 45),\n",
       "   (10531, 36),\n",
       "   (5243, 67),\n",
       "   (4612, 56)]),\n",
       " (544, [(26237, 2319), (13636, 186)]),\n",
       " (592, [(15170, 523), (1780, 322)])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_per_item.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36229756-7593-4644-bc42-891953b31c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(48, (8398, 657)),\n",
       " (208, (26257, 20)),\n",
       " (352, (13653, 359)),\n",
       " (384, (14953, 106)),\n",
       " (464, (7607, 90)),\n",
       " (480, (2915, 240)),\n",
       " (496, (26180, 158)),\n",
       " (512, (19866, 367)),\n",
       " (544, (26237, 2319)),\n",
       " (592, (15170, 523))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First feature and second features: item with the most time spent on and the spent time.\n",
    "longest_items = time_per_item.mapValues(lambda x: sorted(x, key=lambda _x : _x[1])[-1][0])\n",
    "longest_items.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72abe2df-9732-4b32-baad-4278a3c1d39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(48, [(8398, 657), (26404, 35)]),\n",
       " (208, [(26257, 20)]),\n",
       " (352, [(8530, 189), (13653, 359)]),\n",
       " (384,\n",
       "  [(9582, 14),\n",
       "   (1271, 35),\n",
       "   (13412, 49),\n",
       "   (7474, 25),\n",
       "   (12140, 31),\n",
       "   (5692, 44),\n",
       "   (14953, 106),\n",
       "   (784, 29),\n",
       "   (2991, 71),\n",
       "   (27183, 27)]),\n",
       " (464, [(7607, 90)]),\n",
       " (480, [(2915, 240), (7548, 152), (16377, 59)]),\n",
       " (496,\n",
       "  [(23462, 26),\n",
       "   (5880, 52),\n",
       "   (10892, 67),\n",
       "   (6636, 66),\n",
       "   (21197, 51),\n",
       "   (17239, 129),\n",
       "   (26180, 158)]),\n",
       " (512,\n",
       "  [(26283, 26),\n",
       "   (3382, 41),\n",
       "   (14360, 71),\n",
       "   (18765, 25),\n",
       "   (2273, 36),\n",
       "   (4061, 48),\n",
       "   (19866, 367),\n",
       "   (10017, 100),\n",
       "   (9699, 28),\n",
       "   (2366, 68),\n",
       "   (9106, 45),\n",
       "   (10531, 36),\n",
       "   (5243, 67),\n",
       "   (4612, 56)]),\n",
       " (544, [(26237, 2319), (13636, 186)]),\n",
       " (592, [(15170, 523), (1780, 322)])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_per_item.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5354f04-deff-478c-9d45-bd5058f1ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(48, 26404),\n",
       " (208, 26257),\n",
       " (352, 13653),\n",
       " (384, 27183),\n",
       " (464, 7607),\n",
       " (480, 16377),\n",
       " (496, 26180),\n",
       " (512, 4612),\n",
       " (544, 13636),\n",
       " (592, 1780)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third feature: the last item visited in the session.\n",
    "last_item_per_session  = time_per_item.mapValues(lambda x: x[-1][0])\n",
    "last_item_per_session.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "827e2f25-7af5-4d81-baff-9fa8549ee518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(48, (346.0, 311.0)),\n",
       " (208, (20.0, 0.0)),\n",
       " (352, (274.0, 85.0)),\n",
       " (384, (43.1, 25.719447894540817)),\n",
       " (464, (90.0, 0.0)),\n",
       " (480, (150.33333333333334, 73.90233795730386)),\n",
       " (496, (78.42857142857143, 43.709616930887165)),\n",
       " (512, (72.42857142857143, 84.18662456175589)),\n",
       " (544, (1252.5, 1066.5)),\n",
       " (592, (422.5, 100.5))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fourth and fifth features: the mean and standard deviation of the time spend on each item \n",
    "mean_time_per_session = time_per_item.mapValues(lambda x: (np.array(x)[:, 1].mean(), np.array(x)[:, 1].std()))\n",
    "mean_time_per_session.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53ba4dc4-767d-4cd8-8e14-edf958728a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[(243, 1), (13322, 1), (14248, 1)]                                              \n",
      "----\n",
      "[(22705, 1)]\n",
      "----\n",
      "[(2447, 1), (19150, 1), (18737, 1)]\n",
      "----\n",
      "[(22435, 1), (2314, 1)]\n",
      "----\n",
      "[(20086, 1), (222, 1), (19215, 1), (4542, 1)]                       (0 + 4) / 4]\n",
      "----\n",
      "[(6373, 1)]\n",
      "----\n",
      "[(12427, 1)]                                                                    \n",
      "----\n",
      "[(7360, 1)]\n",
      "----[(3254, 1), (24454, 1), (6651, 1), (15373, 1), (24799, 1), (4867, 1), (17495, 1), (20641, 1), (7159, 1), (10445, 1), (10440, 1), (27943, 1), (6392, 1), (27232, 1), (22474, 1)]\n",
      "----\n",
      "\n",
      "[(20556, 1), (19736, 1), (12945, 1), (19736, 1)]\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(747089, <pyspark.resultiterable.ResultIterable at 0x7fb0cc9bf640>),\n",
       " (910665, <pyspark.resultiterable.ResultIterable at 0x7fb0cc9bf5e0>),\n",
       " (1983049, <pyspark.resultiterable.ResultIterable at 0x7fb0cc9bf2b0>),\n",
       " (4385009, <pyspark.resultiterable.ResultIterable at 0x7fb0cc9bf730>),\n",
       " (2188762, <pyspark.resultiterable.ResultIterable at 0x7fb0cc9bf790>),\n",
       " (3994580, <pyspark.resultiterable.ResultIterable at 0x7fb0cc9bf7f0>),\n",
       " (2974789, <pyspark.resultiterable.ResultIterable at 0x7fb0cca28fa0>),\n",
       " (3069125, <pyspark.resultiterable.ResultIterable at 0x7fb0cca28850>),\n",
       " (1584439, <pyspark.resultiterable.ResultIterable at 0x7fb0cca28040>),\n",
       " (2908407, <pyspark.resultiterable.ResultIterable at 0x7fb0cca28dc0>)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_different_items(x):\n",
    "    values = [x_ for x_ in x] \n",
    "    print(values)\n",
    "    print('----')\n",
    "    return x\n",
    "\n",
    "items_per_sessions = (train_sessions.rdd\n",
    "                      .map(lambda x: (x.session_id, (x.item_id, 1)))\n",
    "                      .groupByKey()\n",
    "                      .sample(False, 0.000005, seed=42)\n",
    "                      .mapValues(count_different_items)\n",
    ")\n",
    "items_per_sessions.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e246275-2519-44ad-8ffd-09ca7aee314e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
