{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8b7641-757d-48be-b113-d680a3a617ba",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "From the dataset explored in the 0_data_analysis notebook, we compute several features.\n",
    "\n",
    "In order to obtain a learnable dataset, features must be preprocessed and engineered in order to contain valuable learning data. This notebook will generate a PySpark RDD that contains the preprocessed dataset, ready for feature selection algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfefb50-d3ea-42d8-b0e4-fd5981520e8c",
   "metadata": {},
   "source": [
    "### Configuring and launching the pyspark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82df480c-3e79-468f-a378-e5477796c93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/19 08:13:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/19 08:13:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/19 08:13:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "import sys\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"load_explore\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752e400-58b3-4576-bd3a-a6c47dea0bb1",
   "metadata": {},
   "source": [
    "### Loading the datasets inside spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d668984b-81c2-4ff8-96d7-90b2a739aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loads relevant datas in DataFrames\n",
    "train_sessions = spark.read.load('../Data/train_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "candidate_items = spark.read.load('../Data/candidate_items.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "item_features = spark.read.load('../Data/item_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed91f9d-7fc4-4e26-b549-4d6daebea0f7",
   "metadata": {},
   "source": [
    "## Time related features.\n",
    "\n",
    "As fashion purchases are highly dependant on seasonal trends, we will extract date features from the session item visit dates.\n",
    "\n",
    "For each session, the first item visit date will be taken and used as the reference session date. (The average session duration is an hour)\n",
    "\n",
    "Engineered features include:\n",
    "* Month of the session ✅\n",
    "* Season of the session ✅\n",
    "* Year of the session ✅\n",
    "* Duration of the session ✅\n",
    "* Day period of the session ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1a988c-16d0-4101-86ec-be2db53a5236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, 3703), (28, 87), (36, 43), (44, 33), (48, 657)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def parse_datetime(timestamp):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')    \n",
    "\n",
    "date_parsed_sessions = (train_sessions.rdd\n",
    "                        .map(lambda x: (x.session_id, parse_datetime(x.date)))  # Maps rows to (key=session_id, values=(parsed_date)) tuples\n",
    "                        .cache())\n",
    "\n",
    "# Reduces by key from the MAX monoid\n",
    "max_date_sessions = (\n",
    "    date_parsed_sessions\n",
    "    .reduceByKey(max)\n",
    ")\n",
    "\n",
    "# Reduces by key from the MIN monoid\n",
    "min_date_sessions = (\n",
    "    date_parsed_sessions\n",
    "    .reduceByKey(min)\n",
    ")\n",
    "\n",
    "# Computes delta time in seconds\n",
    "time_sessions_in_seconds = (\n",
    "    max_date_sessions\n",
    "    .join(min_date_sessions)  # Joins the max dates with the min dates from the session_id (on pair per session)\n",
    "    .mapValues(lambda x: (x[0] - x[1]).seconds)  # Computes delta time for each session \n",
    ")\n",
    "\n",
    "time_sessions_in_seconds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "857ed548-6a58-43ad-b082-99d7b86cf976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, datetime.datetime(2020, 2, 26, 17, 22, 48, 903000)), (28, datetime.datetime(2020, 5, 18, 12, 50, 24, 248000)), (36, datetime.datetime(2020, 6, 21, 10, 29, 8, 263000)), (44, datetime.datetime(2020, 11, 27, 20, 45, 10, 302000)), (48, datetime.datetime(2020, 4, 15, 17, 17, 42, 594000))]\n",
      "[(24, 0), (28, 1), (36, 2), (44, 3), (48, 1)]\n",
      "[(24, 4), (28, 3), (36, 2), (44, 5), (48, 4)]\n",
      "[(24, 2), (28, 5), (36, 6), (44, 11), (48, 4)]\n",
      "[(24, 2020), (28, 2020), (36, 2020), (44, 2020), (48, 2020)]\n"
     ]
    }
   ],
   "source": [
    "def get_season(date_time):\n",
    "    '''Converts the date_time into a season\n",
    "    \n",
    "    :returns: an integer\n",
    "        0 -> Winter\n",
    "        1 -> Spring\n",
    "        2 -> Summer\n",
    "        3 -> Autumn\n",
    "    '''    \n",
    "    season = (date_time.month - 1) // 3\n",
    "    season += (date_time.month == 3)&(date_time.day>=20)\n",
    "    season += (date_time.month == 6)&(date_time.day>=21)\n",
    "    season += (date_time.month == 9)&(date_time.day>=23)\n",
    "    season -= 3*int(((date_time.month == 12)&(date_time.day>=21)))\n",
    "    return season\n",
    "\n",
    "def get_day_period(date_time):\n",
    "    '''Converts the date_time into the day of the week.\n",
    "    \n",
    "    0 -> Morning (from 6am to 12am)\n",
    "    1 -> Afternoon (from 12am to 6pm)\n",
    "    2 -> Evening (from 6pm to 12pm)\n",
    "    3 -> Night (from 12pm to 6am)\n",
    "    '''\n",
    "    return date_time.hour // 4\n",
    "\n",
    "# Assigns a season for each session\n",
    "season_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(get_season)\n",
    ")\n",
    "\n",
    "# Assigns a day period (morning, afternoon, evening, night...) for each session\n",
    "day_period_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(get_day_period)\n",
    ")\n",
    "\n",
    "# Assigns a month for each session\n",
    "month_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(lambda x: x.month)\n",
    ")\n",
    "\n",
    "# Assigns a year for each session\n",
    "year_per_session = (\n",
    "    min_date_sessions\n",
    "    .mapValues(lambda x: x.year)\n",
    ")\n",
    "\n",
    "print(min_date_sessions.take(5))\n",
    "print(season_per_session.take(5))\n",
    "print(day_period_per_session.take(5))\n",
    "print(month_per_session.take(5))\n",
    "print(year_per_session.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570a914-1596-41f5-bfdf-72eadae751e9",
   "metadata": {},
   "source": [
    "## Item time related features\n",
    "\n",
    "Computing the items where time was the most spent on. The time spent on an item is the difference between the visit of that item and the visit of the last item. For the last item, the time is computed from the session purchase date.\n",
    "\n",
    "We will extract two feautures:\n",
    "* The item on which the use has spent the most time on ✅\n",
    "* The time spent on that item ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e42080da-0cf0-4f15-94a0-ce54144b5e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24,\n",
       " [(2927, 15),\n",
       "  (2927, 180),\n",
       "  (16064, 23),\n",
       "  (11662, 42),\n",
       "  (434, 3096),\n",
       "  (18539, 183),\n",
       "  (10414, 42),\n",
       "  (28075, 118),\n",
       "  (18476, 191)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing session purchase_date\n",
    "session_purchase_date = (\n",
    "    train_purchases.rdd\n",
    "    .map(lambda x: (x.session_id, parse_datetime(x.date)))\n",
    ")\n",
    "\n",
    "def compute_time_per_item(session_info):\n",
    "    '''Computes the time spent on each item (in seconds)\n",
    "    \n",
    "    :param session_info: The information of the session ([(item_id, date)...], purchase_date)\n",
    "    :returns: Time per item information [(item_id, visit_time)...]\n",
    "    '''\n",
    "    # Unpacks the info\n",
    "    visited_items, purchase_date = session_info\n",
    "    item_time_list = []\n",
    "    \n",
    "    for idx in range(len(visited_items)):\n",
    "        item_id, item_visit_date = visited_items[idx]\n",
    "        \n",
    "        # Last item, need to check the purchase date\n",
    "        if idx == len(visited_items) - 1:\n",
    "            item_time_list.append((item_id, (purchase_date - item_visit_date).seconds))\n",
    "        # Not the last item, checks the next item \n",
    "        else:\n",
    "            _, next_item_visit_date = visited_items[idx + 1]\n",
    "            item_time_list.append((item_id, (next_item_visit_date - item_visit_date).seconds))\n",
    "    \n",
    "    return item_time_list\n",
    "\n",
    "\n",
    "# Here, we group the tuples by their keys (session_id) using Map Reduce\n",
    "items_time_per_session = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, (x.item_id, parse_datetime(x.date))))\n",
    "    .mapValues(lambda x: [(x[0], x[1])])  # Sets all the values inside a list in order to easily reduce and join the sessions\n",
    "    .reduceByKey(lambda x, y: x + y)  # Joins all the sessions togheter by reducing and joining the lists\n",
    "    .join(session_purchase_date)  # Each tuple has now the shape (session_id, ([(item_id, date)...], purchase_date))\n",
    "    .mapValues(lambda x: (sorted(x[0], key=lambda t:t[1]), x[1]))  # For each session, sorts the (item_id, date) tuples by date \n",
    "    .mapValues(compute_time_per_item)\n",
    ")\n",
    "\n",
    "items_time_per_session.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3171ed3a-1d7f-43e1-a5e3-2fb8a3c58a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24, (434, 3096))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_spent_time = (\n",
    "    items_time_per_session\n",
    "    .mapValues(lambda x: sorted(x, reverse=True, key=lambda t: t[1]))  # Sorts the item visits by time\n",
    "    .mapValues(lambda x: x[0])  # Gets the item with the most time spent on\n",
    ")\n",
    "\n",
    "most_spent_time.take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655494d0-248a-4a63-83dd-7fe955d568bb",
   "metadata": {},
   "source": [
    "## Session mean and standard deviation time\n",
    "\n",
    "We will extract two features:\n",
    "\n",
    "* For each session, the mean visit time on each item ✅\n",
    "* For each session, the standard time on each time ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e933198-b6ab-4429-a71f-0a64fa651497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(24, (432.22223, 944.24854))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_std_time_session = (\n",
    "    items_time_per_session\n",
    "    .mapValues(lambda x: np.array(x, dtype=np.float32)[:, 1])  # Gets a numpy array for each time and takes only the visit time (drops the item id)\n",
    "    .mapValues(lambda x: (x.mean(), x.std()))\n",
    ")\n",
    "\n",
    "mean_std_time_session.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35435804-2cfb-4c72-aebf-589ef1ad61f0",
   "metadata": {},
   "source": [
    "## Item revisit time\n",
    "\n",
    "Item that have been revisited are the most likely to catch the interest of the user, and therefore to be purchased.\n",
    "\n",
    "We will extract three features:\n",
    "* The item that has been revisited the most times ✅\n",
    "* The number of times this item has been revisited ✅\n",
    "* The number of items that have been revisited at least once ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a40a7631-69a1-4dff-afa3-c0861cdf3f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, (2927, 2, 1))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "item_visit_counts = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.session_id, x.item_id))  # Maps the rows to a (key=session_id, values=item_id) tuple\n",
    "    .mapValues(lambda x: [x])  # Puts the items inside a list, so they can be reduced easily.\n",
    "    .reduceByKey(lambda x, y: x + y)  # Joins the lists togheter, grouping the tuples by keys\n",
    "    .mapValues(lambda x: np.unique(x, return_counts=True))  # Transforms values into two arrays: an item_id array and an occurence_array\n",
    "    .mapValues(lambda x: np.vstack((x[0], x[1])))\n",
    ")\n",
    "\n",
    "def get_revisited_items(count_array):\n",
    "    '''Gets the number of items revisited at least once'''\n",
    "    revisited_indices = count_array[1, :] > 1\n",
    "    return np.count_nonzero(revisited_indices)\n",
    "\n",
    "def get_item_revisits_info(count_array):\n",
    "    '''Returns the item_id of the item that\n",
    "    has been revisited the most as well as the \n",
    "    number of times it has been revisited\n",
    "    \n",
    "    If no item was revisited return -1, -1, 0\n",
    "    '''\n",
    "    number_of_revisits = get_revisited_items(count_array)\n",
    "    if number_of_revisits == 0:\n",
    "        return (-1, -1, number_of_revisits)\n",
    "    \n",
    "    most_revisited_item_idx = np.argmax(count_array[1, :])\n",
    "    most_revisits = count_array[1, :].max()\n",
    "    \n",
    "    return (count_array[0, most_revisited_item_idx], most_revisits, number_of_revisits)\n",
    "    \n",
    "\n",
    "item_revisit_info = (\n",
    "    item_visit_counts\n",
    "    .mapValues(get_item_revisits_info) \n",
    ")\n",
    "\n",
    "\n",
    "print(item_revisit_info.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b8fb4-fab5-4c91-96d5-e8a91a0b45c7",
   "metadata": {},
   "source": [
    "# Engineering dataset features from item features\n",
    "\n",
    "Each item in the dataset has a finite number of features. Presented in tuples of (feature_cateogy_id, feature_value_id), each item has multiple of those feauters.\n",
    "\n",
    "As there are 73 unique features categories and 904 different unique (feature_category, feature_value) pairs. As most of those categories are not documented, it is difficult to interepret their meaning.\n",
    "\n",
    "In this phase, we will engineer features by performing K-Means clustering using Map-Reduce.\n",
    "\n",
    "Multiple features will be engineered:\n",
    "* One feature telling in wich category cluster is the session belonging ✅\n",
    "* One feature for each cluster on the item feature value clustering (25 clusters) ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a673c8-70c3-4497-a60f-2dd694baf802",
   "metadata": {},
   "source": [
    "Counting the number of occurences of each item category in each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3d8eb90-c2d4-4116-a50c-c6d513b69cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(474192,\n",
       "  array([0.        , 0.00381679, 0.01145038, 0.02290076, 0.01145038,\n",
       "         0.01908397, 0.04961832, 0.        , 0.        , 0.        ,\n",
       "         0.02290076, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.02290076, 0.02290076, 0.02290076, 0.        ,\n",
       "         0.        , 0.        , 0.00381679, 0.01145038, 0.        ,\n",
       "         0.02290076, 0.        , 0.00381679, 0.02671756, 0.04961832,\n",
       "         0.00381679, 0.02290076, 0.00763359, 0.01145038, 0.        ,\n",
       "         0.00381679, 0.00381679, 0.00381679, 0.00381679, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.02290076,\n",
       "         0.01145038, 0.05725191, 0.        , 0.00381679, 0.05725191,\n",
       "         0.        , 0.        , 0.01908397, 0.        , 0.02671756,\n",
       "         0.05725191, 0.        , 0.        , 0.02290076, 0.01908397,\n",
       "         0.05343511, 0.02671756, 0.02290076, 0.00381679, 0.02290076,\n",
       "         0.        , 0.00381679, 0.05343511, 0.04961832, 0.        ,\n",
       "         0.00381679, 0.05343511, 0.01908397], dtype=float32))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts the number of unique category IDs\n",
    "unique_categories_nb = len(\n",
    "    item_features.rdd\n",
    "    .map(lambda x: set([x.feature_category_id]))  # Maps the rows to a set containing the feature category\n",
    "    .reduce(lambda x, y: x.union(y))  # Reduces by joining the sets, keeping only the unique categories values\n",
    ")\n",
    "\n",
    "def initialize_vector(category_nb, max_categories=unique_categories_nb):\n",
    "    '''\n",
    "    Returns a vector of categories, where each element is 0 except the one at the category's index.\n",
    "    '''\n",
    "    vector = np.zeros(max_categories, dtype=np.float32)\n",
    "    vector[category_nb - 1] = 1.0\n",
    "    return vector\n",
    "\n",
    "def normalize_vector(category_vector):\n",
    "    '''\n",
    "    Noramlizes a category vector, dividing all the elements by the sum of of occurences\n",
    "    '''\n",
    "    total_occurences = np.sum(category_vector)\n",
    "    return category_vector / total_occurences\n",
    "\n",
    "\n",
    "normalized_categories_vector_per_session = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.item_id, x.session_id))  # Maps the rows to (key=item_id,values=sessions_id) tuples\n",
    "    .join(item_features.rdd.map(lambda x: (x.item_id, x.feature_category_id)))  # Joins the two datasets togheter by item_id\n",
    "    .map(lambda x: x[1])  # Only keeps some parts of the tuples, now we have (key=session_id,values=feature_category_id)\n",
    "    .mapValues(initialize_vector)  # Encodes the feature category inside a counter vector\n",
    "    .reduceByKey(lambda x, y: x + y)   # Reduces by session_id, and adds the counter vectors\n",
    "    .mapValues(normalize_vector)  # Noramlizes the vector, so that the sum of every element is equal to 1.0\n",
    ")\n",
    "\n",
    "normalized_categories_vector_per_session.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b2f89-408e-4041-8dcc-819e43c7c347",
   "metadata": {},
   "source": [
    "## Clustering on item feature categories\n",
    "\n",
    "Performing clustering from the Normalized Category Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdfca8-cacd-436b-956d-986624244b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CLUSTERING WITH MAP/REDUCE\n",
    "# inspired from https://uv.ulb.ac.be/pluginfile.php/3410436/mod_resource/content/1/Kmeans.html?fbclid=IwAR1h5xJKxQ1nrCHlgtQsofwjgc8B7oVl69GG6Mm8WxChTH3zBc4SKFo3Noo\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def mapstep(x,br_clusters): \n",
    "    \"\"\"\n",
    "    This function returns the closest cluster of the current row x\n",
    "    x : current row\n",
    "    broadcast_clusters : current value of the clusters\n",
    "    \"\"\"\n",
    "    M = br_clusters.value.shape[0]\n",
    "    d=np.zeros(M) # distance of the current row with each cluster\n",
    "    for m in range(M):\n",
    "        '''\n",
    "        if x.shape != br_clusters.value[m].shape:\n",
    "            print('Detected shape problem:')\n",
    "            print(x)\n",
    "            print(br_clusters.value[m])\n",
    "        '''\n",
    "        d[m]=sum(abs(np.subtract(x,br_clusters.value[m]))) # compute distance between cluster m and row x\n",
    "    return np.argmin(d)    \n",
    "\n",
    "def add_values(x, y):\n",
    "    return x[0] + y[0], x[1] + y[1]\n",
    "\n",
    "def k_means_MR(dataset, M, steps):\n",
    "    \"\"\"\n",
    "    dataset : RDD data structure, all columns are used for distance computation (1xn dimension)\n",
    "    M : number of clusters\n",
    "    steps : number of steps\n",
    "    \"\"\"\n",
    "    n = len(dataset.take(1)[0]) # input dimension\n",
    "    clusters = np.array(dataset.takeSample(True,M)) # starts with random clusters\n",
    "    broadcast_clusters = sc.broadcast(clusters) ## broadcast cluster position\n",
    "    for i in range(steps):\n",
    "        distance_set = dataset.map(lambda x : (mapstep(x, broadcast_clusters), (x, 1))) # set containing (closest_cluster,(row value),counter=1)\n",
    "        new_clusters_set = distance_set.reduceByKey(add_values) # adds all cluster related entry (+ increments counter)\n",
    "        new_clusters_set = new_clusters_set.map(lambda x : x[1][0] / x[1][1]) # apply mean operation\n",
    "        new_clusters = np.array(new_clusters_set.take(M))\n",
    "        broadcast_clusters = sc.broadcast(new_clusters)\n",
    "    return broadcast_clusters.value\n",
    "\n",
    "\n",
    "mapped_vectors = normalized_categories_vector_per_session.map(lambda x: x[1]).cache()  # Drops the session id\n",
    "\n",
    "feature_category_clusters = k_means_MR(\n",
    "    mapped_vectors,\n",
    "    10,  # 10 Clusters are taken\n",
    "    5  # Iterates over 5 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9a06d274-6c4f-4245-9186-9b2993b776a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(474192, 8),\n",
       " (1119330, 8),\n",
       " (1309362, 2),\n",
       " (1468758, 8),\n",
       " (1910616, 2),\n",
       " (2691762, 4),\n",
       " (2933592, 0),\n",
       " (4141098, 8),\n",
       " (18, 2),\n",
       " (93486, 2)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_clusters = sc.broadcast(feature_category_clusters) # broadcasting is a good practice for parallel computing\n",
    "clustered_categories = normalized_categories_vector_per_session.map(lambda x : (x[0], mapstep(x[1],broadcast_clusters))) # compute associated clusters for each item\n",
    "\n",
    "clustered_categories.take(10) # (example) OUTPUT [SESSION_ID | CLUSTER_ID]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14cf37-2008-4bf5-930e-d52d37c8822c",
   "metadata": {},
   "source": [
    "## Clustering on item feature values\n",
    "\n",
    "We will perform clustering on the item feature values only. In total, 25 different clusters will be computed and each item will be classified in one of those clusters.\n",
    "\n",
    "Then, for each session we will look at the visited items. For each of those items we will increment the session's counter for the class of that item. In total, 25 new features will be added to our sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "860eaa1d-4ae5-487a-8d56-61b3e415a621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_feature_values = (\n",
    "    item_features.rdd\n",
    "    .map(lambda x: (set([x.feature_value_id])))  # Maps each row to a set containing the feature value\n",
    "    .reduce(lambda x, y: x.union(y))  # Joins the sets, discarding duplicate values (only keeping unique values)\n",
    ")\n",
    "\n",
    "highest_feature_value = max(unique_feature_values)  # Takes the highest value in the set, used as vector sie\n",
    "\n",
    "\n",
    "def initialize_value_vector(value_nb, max_values=highest_feature_value):\n",
    "    '''\n",
    "    Returns a vector of item feature values, where each element is 0 except the one at the values's index.\n",
    "    '''\n",
    "    vector = np.zeros(max_values, dtype=np.float32)\n",
    "    vector[value_nb - 1] = 1.0\n",
    "    return vector\n",
    "\n",
    "encoded_item_values = (\n",
    "    item_features.rdd\n",
    "    .map(lambda x: (x.item_id, x.feature_value_id))  # Maps each row to a (key=item_id, values=feature_value_id) tuples\n",
    "    .mapValues(initialize_value_vector)  # Initializes each value vector (one hot encoding type)\n",
    "    .reduceByKey(lambda x, y: x + y)  # Reduces on each item by summing the feature value vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8f83b868-b5d9-4683-b9fe-84ac411d510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Performs clustering\n",
    "mapped_item_values = encoded_item_values.map(lambda x: x[1])\n",
    "\n",
    "feature_values_clusters = k_means_MR(\n",
    "    mapped_item_values,\n",
    "    25,  # As analysed in notebook 2b_feature_clustering, 25 is a good cluster compromise\n",
    "    5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6f4b2600-d027-42f1-af4d-b8d8bb3faa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 12),\n",
       " (4, 3),\n",
       " (8, 0),\n",
       " (10, 1),\n",
       " (14, 14),\n",
       " (16, 13),\n",
       " (18, 2),\n",
       " (20, 3),\n",
       " (24, 1),\n",
       " (26, 15)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_clusters = sc.broadcast(feature_values_clusters) # broadcasting is a good practice for parallel computing\n",
    "clustered_item_values = encoded_item_values.map(lambda x : (x[0], mapstep(x[1],broadcast_clusters))) # compute associated clusters for each item\n",
    "\n",
    "clustered_item_values.take(10) # (example) OUTPUT [ITEM_ID | CLUSTER_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2dbccdf0-de8d-42de-b81f-c8718d32200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(474192,\n",
       "  (0.26666668,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.13333334,\n",
       "   0.06666667,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.13333334,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.2,\n",
       "   0.0,\n",
       "   0.2))]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encodes the clustered item values in one-hot encoding\n",
    "encoded_item_values = (\n",
    "    clustered_item_values\n",
    "    .mapValues(lambda x: initialize_value_vector(x, max_values=25))\n",
    ")\n",
    "\n",
    "# Computes the vector for each session\n",
    "item_features_session = (\n",
    "    train_sessions.rdd\n",
    "    .map(lambda x: (x.item_id, x.session_id))\n",
    "    .join(encoded_item_values)\n",
    "    .map(lambda x: (x[1][0], x[1][1]))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .mapValues(normalize_vector)\n",
    "    .mapValues(tuple)\n",
    ")\n",
    "\n",
    "item_features_session.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af26d9-bbe1-4159-aa51-d6e832195065",
   "metadata": {},
   "source": [
    "# Save the final dataset\n",
    "\n",
    "The final engineered dataset, containing all the values, is created by joining the other engineered value datasets with the session_id as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8e9900eb-3a07-49b9-89b5-de7738fc5540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(720,\n",
       "  [23943,\n",
       "   1,\n",
       "   2,\n",
       "   4,\n",
       "   2021,\n",
       "   21890,\n",
       "   20444,\n",
       "   5988.25,\n",
       "   8466.431,\n",
       "   21890,\n",
       "   4,\n",
       "   1,\n",
       "   6,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0])]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unpack_tuples(row):\n",
    "    out = []\n",
    "    for elem in row:\n",
    "        if type(elem) == tuple:\n",
    "            out.extend(unpack_tuples(elem))\n",
    "        else:\n",
    "            out.append(elem)\n",
    "    return out\n",
    "\n",
    "final_engineered_dataset = (\n",
    "    time_sessions_in_seconds\n",
    "    .join(season_per_session)\n",
    "    .join(day_period_per_session)\n",
    "    .join(month_per_session)\n",
    "    .join(year_per_session)\n",
    "    .join(most_spent_time)\n",
    "    .join(mean_std_time_session)\n",
    "    .join(item_revisit_info)\n",
    "    .join(clustered_categories)\n",
    "    .join(item_features_session)\n",
    "    .mapValues(unpack_tuples)\n",
    ")\n",
    "\n",
    "\n",
    "final_engineered_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "033f888b-f6fb-4267-81e1-57feab081ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tuple_final_engineered_dataset = (\n",
    "    final_engineered_dataset.map(lambda x: ((x[0],)+tuple(x[1])))\n",
    ")\n",
    "\n",
    "tuple_final_engineered_dataset.saveAsTextFile('/PROJ/Data/session_engineered_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e8088-d5f1-4c73-a4de-915d1f234eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
