{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a70e1a36-77d5-438d-b431-f871090c0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"feature_selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "362919ae-93d3-4df7-b3ff-46a1ae092e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_sessions_engineered = spark.read.csv('../Data/session_engineered_features.txt',header=False,\n",
    "                                          inferSchema=True)\n",
    "\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "clusters = spark.read.load('../Data/clusters.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true') # column 1 (2) = item_id, column 2 (136) = cluster\n",
    "clusters = clusters.withColumnRenamed(\"2\",\"item_id_\")\n",
    "clusters = clusters.withColumnRenamed(\"136\",\"cluster_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a131af4-fb02-4ca7-bebb-37ef32a87cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=720, _c37=0.0, _c5=2021.0, _c1=23943.0, _c35=0.0, _c18=0.0, _c36=0.0, _c19=0.0, _c33=0.0, _c6=21890.0, _c34=0.0, _c17=0.0, _c32=0.0, _c22=0.0, _c20=0.0, _c29=0.0, _c27=0.0, _c31=0.0, _c15=0.0, _c24=0.0, _c14=0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_indices = [-1, 36, 4, 0, 34, 17, 35, 18, 32, 5, 33, 16, 31, 21, 19, 28, 26, 30, 14, 23, 13]\n",
    "columns_to_keep = []\n",
    "for elem in selected_features_indices:\n",
    "    columns_to_keep.append(train_sessions_engineered.columns[elem+1])\n",
    "train_sessions_engineered = train_sessions_engineered.select(columns_to_keep)\n",
    "train_sessions_engineered.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6f9222b-2b29-4493-9d1e-34eaa2ce9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the dataframes on the session ids and drop useless columns\n",
    "train_df = train_sessions_engineered.join(train_purchases,train_sessions_engineered._c0 == train_purchases.session_id,\"inner\" )\n",
    "for col in ['_c0','session_id','date']:\n",
    "    train_df = train_df.drop(col)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51801040-3e25-4b24-830d-a69f0f4ee35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_c37=0.0, _c5=2020.0, _c1=0.0, _c35=0.0, _c18=0.0, _c36=0.0, _c19=1.0, _c33=0.0, _c6=15654.0, _c34=0.0, _c17=0.0, _c32=0.0, _c22=0.0, _c20=0.0, _c29=0.0, _c27=0.0, _c31=0.0, _c15=0.0, _c24=0.0, _c14=0.0, item_id=18626)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b040a73e-9b18-4efd-b5e4-341ddc313c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c969b-113e-4de8-86d5-83156617f48d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NAIVE BAYES (without clustering approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40006836-ca52-49ce-899f-bb40ab3036d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "def labelData(data):\n",
    "    return data.rdd.map(lambda x: LabeledPoint(int(x[-1]), x[:-1]))\n",
    "\n",
    "training_data, testing_data = labelData(train_df).randomSplit([0.8, 0.2])\n",
    "\n",
    "model = NaiveBayes.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5438498c-4afa-4075-b444-8765787a937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(False, 199250), (True, 8)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comp(row):\n",
    "    if row[0] == row[1]:\n",
    "        return (True,(1))\n",
    "    else:\n",
    "        return (False,(1))\n",
    "    \n",
    "testing_data.map(lambda p: (model.predict(p.features), p.label)).map(lambda x : comp(x)).reduceByKey(lambda x,y : np.add(x,y)).take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "007340d3-f58c-4988-b232-00863762cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pyspark.mllib.linalg import _convert_to_vector\n",
    "from pyspark import RDD\n",
    "\n",
    "# define custom predict function \n",
    "def custom_predict(model_bayes, x):\n",
    "    labels_ = model_bayes.labels\n",
    "    pi = model_bayes.pi\n",
    "    theta = model_bayes.theta\n",
    "    if isinstance(x, RDD):\n",
    "        return x.map(lambda v: custom_predict(model_bayes,v))\n",
    "    x = _convert_to_vector(x)\n",
    "    x = pi + x.dot(theta.transpose())\n",
    "    indices = np.argpartition(x, -100)[-100:]\n",
    "    top100 = labels_[indices]\n",
    "    return top100 #labels_[numpy.argmax(pi + x.dot(theta.transpose()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9fd8990-ece7-4c6c-bbf5-07c7b5fd4209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(21632.0, 8345.0),\n",
       " (20947.0, 4028.0),\n",
       " (7153.0, 25976.0),\n",
       " (20947.0, 25415.0),\n",
       " (18392.0, 8193.0),\n",
       " (21632.0, 1640.0),\n",
       " (15932.0, 717.0),\n",
       " (15743.0, 3774.0),\n",
       " (20297.0, 24200.0),\n",
       " (6753.0, 19157.0),\n",
       " (17321.0, 23185.0),\n",
       " (14369.0, 5642.0),\n",
       " (21571.0, 9262.0),\n",
       " (22108.0, 9184.0),\n",
       " (8322.0, 21943.0),\n",
       " (11495.0, 21781.0),\n",
       " (22831.0, 27499.0),\n",
       " (25936.0, 4917.0),\n",
       " (16297.0, 5146.0),\n",
       " (21715.0, 974.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.map(lambda p: (model.predict(p.features), p.label)).take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d006d-a397-4c0b-96b7-106b5c49b03d",
   "metadata": {},
   "source": [
    "# TOP 100 with bayes pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9b25ef3-28cc-43e2-87c6-5754e5680937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 82:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25415.0\n",
      "[ 2877. 17937. 11495. 12702. 11093. 18971. 27176. 18317. 20396. 21841.\n",
      "  4435. 12079. 19241. 13319. 18839. 23896. 15040.  9870.  8184. 22157.\n",
      "   676. 15521.  7800. 21549. 18548.  9639. 27625.  8769. 20457. 25083.\n",
      " 16466.   312.  5251. 15335.  3941.  4047.  7438. 14174.  1341. 20292.\n",
      "  2521. 23677. 21815.  9340. 23087.  3539. 19947.  2980.  9278. 25989.\n",
      " 14456.  6973. 27993.  3892. 15398. 17321. 23968.  4090. 20947. 11767.\n",
      "  6734. 22624. 12542. 21734.  6614. 17773. 25638. 10825.  2417. 20984.\n",
      " 24202. 27789. 13754. 15106. 10718. 25471. 24316. 13249. 15919.  8881.\n",
      "  8841. 11285.  6281.  1269. 19703. 23031. 12468.  8578. 22842. 27019.\n",
      " 27371.  5047.  6013. 21715. 18152.  5989.  8274. 14177. 16616. 20963.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top100 = custom_predict(model,testing_data.take(4)[3].features)\n",
    "print(testing_data.take(4)[3].label)\n",
    "print(top100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82e9e843-7a22-4e95-9164-37572ef6f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_top100 = testing_data.map(lambda p: (p.label in custom_predict(model,p.features), p.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81f5ec85-b609-4512-ae79-54027c919289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(False, 199036), (True, 222)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top100.map(lambda x : (x[0],(1))).reduceByKey(lambda x,y : np.add(x,y)).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d68dd8-426b-45cf-8114-544e98bf3ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
