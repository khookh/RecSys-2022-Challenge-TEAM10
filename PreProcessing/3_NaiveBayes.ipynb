{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a70e1a36-77d5-438d-b431-f871090c0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"feature_selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "362919ae-93d3-4df7-b3ff-46a1ae092e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/28 08:59:07 WARN CacheManager: Asked to cache already cached data.        \n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#train_sessions_engineered = spark.read.csv('../Data/session_engineered_features.csv',header=False,\n",
    "#                                          inferSchema=True)\n",
    "train_sessions_engineered = spark.read.load('../Data/session_engineered_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true').cache()\n",
    "\n",
    "\n",
    "train_sessions_engineered = spark.read.load('../Data/TEMPtrain_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eef2309c-ba03-4c89-a341-9e30c7056b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_purchases.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d37c048f-4d62-44fc-9d13-3fd93852ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['session_id', 'session_time', 'season', 'year', 'std_time', 'most_frequently_bought_for_most_revisited', 'first_item_visited', 'last_item_visited', '1', '5', '8', '9', '10', '11', '15', '16', '18', '19', '21', '22', '24']\n"
     ]
    }
   ],
   "source": [
    "mrmr = [0,1, 2, 5, 11, 15, 16, 17, 19, 23, 26, 27, 28, 29, 33, 34, 36, 37, 39, 40, 42]\n",
    "columns_to_keep = [train_sessions_engineered.columns[ind] for ind in mrmr] \n",
    "print(columns_to_keep)\n",
    "\n",
    "for col in train_sessions_engineered.columns:\n",
    "    if not col in columns_to_keep:\n",
    "        train_sessions_engineered = train_sessions_engineered.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b20f6d44-3391-4a2e-b51c-d0c72cc713ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# BINING \n",
    "from pyspark.ml.feature import Bucketizer\n",
    "to_bin = ['session_time','std_time']#,'1', '5', '8', '9', '10', '11', '15', '16', '18', '19', '21', '22', '24']\n",
    "for col in to_bin:\n",
    "    print(col)\n",
    "    column_values = train_sessions_engineered.select(col).collect()\n",
    "    splits_list = [0]\n",
    "    for i in range(10):\n",
    "        p = 10 * (i+1)\n",
    "        splits_list.append(np.percentile(column_values, p))\n",
    "    splits_list.append(float('Inf'))\n",
    "    bucketizer = Bucketizer(splits=list(np.unique(splits_list)),inputCol=col, outputCol=col+\"_binned\")\n",
    "    train_sessions_engineered = bucketizer.setHandleInvalid(\"keep\").transform(train_sessions_engineered)\n",
    "    train_sessions_engineered = train_sessions_engineered.drop(col)\n",
    "    train_sessions_engineered = train_sessions_engineered.withColumnRenamed(col+\"_binned\",col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5466537-6bfb-4ac7-87c4-057dfac6cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the dataframes on the session ids and drop useless columns\n",
    "train_df = train_sessions_engineered.join(train_purchases,train_sessions_engineered.session_id == train_purchases.session_id,\"inner\" ).cache()\n",
    "for col in ['session_id','date']:\n",
    "    train_df = train_df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "83acd03c-2362-49cf-9de1-06e71d91dd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(season=1.0, year=1.0, most_frequently_bought_for_most_revisited=39.0, first_item_visited=22.0, last_item_visited=5.0, 1=2.0, 5=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0, 15=0.0, 16=0.0, 18=0.0, 19=0.0, 21=0.0, 22=0.0, 24=0.0, session_time=3.0, std_time=2.0, item_id=1640)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.take(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c66567ef-5eee-457c-8152-463bd098d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.write.option(\"header\",True).csv('/PROJ/Data/data_processed_for_modelV2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a868b-5b99-44d3-baeb-1581d4ba557a",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40006836-ca52-49ce-899f-bb40ab3036d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/28 09:01:28 WARN CacheManager: Asked to cache already cached data.        \n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "train_df = spark.read.load('/PROJ/Data/data_processed_for_modelV2.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true').cache()\n",
    "\n",
    "def if_p(row):\n",
    "    row_bis = []\n",
    "    for count,elem in enumerate(row):\n",
    "        if elem == -1:\n",
    "            row_bis.append(0)\n",
    "        else:\n",
    "            row_bis.append(elem)\n",
    "    return row_bis\n",
    "\n",
    "train_df = train_df.rdd.map(if_p)\n",
    "\n",
    "def labelData(data):\n",
    "    return data.map(lambda x: LabeledPoint(int(x[-1]), x[:-1]))\n",
    "\n",
    "training_data, testing_data = labelData(train_df).randomSplit([0.9, 0.1])\n",
    "\n",
    "model = NaiveBayes.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "007340d3-f58c-4988-b232-00863762cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pyspark.mllib.linalg import _convert_to_vector\n",
    "from pyspark import RDD\n",
    "\n",
    "# define custom predict function \n",
    "def custom_predict(model_bayes, x):\n",
    "    labels_ = model_bayes.labels\n",
    "    pi = model_bayes.pi\n",
    "    theta = model_bayes.theta\n",
    "    if isinstance(x, RDD):\n",
    "        return x.map(lambda v: custom_predict(model_bayes,v))\n",
    "    x = _convert_to_vector(x)\n",
    "    x = pi + x.dot(theta.transpose())\n",
    "    indices = np.argsort(x)[-100:]\n",
    "    top100 = labels_[indices]\n",
    "    return top100 #labels_[numpy.argmax(pi + x.dot(theta.transpose()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b9fd8990-ece7-4c6c-bbf5-07c7b5fd4209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10390.0, 2173.0),\n",
       " (14959.0, 4799.0),\n",
       " (13540.0, 9846.0),\n",
       " (15290.0, 11785.0),\n",
       " (12990.0, 21886.0),\n",
       " (13872.0, 14592.0),\n",
       " (8060.0, 11376.0),\n",
       " (19511.0, 18948.0),\n",
       " (8060.0, 26853.0),\n",
       " (27555.0, 115.0),\n",
       " (8060.0, 14529.0),\n",
       " (1333.0, 6943.0),\n",
       " (27613.0, 16064.0),\n",
       " (8060.0, 27225.0),\n",
       " (19882.0, 1374.0),\n",
       " (19882.0, 19968.0),\n",
       " (18981.0, 14336.0),\n",
       " (20236.0, 742.0),\n",
       " (27555.0, 9992.0),\n",
       " (8060.0, 20689.0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.map(lambda p: (model.predict(p.features), p.label)).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1245bdb9-c7f3-49af-9c72-7dac0508f31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15290.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(testing_data.take(4)[3].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d5fb1-d923-4db7-99a9-7e06d30db0be",
   "metadata": {},
   "source": [
    "# TOP 100 with bayes pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7608ed0e-5e2a-4256-ba67-1ef92ec18202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10983. 19117. 17190. 22747.    45. 26785.  4015. 18239. 17297. 22669.\n",
      "  3856. 13922. 22607.  7958. 20374. 25109.  3406. 27892.  5615.  2690.\n",
      " 25213.  7640.  6899. 14733. 13596.  4028. 27362. 20770.   769. 11491.\n",
      " 23216. 17431. 17339.  1644.  8755. 25794. 17221. 23088.  5683. 16700.\n",
      " 12355. 14550. 18046. 13296.  3680.  9799. 18610. 14977. 14392.   972.\n",
      "  8231.  9184.  7792.  3859. 15501.  3747. 18962. 25091. 22311. 11053.\n",
      "  5657. 11162. 17376. 23286.  5481. 13558. 11209. 23956.  1572. 27225.\n",
      " 23905. 19170. 13994.  9427. 15812.  8142. 13924. 21375. 27205. 22992.\n",
      "  3106.  2814. 19521. 12665.  5367. 11237.  4375.  1752. 12556. 10385.\n",
      "  6749.  8060. 27613. 20906. 14830.  8592.  5340. 15104. 25723. 15290.]\n"
     ]
    }
   ],
   "source": [
    "top100 = custom_predict(model,testing_data.take(4)[3].features)\n",
    "print(top100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5b5913aa-cd3b-495c-8eec-870061672faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_top100 = testing_data.map(lambda p: (p.label in custom_predict(model,p.features), p.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9856a87f-afea-4c9c-afad-d507f82537bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, 2173.0),\n",
       " (True, 4799.0),\n",
       " (False, 9846.0),\n",
       " (False, 11785.0),\n",
       " (False, 21886.0),\n",
       " (False, 14592.0),\n",
       " (False, 11376.0),\n",
       " (False, 18948.0),\n",
       " (False, 26853.0),\n",
       " (True, 115.0)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top100.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "00690f21-bb08-4994-8c8a-7c72955685cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(False, 81461), (True, 18545)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top100.map(lambda x : (x[0],(1))).reduceByKey(lambda x,y : np.add(x,y)).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30511cad-5a8f-45b6-b8c9-9976a61f0f5a",
   "metadata": {},
   "source": [
    "#### VALIDATION SET SCORE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "42f8780b-9a43-4c2e-a55d-04a47239fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(top_100,label):\n",
    "    if label in top_100:\n",
    "        return (np.where(np.array(top_100) == label)[0][0] + 1)/100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c24f8f66-bfa4-41e4-9386-182b2915624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1222926624402534"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.map(lambda p: (score(custom_predict(model,p.features),p.label))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce3e4f-dd63-4d9c-bdbf-fac420c9259a",
   "metadata": {},
   "source": [
    "# Competition Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c43db26-3a13-4beb-9766-6ea57b926050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df = spark.read.load('/PROJ/Data/test_engineered_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "train_sessions_engineered = spark.read.load('../Data/TEMPtrain_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "454436e0-733e-433f-a10a-a7fc81af0350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(session_id=5400.0, session_time=76.0, season=2.0, day_period=1.0, month=5.0, year=1.0, item_most_time_spent=13.0, most_time_spent_on_item=65.0, most_frequently_bought_for_time_spent=1.0, least_frequently_bought_for_time_spent=13.0, mean_time=25.33333396911621, std_time=28.40578842163086, item_most_visited=13.0, number_o_visit=2.0, number_o_revisited_items=1.0, most_frequently_bought_for_most_revisited=5.0, first_item_visited=33.0, last_item_visited=13.0, normalized_features_vector=13.0, 1=6.0, 2=0.0, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0, 12=0.0, 13=0.0, 14=0.0, 15=0.0, 16=0.0, 17=0.0, 18=0.0, 19=0.0, 20=0.0, 21=0.0, 22=1.0, 23=0.0, 24=0.0, 25=0.0, _45=0.0)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c67c7df6-def2-4e74-b88a-1a295c9500b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['session_id', 'session_time', 'season', 'year', 'std_time', 'most_frequently_bought_for_most_revisited', 'first_item_visited', 'last_item_visited', '1', '5', '8', '9', '10', '11', '15', '16', '18', '19', '21', '22', '24']\n"
     ]
    }
   ],
   "source": [
    "mrmr = [0,1, 2, 5, 11, 15, 16, 17, 19, 23, 26, 27, 28, 29, 33, 34, 36, 37, 39, 40, 42]\n",
    "columns_to_keep = [test_df.columns[ind] for ind in mrmr] \n",
    "print(columns_to_keep)\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if not col in columns_to_keep:\n",
    "        test_df = test_df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "92eb627c-d1c5-452d-8aea-ad84338fae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# BINING \n",
    "from pyspark.ml.feature import Bucketizer\n",
    "to_bin = ['session_time','std_time']#,'1', '5', '8', '9', '10', '11', '15', '16', '18', '19', '21', '22', '24']\n",
    "for col in to_bin:\n",
    "    print(col)\n",
    "    column_values = train_sessions_engineered.select(col).collect()\n",
    "    splits_list = [0]\n",
    "    for i in range(10):\n",
    "        p = 10 * (i+1)\n",
    "        splits_list.append(np.percentile(column_values, p))\n",
    "    splits_list.append(float('Inf'))\n",
    "    bucketizer = Bucketizer(splits=list(np.unique(splits_list)),inputCol=col, outputCol=col+\"_binned\")\n",
    "    test_df = bucketizer.setHandleInvalid(\"keep\").transform(test_df)\n",
    "    test_df = test_df.drop(col)\n",
    "    test_df = test_df.withColumnRenamed(col+\"_binned\",col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6fc0f8bc-b5fa-47c8-9944-3aa45d835bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0, 1.0, 5.0, 33.0, 13.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "test_row = test_df.take(1)[0][1:]\n",
    "print(test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6db5e6ac-7751-4594-af05-51980c5d3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = custom_predict(model,test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1a3a439d-e673-43e7-a1f2-a995ba0a98ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22432.,  5509., 16767., 15705., 16910.,  9279.,  4065., 17106.,\n",
       "       25975., 13215., 21929., 20156., 20124., 19107., 26483.,  7827.,\n",
       "       12062., 20122., 10825., 27914.,  5303.,  6470., 23684., 25401.,\n",
       "       23547., 20441., 16959., 26353., 22591., 19642.,  8136.,  7341.,\n",
       "       19837., 26413., 21407., 16093., 13317., 24639.,  2071., 11380.,\n",
       "       22326.,  5994., 13650., 17209., 20242., 17777.,  5913., 15939.,\n",
       "       17570., 20533., 27882., 11512., 24913., 22554.,  1363.,  6268.,\n",
       "       19175., 20274.,  5032., 21398., 26970.,  6844.,  1446., 14580.,\n",
       "        2130., 18225., 18200.,  8330., 26249., 24406., 15731., 20160.,\n",
       "       13602., 13631., 14196., 11511., 22131.,  4497., 16104.,  8800.,\n",
       "       22855., 21975.,  5226., 11695.,  4930.,   650., 12253., 23315.,\n",
       "        6094., 26574.,  3403.,  1214.,   185., 27414., 10970.,  9514.,\n",
       "       14734.,   262., 22547.,  6253.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred # where last item = top1 prediction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "850d2d6e-6a8c-4c19-b556-91a4cec8f8d9",
   "metadata": {},
   "source": [
    "array([26428., 24243.,  8535., 26565., 16073., 15542.,  3679., 22719.,\n",
    "        2967., 24870., 21890.,  2952., 18868., 22886., 10372., 26585.,\n",
    "        4899., 19525., 19372., 18112., 20093.,  7464., 20303., 26249.,\n",
    "       22863.,  4171.,  9406.,  5005., 12621., 23578., 21578., 23367.,\n",
    "       11923.,  5021., 26836., 11275.,    74., 21616., 19145., 13310.,\n",
    "       21272.,  5433., 15593.,  7750., 12975., 26451., 18489., 23719.,\n",
    "       11804., 20612.,  6196., 19528., 19402., 28109.,  8188., 26853.,\n",
    "        7902.,  7096.,   955., 18289., 20449., 19348., 22524.,  1148.,\n",
    "       15424.,  3438., 10338., 17571.,  4611., 16766.,  2188., 12555.,\n",
    "        4364., 23450., 22175., 13987., 22209.,  6736., 13656., 21343.,\n",
    "        6762., 20257., 27393., 18509., 24736., 15249., 16929.,   340.,\n",
    "         436., 14648.,  4254., 21215., 16846.,  8807.,   403.,  5463.,\n",
    "        8577.,  4193., 27852., 23789.])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65a513f3-91b4-4dbe-8820-c45c2070e6b1",
   "metadata": {},
   "source": [
    "array([10785., 15542., 27202., 12518., 24465., 15844., 15593., 19372.,\n",
    "       21675., 17715.,   485., 10111., 24887., 14379.,  1641.,  5272.,\n",
    "       10304.,   409., 10382.,  8807., 11249.,  3679., 17164., 10724.,\n",
    "        5724., 21040.,  3334., 17585.,  9538.,  5570.,  1592., 12872.,\n",
    "       20008., 14652.,  4990., 22736., 11313., 21160.,  4747., 14539.,\n",
    "       22840., 23293., 10945., 15121., 23657.,  5369.,  4899.,  8434.,\n",
    "       20724.,  4296., 20604., 17864., 25632., 16029., 12912., 19320.,\n",
    "       10278., 26258.,  1513., 17005., 19757.,  3720.,  9344., 16441.,\n",
    "       21001., 24888., 22811., 27086., 27989., 14988.,  5282., 13746.,\n",
    "       13537., 10295.,  6157., 21819., 18258., 22331.,  3966.,  8017.,\n",
    "        6762., 20258., 27937., 25308., 27403., 25081., 15188., 14160.,\n",
    "       14451., 26373., 20083.,  1960., 13955., 16368., 17461.,  3319.,\n",
    "        3184., 27819., 10976., 10605.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "27592062-0605-41cd-8c26-0247da532734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "rank = [i for i in range(100,0,-1)]\n",
    "df = []\n",
    "for count, input_row in enumerate(test_df.collect()):\n",
    "    if not count+1 % 5000 :\n",
    "        print(count)\n",
    "    session_id = int(input_row[0])\n",
    "    pred = custom_predict(model,input_row[1:])\n",
    "    df += [[session_id, int(pred[i]), rank[i]] for i in range(100)]\n",
    "output_df = spark.createDataFrame([[session_id, int(pred[i]), rank[i]] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e07497cd-6857-403a-8f5b-6138949fd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = spark.createDataFrame(df,['session_id','item_id','rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20d0b6f0-7d75-4e10-baa5-227cfab843cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/28 09:08:16 WARN TaskSetManager: Stage 147 contains a task of very large size (58532 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_df.coalesce(1).write.option(\"header\",True).csv('/PROJ/Data/leaderboard_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5b5a89f2-a0ba-4cd9-9560-d2a2d1eb290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/28 09:08:20 WARN TaskSetManager: Stage 148 contains a task of very large size (14623 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2577bb69-fa01-4f63-812b-a965d80b2033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/28 09:08:23 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    }
   ],
   "source": [
    "output_df = spark.read.load('/PROJ/Data/leaderboard_prediction.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "464dc91d-8ed6-4b36-b78f-2ef369660670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(session_id=5400, item_id=22432, rank=100),\n",
       " Row(session_id=5400, item_id=5509, rank=99),\n",
       " Row(session_id=5400, item_id=16767, rank=98),\n",
       " Row(session_id=5400, item_id=15705, rank=97),\n",
       " Row(session_id=5400, item_id=16910, rank=96),\n",
       " Row(session_id=5400, item_id=9279, rank=95),\n",
       " Row(session_id=5400, item_id=4065, rank=94),\n",
       " Row(session_id=5400, item_id=17106, rank=93),\n",
       " Row(session_id=5400, item_id=25975, rank=92),\n",
       " Row(session_id=5400, item_id=13215, rank=91)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187c1ff-d36b-45d8-84f7-9c4df69f2a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
