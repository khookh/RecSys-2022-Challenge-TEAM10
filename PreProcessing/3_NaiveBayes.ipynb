{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70e1a36-77d5-438d-b431-f871090c0153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/27 10:37:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/27 10:37:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/27 10:37:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# launch this cell if you have issues on windows with py4j (think about updating your PATH)\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# starts a spark session from notebook\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"feature_selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362919ae-93d3-4df7-b3ff-46a1ae092e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 10:38:02 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#train_sessions_engineered = spark.read.csv('../Data/session_engineered_features.csv',header=False,\n",
    "#                                          inferSchema=True)\n",
    "train_sessions_engineered = spark.read.load('../Data/session_engineered_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true').cache()\n",
    "\n",
    "train_purchases = spark.read.load('../Data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef2309c-ba03-4c89-a341-9e30c7056b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_purchases.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37c048f-4d62-44fc-9d13-3fd93852ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['session_id', 'session_time', 'season', 'year', 'std_time', 'most_frequently_bought_for_most_revisited', 'first_item_visited', 'last_item_visited', '1', '5', '8', '9', '10', '11', '15', '16', '18', '19', '21', '22', '24']\n"
     ]
    }
   ],
   "source": [
    "mrmr = [0,1, 2, 5, 11, 15, 16, 17, 19, 23, 26, 27, 28, 29, 33, 34, 36, 37, 39, 40, 42]\n",
    "columns_to_keep = [train_sessions_engineered.columns[ind] for ind in mrmr] \n",
    "print(columns_to_keep)\n",
    "\n",
    "for col in train_sessions_engineered.columns:\n",
    "    if not col in columns_to_keep:\n",
    "        train_sessions_engineered = train_sessions_engineered.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20f6d44-3391-4a2e-b51c-d0c72cc713ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# BINING \n",
    "from pyspark.ml.feature import Bucketizer\n",
    "to_bin = ['session_time','std_time']#,'1', '5', '8', '9', '10', '11', '15', '16', '18', '19', '21', '22', '24']\n",
    "for col in to_bin:\n",
    "    print(col)\n",
    "    column_values = train_sessions_engineered.select(col).collect()\n",
    "    splits_list = [0]\n",
    "    for i in range(10):\n",
    "        p = 10 * (i+1)\n",
    "        splits_list.append(np.percentile(column_values, p))\n",
    "    splits_list.append(float('Inf'))\n",
    "    bucketizer = Bucketizer(splits=list(np.unique(splits_list)),inputCol=col, outputCol=col+\"_binned\")\n",
    "    train_sessions_engineered = bucketizer.setHandleInvalid(\"keep\").transform(train_sessions_engineered)\n",
    "    train_sessions_engineered = train_sessions_engineered.drop(col)\n",
    "    train_sessions_engineered = train_sessions_engineered.withColumnRenamed(col+\"_binned\",col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5466537-6bfb-4ac7-87c4-057dfac6cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the dataframes on the session ids and drop useless columns\n",
    "train_df = train_sessions_engineered.join(train_purchases,train_sessions_engineered.session_id == train_purchases.session_id,\"inner\" ).cache()\n",
    "for col in ['session_id','date']:\n",
    "    train_df = train_df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83acd03c-2362-49cf-9de1-06e71d91dd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(season=1.0, year=1.0, most_frequently_bought_for_most_revisited=5.0, first_item_visited=23.0, last_item_visited=23.0, 1=2.0, 5=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0, 15=0.0, 16=0.0, 18=0.0, 19=0.0, 21=0.0, 22=0.0, 24=0.0, session_time=3.0, std_time=2.0, item_id=1640)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.take(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66567ef-5eee-457c-8152-463bd098d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.write.option(\"header\",True).csv('/PROJ/Data/data_processed_for_modelV2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a868b-5b99-44d3-baeb-1581d4ba557a",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40006836-ca52-49ce-899f-bb40ab3036d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 10:57:51 WARN CacheManager: Asked to cache already cached data.        \n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "train_df = spark.read.load('/PROJ/Data/data_processed_for_modelV2.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true').cache()\n",
    "\n",
    "def if_p(row):\n",
    "    row_bis = []\n",
    "    for count,elem in enumerate(row):\n",
    "        if elem == -1:\n",
    "            row_bis.append(0)\n",
    "        else:\n",
    "            row_bis.append(elem)\n",
    "    return row_bis\n",
    "\n",
    "train_df = train_df.rdd.map(if_p)\n",
    "\n",
    "def labelData(data):\n",
    "    return data.map(lambda x: LabeledPoint(int(x[-1]), x[:-1]))\n",
    "\n",
    "training_data, testing_data = labelData(train_df).randomSplit([0.9, 0.1])\n",
    "\n",
    "model = NaiveBayes.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "007340d3-f58c-4988-b232-00863762cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pyspark.mllib.linalg import _convert_to_vector\n",
    "from pyspark import RDD\n",
    "\n",
    "# define custom predict function \n",
    "def custom_predict(model_bayes, x):\n",
    "    labels_ = model_bayes.labels\n",
    "    pi = model_bayes.pi\n",
    "    theta = model_bayes.theta\n",
    "    if isinstance(x, RDD):\n",
    "        return x.map(lambda v: custom_predict(model_bayes,v))\n",
    "    x = _convert_to_vector(x)\n",
    "    x = pi + x.dot(theta.transpose())\n",
    "    indices = np.argsort(x)[-100:]\n",
    "    top100 = labels_[indices]\n",
    "    return top100 #labels_[numpy.argmax(pi + x.dot(theta.transpose()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9fd8990-ece7-4c6c-bbf5-07c7b5fd4209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8025.0, 13063.0),\n",
       " (19452.0, 17760.0),\n",
       " (4193.0, 9846.0),\n",
       " (25007.0, 13712.0),\n",
       " (25007.0, 21004.0),\n",
       " (23972.0, 7566.0),\n",
       " (8060.0, 16876.0),\n",
       " (18981.0, 14592.0),\n",
       " (8060.0, 795.0),\n",
       " (8060.0, 11376.0),\n",
       " (19882.0, 3273.0),\n",
       " (8060.0, 20599.0),\n",
       " (8060.0, 4446.0),\n",
       " (15128.0, 23272.0),\n",
       " (4130.0, 23170.0),\n",
       " (4193.0, 26123.0),\n",
       " (25374.0, 12263.0),\n",
       " (2133.0, 22788.0),\n",
       " (9184.0, 22974.0),\n",
       " (8060.0, 21204.0)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.map(lambda p: (model.predict(p.features), p.label)).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1245bdb9-c7f3-49af-9c72-7dac0508f31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25007.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(testing_data.take(4)[3].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d5fb1-d923-4db7-99a9-7e06d30db0be",
   "metadata": {},
   "source": [
    "# TOP 100 with bayes pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7608ed0e-5e2a-4256-ba67-1ef92ec18202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18483. 19940. 12179. 14550. 28133.  3741.  3373. 16064. 18723. 26433.\n",
      "  4886.  2133. 19170.  6146.  4028. 13698. 10090. 11196. 11923. 20202.\n",
      "  5414.  7640. 26876. 24367.  4917. 27330. 11227. 13124. 17089. 15501.\n",
      " 24022. 23451. 10983. 19882.  6685.  2173. 27087. 24034. 23794.   753.\n",
      "   972. 27835.  1367. 23088. 10223. 17316.  2570. 25516.   412. 28035.\n",
      "  5810.   773.  8746. 20599. 12251.  1272.  7260. 17201. 25785.  9427.\n",
      " 15107.  3235. 27225.  6035. 14392. 19955.  6412.  8959. 17648.  2366.\n",
      " 21781. 27222.  6806.  1597.   434. 10717. 18156.  2072.  9522. 22607.\n",
      "  1953. 23967. 20770.  1365. 24921. 13226. 10486. 19087.  8057.  2447.\n",
      "  2314. 25811. 21148.  1199. 10390.  1358.  8398.  8622.  8060. 25007.]\n"
     ]
    }
   ],
   "source": [
    "top100 = custom_predict(model,testing_data.take(4)[3].features)\n",
    "print(top100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b5913aa-cd3b-495c-8eec-870061672faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_top100 = testing_data.map(lambda p: (p.label in custom_predict(model,p.features), p.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9856a87f-afea-4c9c-afad-d507f82537bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, 13063.0),\n",
       " (False, 17760.0),\n",
       " (False, 9846.0),\n",
       " (False, 13712.0),\n",
       " (False, 21004.0),\n",
       " (False, 7566.0),\n",
       " (True, 16876.0),\n",
       " (False, 14592.0),\n",
       " (False, 795.0),\n",
       " (False, 11376.0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top100.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00690f21-bb08-4994-8c8a-7c72955685cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(False, 80670), (True, 19417)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top100.map(lambda x : (x[0],(1))).reduceByKey(lambda x,y : np.add(x,y)).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30511cad-5a8f-45b6-b8c9-9976a61f0f5a",
   "metadata": {},
   "source": [
    "#### VALIDATION SET SCORE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42f8780b-9a43-4c2e-a55d-04a47239fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(top_100,label):\n",
    "    if label in top_100:\n",
    "        return (np.where(np.array(top_100) == label)[0][0] + 1)/100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c24f8f66-bfa4-41e4-9386-182b2915624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12866616044041698"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.map(lambda p: (score(custom_predict(model,p.features),p.label))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce3e4f-dd63-4d9c-bdbf-fac420c9259a",
   "metadata": {},
   "source": [
    "# Competition Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5c43db26-3a13-4beb-9766-6ea57b926050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 14:03:25 WARN CacheManager: Asked to cache already cached data.\n",
      "22/05/27 14:03:28 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    }
   ],
   "source": [
    "test_df = spark.read.load('/PROJ/Data/test_engineered_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true').cache()\n",
    "\n",
    "train_sessions_engineered = spark.read.load('../Data/session_engineered_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "454436e0-733e-433f-a10a-a7fc81af0350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(session_id=5400.0, session_time=76.0, season=2.0, day_period=1.0, month=5.0, year=1.0, item_most_time_spent=13.0, most_time_spent_on_item=65.0, most_frequently_bought_for_time_spent=1.0, least_frequently_bought_for_time_spent=13.0, mean_time=25.33333396911621, std_time=28.40578842163086, item_most_visited=13.0, number_o_visit=2.0, number_o_revisited_items=1.0, most_frequently_bought_for_most_revisited=5.0, first_item_visited=33.0, last_item_visited=13.0, normalized_features_vector=13.0, 1=6.0, 2=0.0, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0, 12=0.0, 13=0.0, 14=0.0, 15=0.0, 16=0.0, 17=0.0, 18=0.0, 19=0.0, 20=0.0, 21=0.0, 22=1.0, 23=0.0, 24=0.0, 25=0.0, _45=0.0)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c67c7df6-def2-4e74-b88a-1a295c9500b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['session_id', 'session_time', 'season', 'year', 'std_time', 'most_frequently_bought_for_most_revisited', 'first_item_visited', 'last_item_visited', '1', '5', '8', '9', '10', '11', '15', '16', '18', '19', '21', '22', '24']\n"
     ]
    }
   ],
   "source": [
    "mrmr = [0,1, 2, 5, 11, 15, 16, 17, 19, 23, 26, 27, 28, 29, 33, 34, 36, 37, 39, 40, 42]\n",
    "columns_to_keep = [test_df.columns[ind] for ind in mrmr] \n",
    "print(columns_to_keep)\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if not col in columns_to_keep:\n",
    "        test_df = test_df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "92eb627c-d1c5-452d-8aea-ad84338fae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_time\n",
      "std_time\n"
     ]
    }
   ],
   "source": [
    "# BINING \n",
    "from pyspark.ml.feature import Bucketizer\n",
    "to_bin = ['session_time','std_time']#,'1', '5', '8', '9', '10', '11', '15', '16', '18', '19', '21', '22', '24']\n",
    "for col in to_bin:\n",
    "    print(col)\n",
    "    column_values = train_sessions_engineered.select(col).collect()\n",
    "    splits_list = [0]\n",
    "    for i in range(10):\n",
    "        p = 10 * (i+1)\n",
    "        splits_list.append(np.percentile(column_values, p))\n",
    "    splits_list.append(float('Inf'))\n",
    "    bucketizer = Bucketizer(splits=list(np.unique(splits_list)),inputCol=col, outputCol=col+\"_binned\")\n",
    "    test_df = bucketizer.setHandleInvalid(\"keep\").transform(test_df)\n",
    "    test_df = test_df.drop(col)\n",
    "    test_df = test_df.withColumnRenamed(col+\"_binned\",col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6fc0f8bc-b5fa-47c8-9944-3aa45d835bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0, 1.0, 5.0, 33.0, 13.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "test_row = test_df.take(1)[0][1:]\n",
    "print(test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6db5e6ac-7751-4594-af05-51980c5d3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = custom_predict(model,test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1a3a439d-e673-43e7-a1f2-a995ba0a98ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3679., 10372., 14782.,  1037., 25632., 21890., 22863., 23578.,\n",
       "        5335., 23719., 26836., 18868., 26585.,  5433., 19402.,  5005.,\n",
       "       10724., 26565.,    74.,  4899.,  4171., 21343., 22886., 27001.,\n",
       "       21272., 26249., 10953., 19145., 24286., 26097.,  8881.,  9406.,\n",
       "       24538.,  9284., 24989., 11275., 24870., 15593., 21616., 16967.,\n",
       "       11923.,  4611., 22719.,  3438., 19372., 20449., 11804., 20612.,\n",
       "        1148.,  7464., 12975., 20303.,  7902.,  7096., 17571., 16766.,\n",
       "        6196., 26853.,  2967.,  7750., 22524.,  8188.,  6762., 18489.,\n",
       "        2188., 15542., 12555.,  5021.,   955., 27393., 19348.,  6736.,\n",
       "       13656., 23450., 18289.,  4364., 22175., 22209., 24736., 14648.,\n",
       "       15249., 18509., 19528.,   340., 15424.,  4254., 16929., 10338.,\n",
       "       16846., 20257., 21215., 13987.,   436.,  5463.,   403.,  8577.,\n",
       "        8807.,  4193., 27852., 23789.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred # where last item = top1 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8b083-33e7-4486-9bdb-6e6dab7efb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "array([10785., 15542., 27202., 12518., 24465., 15844., 15593., 19372.,\n",
    "       21675., 17715.,   485., 10111., 24887., 14379.,  1641.,  5272.,\n",
    "       10304.,   409., 10382.,  8807., 11249.,  3679., 17164., 10724.,\n",
    "        5724., 21040.,  3334., 17585.,  9538.,  5570.,  1592., 12872.,\n",
    "       20008., 14652.,  4990., 22736., 11313., 21160.,  4747., 14539.,\n",
    "       22840., 23293., 10945., 15121., 23657.,  5369.,  4899.,  8434.,\n",
    "       20724.,  4296., 20604., 17864., 25632., 16029., 12912., 19320.,\n",
    "       10278., 26258.,  1513., 17005., 19757.,  3720.,  9344., 16441.,\n",
    "       21001., 24888., 22811., 27086., 27989., 14988.,  5282., 13746.,\n",
    "       13537., 10295.,  6157., 21819., 18258., 22331.,  3966.,  8017.,\n",
    "        6762., 20258., 27937., 25308., 27403., 25081., 15188., 14160.,\n",
    "       14451., 26373., 20083.,  1960., 13955., 16368., 17461.,  3319.,\n",
    "        3184., 27819., 10976., 10605.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "27592062-0605-41cd-8c26-0247da532734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "rank = [i for i in range(100,0,-1)]\n",
    "df = []\n",
    "for count, input_row in enumerate(test_df.collect()):\n",
    "    if not count % 5000 :\n",
    "        print(count)\n",
    "    session_id = int(input_row[0])\n",
    "    pred = custom_predict(model,input_row[1:])\n",
    "    df += [[session_id, int(pred[i]), rank[i]] for i in range(100)]\n",
    "output_df = spark.createDataFrame([[session_id, int(pred[i]), rank[i]] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e07497cd-6857-403a-8f5b-6138949fd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = spark.createDataFrame(df,['session_id','item_id','rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "20d0b6f0-7d75-4e10-baa5-227cfab843cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 14:07:13 WARN TaskSetManager: Stage 135 contains a task of very large size (58512 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_df.coalesce(1).write.option(\"header\",True).csv('/PROJ/Data/leaderboard_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5b5a89f2-a0ba-4cd9-9560-d2a2d1eb290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 14:07:16 WARN TaskSetManager: Stage 136 contains a task of very large size (14618 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2577bb69-fa01-4f63-812b-a965d80b2033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 14:07:20 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    }
   ],
   "source": [
    "output_df = spark.read.load('/PROJ/Data/leaderboard_prediction.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "464dc91d-8ed6-4b36-b78f-2ef369660670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(session_id=5400, item_id=3679, rank=100),\n",
       " Row(session_id=5400, item_id=10372, rank=99),\n",
       " Row(session_id=5400, item_id=14782, rank=98),\n",
       " Row(session_id=5400, item_id=1037, rank=97),\n",
       " Row(session_id=5400, item_id=25632, rank=96),\n",
       " Row(session_id=5400, item_id=21890, rank=95),\n",
       " Row(session_id=5400, item_id=22863, rank=94),\n",
       " Row(session_id=5400, item_id=23578, rank=93),\n",
       " Row(session_id=5400, item_id=5335, rank=92),\n",
       " Row(session_id=5400, item_id=23719, rank=91)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187c1ff-d36b-45d8-84f7-9c4df69f2a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
